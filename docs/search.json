[
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html",
    "title": "Sexism Detection",
    "section": "",
    "text": "Credits: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\nKeywords: Sexism Detection, Multi-class Classification, LLMs, Prompting"
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#problem-definition",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#problem-definition",
    "title": "Sexism Detection",
    "section": "Problem definition",
    "text": "Problem definition\nGiven an input text sentence, the task is to label the sentence as sexist or not sexist (binary classification).\n\nExamples:\nText: *``Schedule a date with her, then don’t show up. Then text her “GOTCHA B___H”.’’*\nLabel: Sexist\nText: ``That’s completely ridiculous a woman flashing her boobs is not sexual assault in the slightest.’’\nLabel: Not sexist"
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#approach",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#approach",
    "title": "Sexism Detection",
    "section": "Approach",
    "text": "Approach\nWe will tackle the binary classification task with LLMs.\nIn particular, we’ll consider zero-/few-shot prompting approaches to assess the capability of some popular open-source LLMs on this task."
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#preliminaries",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#preliminaries",
    "title": "Sexism Detection",
    "section": "Preliminaries",
    "text": "Preliminaries\nWe are going to download LLMs from Huggingface.\nMany of these open-source LLMs require you to accept their “Community License Agreement” to download them.\nIn summary:\n\nIf not already, create an account of Huggingface (~2 mins)\nCheck a LLM model card page (e.g., Mistral v3) and accept its “Community License Agreement”.\nGo to your account -&gt; Settings -&gt; Access Tokens -&gt; Create new token -&gt; “Repositories permissions” -&gt; add the LLM model card you want to use.\nSave the token (we’ll need it later)\n\n\nHuggingface Login\nOnce we have created an account and an access token, we need to login to Huggingface via code.\n\nType your token and press Enter\nYou can say No to Github linking\n\n\n!huggingface-cli login\n\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `nlp_assignment2` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `nlp_assignment2`\n\n\nAfter login, you can download all models associated with your access token in addition to those that are not protected by an access token.\n\n\nData Loading\nSince we are only interested in prompting, we do not require a train dataset.\nWe have preparared a small test set version of EDOS in our dedicated Github repository.\nCheck the Assignment 2/data folder.\nIt contains:\n\na2_test.csv → a small test set of 300 samples.\ndemonstrations.csv -&gt; a batch of 1000 samples for few-shot prompting.\n\nBoth datasets contain a balanced number of sexist and not sexist samples.\n\n\nInstructions\nWe require you to:\n\nDownload the A2/data folder.\nEncode a2_test.csv into a pandas.DataFrame object.\n\n\n!pip install transformers --quiet\n!pip install datasets --quiet\n!pip install accelerate --quiet\n!pip install evaluate --quiet\n!pip install bitsandbytes --quiet\n\n\nimport torch\ntorch.cuda.is_available()\n\nTrue\n\n\n\n# system packages\nimport os\nimport json\nimport requests\nimport re\nfrom IPython.utils.io import capture_output\n\n# data and numerical management packages\nimport pandas as pd\nimport numpy as np\nimport random\n\nfrom tqdm import tqdm\nimport torch\nimport datasets\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot as plt\nfrom collections import Counter\nfrom matplotlib.ticker import MaxNLocator\nfrom tabulate import tabulate\n\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nfrom typing import Optional\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"punkt_tab\")\nnltk.download(\"wordnet\")\nnltk.download(\"averaged_perceptron_tagger_eng\")\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\ndef set_reproducibility(seed : int) -&gt; None:\n\n    \"\"\"\n    Sets a reproducible environment\n\n    Input:\n    seed: an integer used as seed\n    \"\"\"\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.random.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n\n    # Ensure reproducibility for PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_reproducibility(seed=42)\n\n\nclass DownloadProgressBar(tqdm):\n\n    def update_to(self, b=1, bsize=1, tsize=None):\n\n        if tsize is not None:\n            self.total = tsize\n        self.update(b * bsize - self.n)\n\ndef download_dataset(url: str, file_name: str, directory: str) -&gt; None:\n\n    \"\"\"\n    Download and store a file inside the directory\n\n    Input:\n    url: the url of the file to download\n    file_name: the name to save the file with\n    directory: the name in which save the file\n    \"\"\"\n\n    local_file_path = os.path.join(directory, file_name)\n\n    print(\"Downloading dataset...\")\n    with DownloadProgressBar(unit='B', unit_scale=True,\n                             miniters=1, desc=url.split('/')[-1]) as t:\n        response = requests.get(url)\n    print(\"Download complete!\")\n\n    print(f\"Saving the Dataset {file_name}.csv into local environment...\")\n    if response.status_code == 200:\n        with open(local_file_path, 'wb') as file:\n            file.write(response.content)\n        print(f\"File saved successfully\")\n    else:\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\n\nWe downloaded the files a2_test.csv and demonstration.csv inside the new dirctory Dataset.\n\nurl = \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%202/data/\"\n\ndirectory = 'Dataset'\nprint(f\"Creating the directory {directory}...\")\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\nfile_name = ['a2_test', 'demonstrations']\ndirectory = 'Dataset'\nfor name in file_name:\n    download_dataset(url+name+'.csv', name, directory)\n\nCreating the directory Dataset...\nDownloading dataset...\n\n\na2_test.csv: 0.00B [00:00, ?B/s]\n\n\nDownload complete!\nSaving the Dataset a2_test.csv into local environment...\nFile saved successfully\nDownloading dataset...\n\n\ndemonstrations.csv: 0.00B [00:00, ?B/s]\n\n\nDownload complete!\nSaving the Dataset demonstrations.csv into local environment...\nFile saved successfully\n\n\n\n\n\nWe stored the file a2_test.csv in the variable a2_test mapping the target feature from categorical to numerical.\n\nlabel_mapping = {'sexist': 1, 'not sexist': 0}\nlocal_file_path = os.path.join(directory, 'a2_test')\na2_test = pd.read_csv(local_file_path, converters={'label_sexist' : lambda x: label_mapping[x]})\na2_test.head()\n\n\n\n\n\n\n\n\nrewire_id\ntext\nlabel_sexist\n\n\n\n\n0\nsexism2022_english-17133\nThe boys must be gaming because there goes the...\n0\n\n\n1\nsexism2022_english-14197\nLook at those eyes. Either someone unexpectedl...\n1\n\n\n2\nsexism2022_english-3018\nOld man mogs everyone in this sub\n0\n\n\n3\nsexism2022_english-5301\nExcellent, I was just looking at another post ...\n0\n\n\n4\nsexism2022_english-17796\nSo you run back to daddy whenever you need hel...\n1\n\n\n\n\n\n\n\nLet’s explore the data, to understand whether there are some imbalances in the data distributions.\n\ndef plot_data_distribution(df: pd.DataFrame) -&gt; None:\n\n    \"\"\"\n    Shows the distribution of a dataset split.\n\n    Input:\n    df: split of the dataset (pd.DataFrame)\n    \"\"\"\n\n    label_1 = len(df[df[\"label_sexist\"] == 1])\n    label_0 = len(df[df[\"label_sexist\"] == 0])\n    bar_colors = [\"#6baed6\", \"#2171b5\"]\n    bars = plt.bar([\"label 1\", \"label 0\"], [label_1, label_0], color=bar_colors)\n    percentages = [label_1 / len(df)*100, label_0 / len(df)*100]\n    for bar, percentage in zip(bars, percentages):\n        plt.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height(),\n            f\"{percentage:.2f}%\",\n            ha=\"center\",\n            va=\"bottom\",\n        )\n    plt.title('Test data distribution')\n    plt.ylabel(\"label count\")\n    plt.show()\n\nplot_data_distribution(a2_test)\n\n\n\n\n\n\n\n\nAs we can see from the plot the test set is perfectly balanced between the sexism (label 1) and not sexism (label 0).\n\ntest_data = Dataset.from_pandas(a2_test)\nground_truth = np.array(test_data['label_sexist'])"
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#which-llms",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#which-llms",
    "title": "Sexism Detection",
    "section": "Which LLMs?",
    "text": "Which LLMs?\nThe pool of LLMs is ever increasing and it’s impossible to keep track of all new entries.\nWe focus on popular open-source models.\n\nMistral v2\nMistral v3\nLlama v3.1\nPhi3-mini\n\nOther open-source models are more than welcome!\n\nInstructions\nIn order to get Task 1 points, we require you to:\n\nPick 2 model cards from the provided list.\nFor each model:\n\nDefine a separate section of your notebook for the model.\nSetup a quantization configuration for the model.\nLoad the model via HuggingFace APIs.\n\n\n\n\nNotes\n\nThere’s a popular library integrated with Huggingface’s transformers to perform quantization.\nDefine two separate sections of your notebook to show that you have implemented the prompting pipeline for each selected model card.\n\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n\ndef get_model(model_card : str) -&gt; tuple:\n\n    \"\"\"\n    Download the model and its tokenizer from Hugging Face and set the configuratons\n\n    Input:\n    model_card: name of the model to dowload\n\n    Output:\n    model: the downloaded model\n    tokenizer: the tokenizer associated with the model\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_card)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_card,\n        return_dict=True,\n        quantization_config=bnb_config,\n        device_map='auto'\n    )\n\n    generation_config = model.generation_config\n    generation_config.max_new_tokens = 100\n    generation_config.eos_token_id = tokenizer.eos_token_id\n    generation_config.pad_token_id = tokenizer.eos_token_id\n    generation_config.temperature = None\n    generation_config.num_return_sequences = 1\n\n    return model, tokenizer\n\nWe decide to use Phi3-mini, a model with 3.8 billions parameters developed by Microsoft.\n\nmodel_card = \"microsoft/Phi-3-mini-4k-instruct\"\n\nmodel_1, tokenizer_1 = get_model(model_card)"
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#prompt-template",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#prompt-template",
    "title": "Sexism Detection",
    "section": "Prompt Template",
    "text": "Prompt Template\nUse the following prompt template to process input texts.\n\nzero_shot_prompt = [\n    {\n\n        'role': 'system',\n        'content': 'You are an annotator for sexism detection.'\n\n    },\n\n    {\n\n        'role': 'user',\n        'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n\n        TEXT:\n        {text}\n\n        ANSWER:\n        \"\"\"\n\n    }\n\n]\n\n\nInstructions\nIn order to get Task 2 points, we require you to:\n\nWrite a prepare_prompts function as the one reported below.\n\n\ndef prepare_prompts(texts: Dataset, prompt_template: list, tokenizer: transformers.AutoTokenizer) -&gt; DataLoader:\n\n    \"\"\"\n     Formats input text samples into instructions prompts.\n\n    Inputs:\n      texts: input texts to classify via prompting\n      prompt_template: the prompt template provided in this assignment\n      tokenizer: the transformers Tokenizer object instance associated with the chosen model card\n\n    Outputs:\n      input texts to classify in the form of instruction prompts\n    \"\"\"\n\n    def collate_fn(batch):\n        texts = tokenizer.batch_encode_plus([it['text'] for it in batch], return_tensors='pt', padding=True, truncation=True)\n        sentiment = torch.tensor([it['label_sexist'] for it in batch])\n        return texts, sentiment\n\n    def format_text(text):\n        prompt = tokenizer.apply_chat_template(prompt_template, tokenize=False, add_generation_prompt=True)\n        text['text'] = prompt.format(text=text['text'])\n        return text\n\n    zero_shot_data = texts.map(lambda x: format_text(x))\n    zero_shot_data = zero_shot_data.select_columns(['text', 'label_sexist'])\n\n    print(zero_shot_data['text'][0])\n    data_loader = DataLoader(zero_shot_data,\n                             batch_size=1,\n                             shuffle=False,\n                             collate_fn=collate_fn)\n\n    return data_loader\n\n\ndata_loader = prepare_prompts(test_data, zero_shot_prompt, tokenizer_1)\n\n\n\n\n&lt;|system|&gt;\nYou are an annotator for sexism detection.&lt;|end|&gt;\n&lt;|user|&gt;\nYour task is to classify input text as containing sexism or not. Respond only YES or NO.\n\n        TEXT:\n        The boys must be gaming because there goes the wifi.\n\n        ANSWER:\n        &lt;|end|&gt;\n&lt;|assistant|&gt;\n\n\n\n\n\nNotes\n\nYou are free to modify the prompt format (not its content) as you like depending on your code implementation.\nNote that the provided prompt has placeholders. You need to format the string to replace placeholders. Huggingface might have dedicated APIs for this."
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#notes-2",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#notes-2",
    "title": "Sexism Detection",
    "section": "Notes",
    "text": "Notes\n\nAccording to our tests, it should take you ~10 mins to perform full inference on 300 samples."
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#instructions-5",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#instructions-5",
    "title": "Sexism Detection",
    "section": "Instructions",
    "text": "Instructions\nIn order to get Task 5 points, we require you to:\n\nLoad demonstrations.csv and encode it into a pandas.DataFrame object.\nDefine a build_few_shot_demonstrations function as the one reported below.\nPerform few-shot inference as in Task 3.\nCompute metrics as in Task 4.\n\nLoad demonstrations.csv and encode it into a pandas.DataFrame object, mapping the taget feature in YES and NO.\n\nlocal_file_path = os.path.join(directory, 'demonstrations')\nlabel_mapping = {'sexist': 'YES', 'not sexist': 'NO'}\ndemonstrations = pd.read_csv(local_file_path, converters={'label_sexist' : lambda x: label_mapping[x]})\ndemonstrations.head()\n\n\n\n\n\n\n\n\nrewire_id\ntext\nlabel_sexist\n\n\n\n\n0\nsexism2022_english-10348\nI would never have my kid go to a daycare with...\nNO\n\n\n1\nsexism2022_english-7950\nDemocrat women are so butt fucking ugly.\nYES\n\n\n2\nsexism2022_english-7742\nIf you really wanted to twist the knife you’d ...\nNO\n\n\n3\nsexism2022_english-16170\nThose women are pro-rape. They didn’t care abo...\nYES\n\n\n4\nsexism2022_english-9857\nMarried a bitch... Is it the standard scenario...\nYES\n\n\n\n\n\n\n\nDefine a build_few_shot_demonstrations function as the one reported below\n\ndef build_few_shot_demonstrations(demonstrations: pd.DataFrame, num_per_class: int=2) -&gt; str:\n\n    \"\"\"\n    This function extracts random examples from the demonstrations.\n\n    Inputs:\n\n      demonstrations: the pandas.DataFrame object wrapping demonstrations.csv\n      num_per_class: number of demonstrations per class\n\n    Outputs:\n      a string of textual demonstrations to inject into the prompt template.\n    \"\"\"\n\n    demonstration_list = []\n    for i in range(num_per_class):\n        demonstration_list.append((demonstrations[demonstrations['label_sexist'] == 'YES'].sample(1, axis = 0)['text'].values[0], 'YES'))\n        demonstration_list.append((demonstrations[demonstrations['label_sexist'] == 'NO'].sample(1, axis = 0)['text'].values[0], 'NO'))\n\n    demonstration_samples = '\\n'.join([f'TEXT: {text}\\nANSWER: {answer}' for (text, answer) in demonstration_list])\n    return demonstration_samples\n\n\ndef prepare_prompts_few_shot(texts: Dataset,\n                             prompt_template: list,\n                             tokenizer: transformers.AutoTokenizer,\n                             demonstration_samples: Optional[str] = None,\n                             num_per_class: int = 2) -&gt; DataLoader:\n\n    \"\"\"\n    This function formats input text samples into instructions prompts with examples.\n\n    Inputs:\n      texts: input texts to classify via prompting\n      prompt_template: the prompt template provided in this assignment\n      tokenizer: the transformers Tokenizer object instance associated with the chosen model card\n      demonstration_samples: the examples used to format the prompt\n\n    Outputs:\n      input texts to classify in the form of instruction prompts\n    \"\"\"\n\n    def format_text(text):\n        prompt = tokenizer.apply_chat_template(prompt_template, tokenize=False, add_generation_prompt=True)\n        if demonstration_samples is None:\n          text['text'] = prompt.format(text=text['text'], examples= build_few_shot_demonstrations(demonstrations, num_per_class = num_per_class))\n        else:\n          text['text'] = prompt.format(text=text['text'], examples= demonstration_samples)\n        return text\n\n    def collate_fn(batch):\n        texts = tokenizer.batch_encode_plus([it['text'] for it in batch], return_tensors='pt', padding=True, truncation=True)\n        sentiment = torch.tensor([it['label_sexist'] for it in batch])\n        return texts, sentiment\n\n    few_shot_data = texts.map(lambda x: format_text(x))\n    few_shot_data = few_shot_data.select_columns(['text', 'label_sexist'])\n\n    print(few_shot_data['text'][0])\n    data_loader = DataLoader(few_shot_data,\n                             batch_size=1,\n                             shuffle=False,\n                             collate_fn=collate_fn)\n\n\n    return data_loader\n\nTo ensure reproducibility and a fair comparison between the models, we limited the demonstrations to two examples per class, randomly extracted from the demonstrations file. All prompt templates are then formatted using the extracted sentences from the dataset and the same fixed set of demonstrations for consistency across all experiments.\n\ndemonstration_samples = build_few_shot_demonstrations(demonstrations, num_per_class=2)\nprint(demonstration_samples)\n\nTEXT: nothing irks a left wing feminist worse than seeing a beautiful virtuous woman who is conservative they HATE her H-A-T-E so they try to suppress her next it will be illegal to be born beautiful they'll call it \"Hate Birth\"\nANSWER: YES\nTEXT: Reposting this since all the mailboxes were full over the weekend. Please everybody call! #ReleaseTheMemo\nANSWER: NO\nTEXT: It's not old. The women that are 35+ are far more mature, know what they want, and go for it. They don't play the same games and I appreciate them far more for it. Also, they fuck you so much better... no joke.\nANSWER: YES\nTEXT: Greece, African and moslems first stop to the UK.\nANSWER: NO\n\n\n\ndata_loader_fs = prepare_prompts_few_shot(test_data, prompt_template= few_shot_prompt, tokenizer= tokenizer_1, demonstration_samples= demonstration_samples)\n\n\n\n\n&lt;|system|&gt;\nYou are an annotator for sexism detection.&lt;|end|&gt;\n&lt;|user|&gt;\nYour task is to classify input text as containing sexism or not. Respond only YES or NO.\n\n        EXAMPLES:\n        TEXT: nothing irks a left wing feminist worse than seeing a beautiful virtuous woman who is conservative they HATE her H-A-T-E so they try to suppress her next it will be illegal to be born beautiful they'll call it \"Hate Birth\"\nANSWER: YES\nTEXT: Reposting this since all the mailboxes were full over the weekend. Please everybody call! #ReleaseTheMemo\nANSWER: NO\nTEXT: It's not old. The women that are 35+ are far more mature, know what they want, and go for it. They don't play the same games and I appreciate them far more for it. Also, they fuck you so much better... no joke.\nANSWER: YES\nTEXT: Greece, African and moslems first stop to the UK.\nANSWER: NO\n\n        TEXT:\n        The boys must be gaming because there goes the wifi.\n\n        ANSWER:\n        &lt;|end|&gt;\n&lt;|assistant|&gt;\n\n\n\nPerform few-shot inference as in Task 3.\n\nphi_pred_fs = generate_responses(model_1, data_loader_fs)\n\nGenerating responses: 100%|██████████| 300/300 [03:38&lt;00:00,  1.37it/s]\n\n\n\nresponse = tokenizer_1.batch_decode(phi_pred_fs, skip_special_tokens=True)\nraw_response = np.array([extract_response(item) for item in response])\nresponses_df['phi3_few_shot'] = raw_response\n\nCompute metrics as in Task 4.\n\nmetrics_few_shot_phi = compute_metrics(raw_response, ground_truth)\nmetrics['phi3_few_shot'] = metrics_few_shot_phi\n\nprint(f\"\"\"fail_ratio: {metrics_few_shot_phi[\"fail_ratio\"]:.4f},\naccuracy: {metrics_few_shot_phi[\"accuracy_score\"]:.4f}\n\"\"\"\n)\n\nfail_ratio: 0.0000,\naccuracy: 0.6700"
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#notes-3",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#notes-3",
    "title": "Sexism Detection",
    "section": "Notes",
    "text": "Notes\n\nYou are free to pick any value for num_per_class.\nAccording to our tests, few-shot prompting increases inference time by some minutes (we experimented with num_per_class \\(\\in [2, 4]\\))."
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#model-performances",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#model-performances",
    "title": "Sexism Detection",
    "section": "Model performances",
    "text": "Model performances\nLet’s explore the Confusion Matrix and Classification Reports of the models to analyse the errors.\n\ndef plot_results(model_name : str, metrics : dict) -&gt; None:\n\n    \"\"\"\n    This function plots the confusion matrix and classification report of the desired model\n    for both the zero shot and few shot\n\n    Input\n    model_name: the name of the model to plot the metrics of\n    metrics: a dictionary containing the computed metrics\n    \"\"\"\n\n    fig, axs = plt.subplots(2,2, figsize=(10, 5))\n    keys = [key for key in metrics.keys() if model_name in key]\n\n    for idx, key in enumerate(keys):\n        cm = metrics[key]['cm']\n        disp = ConfusionMatrixDisplay(cm)\n        axs[0][idx].set_title(key)\n        disp.plot(ax=axs[0][idx], colorbar=False, cmap='Blues')\n\n        cr = metrics[key]['cr']\n\n        report_df = pd.DataFrame(cr).T\n        report_df.iloc[:, :-1] = report_df.iloc[:, :-1].round(2)\n        report_df.loc[\"accuracy\", [\"precision\", \"recall\"]] = None\n        report_df.loc[\"accuracy\", \"support\"] = 300\n\n        # Create a styled table\n        axs[1][idx].axis(\"off\")\n        table_data = report_df.reset_index()\n        table = axs[1][idx].table(cellText=table_data.values, colLabels=table_data.columns,\n                        cellLoc=\"center\", loc=\"center\", bbox=[0, 0, 1, 1])\n\n        # Style the table\n        table.auto_set_font_size(False)\n        table.set_fontsize(10)\n        table.auto_set_column_width(col=list(range(len(table_data.columns))))\n\n        # Apply styles to headers and rows\n        for (row, col), cell in table.get_celld().items():\n            cell.set_edgecolor(\"black\")\n            if row == 0:\n                cell.set_text_props(weight=\"bold\")\n                cell.set_facecolor(\"#f4f4f4\")\n            elif col == 0:\n                cell.set_text_props(weight=\"bold\")\n                cell.set_facecolor(\"#f4f4f4\")\n            else:\n                value = table_data.values[row - 1, col] if row &gt; 0 else table_data.columns[col]  # Adjust index for header\n                if pd.isna(value):\n                    cell.get_text().set_text(\"\")\n                cell.set_facecolor(\"white\")\n                cell.set_alpha(0.9)\n\n        axs[1][idx].set_title(key, fontsize=14)\n\n    plt.tight_layout()\n    plt.show()\n\nObserving Phi3 confusion matrix and classification report we can note that: - the error are quite balanced in both the zero shot and few shot between the sexist and non sexist label; this is confirmed also by the classification report, indeed the precision and recall on zero shot are basically equal, this is positive since the models seems to not present strong imbalances. - from the accuracy score we can see that the perfomances slightly increase in the few shot, this was expected since providing demonstration should help the model to better classify sentences. Diving deeper we can notice that only 6 sentences more are correctly classified in few shot, with respect to zero shot and all of them are from the label 1. This lead to different values in precision and recall and it could imply that the demonstration sentences, taken randomly, only help the model to understand sexism.\n\nplot_results('phi', metrics)\n\n\n\n\n\n\n\n\nObserving Mistral v0.3 confusion matrix and classification report we can note that: - The errors are highly imbalanced in the zero shot, indeed the model almost predicts all the sentences as sexist. Precision and recall are completely different on label 0, highlighting the fact that almost all sentences have been wrongly classified. In fact, the accuracy is slightly better than the one of the random classifier (0.59). - In few-shot, the performance improves, reaching 0.7 accuracy. This improvement highlights, even more than Phi3, the importance of the examples provided. However, the model remains strongly imbalanced in its predictions, with 81 of the total errors made on label 1. Additionally, we observe a slight decrease in Mistral’s performance on label 1 in few-shot, with the model misclassifying 7 more sentences compared to zero-shot\n\nplot_results('mistral', metrics)\n\n\n\n\n\n\n\n\nAfter analyzing the performance of each model individually, let’s now compare them by focusing on the key metrics.\n\ndef show_performances_table(metrics : dict, save_file : bool = False, file_name : str = None) -&gt; None:\n\n    \"\"\"\n    Shows in table format the metrics contained in the metrics_dict and saved them as a tex table whether required\n\n    Input\n    metrics: dictionary containing the metrics to show in table format\n    save_file: boolean attribute, specify whther the table will be saved in your local environment\n    file_name: the name of the file to be saved with\n    \"\"\"\n\n    if save_file and file_name is None:\n        raise ValueError(\"file_name must be specified if save_file is setted to True\")\n\n    performances = {'accuracy_score':[], 'fail_ratio':[], 'f1-score':[]}\n\n    for key in metrics.keys():\n      performances['accuracy_score'].append(metrics[key]['accuracy_score'])\n      performances['fail_ratio'].append(metrics[key]['fail_ratio'])\n      for k in metrics[key]['cr']['macro avg'].keys():\n        if k in performances:\n          performances[k].append(metrics[key]['cr']['weighted avg'][k])\n\n    df = pd.DataFrame.from_dict(performances, orient='index', columns=metrics.keys())\n    df = df.map(lambda x: f\"{x:.3f}\" if isinstance(x, (int, float)) else x)\n    df = df.T\n    print(tabulate(df, headers=df.columns, tablefmt=\"fancy_grid\"))\n\n    if save_file:\n      latex_table = tabulate(df, headers=df.columns, tablefmt=\"latex\")\n      with open(f\"{file_name}.tex\", \"w\") as f:\n        f.write(latex_table)\n\n\nshow_performances_table(metrics, save_file= True, file_name= 'overall_results')\n\n╒═══════════════════╤══════════════════╤══════════════╤════════════╕\n│                   │   accuracy_score │   fail_ratio │   f1-score │\n╞═══════════════════╪══════════════════╪══════════════╪════════════╡\n│ phi3_zero_shot    │            0.643 │            0 │      0.642 │\n├───────────────────┼──────────────────┼──────────────┼────────────┤\n│ phi3_few_shot     │            0.67  │            0 │      0.668 │\n├───────────────────┼──────────────────┼──────────────┼────────────┤\n│ mistral_zero_shot │            0.59  │            0 │      0.516 │\n├───────────────────┼──────────────────┼──────────────┼────────────┤\n│ mistral_few_shot  │            0.697 │            0 │      0.679 │\n╘═══════════════════╧══════════════════╧══════════════╧════════════╛\n\n\nBy presenting the main metrics of the models in a table, we observe that all of them achieve the maximum possible score in fail_ratio, as none of the models produce incorrect answers.\nPhi3 performs better in the zero-shot setting, but this may be due to the significant imbalance in Mistral’s predictions, as previously discussed. However, in the few-shot setting, Mistral shows a significant improvement, surpassing Phi3. This improvement could be attributed to Mistral’s larger size, with 7B parameters compared to 3.8B in Phi3. We hypothesize that the higher number of parameters helps Mistral better leverage the provided examples, resulting in improved few-shot performance.\n\nFurther analysis\nTo better understand the factors that lead the model to make errors, we conduct two explorations: - Since we limit the model’s token generation to 100 tokens and just one sentence, it is possible that some answers are followed by explanations. Including these explanations could provide a clearer understanding of the errors. We analyse whether such answers exist and identify which of them are misclassified. - We decide to plot all the words that appear more frequently in misclassified sentences. This is done to explore whether certain words are common across the models and whether they carry significant meaning for our task. Stop words are excluded to avoid misleading comparisons.\n\n\nWrong answers Exploration\nWe decide to explore all those misclassified sentences whose answer is followed by an explanation.\nThe output is structured in the following way:\n\nfirst line: the sentence misclassified\nsecond line: the ground truth label\nthird line: the answer of the model\n\n\ndef display_sentence_explained(\n                    predictions_df : pd.DataFrame,\n                    model_name: str) -&gt; None:\n    \"\"\"\n      Displays misclassified sentences with explanations.\n\n    Input:\n    predictions_df: a DataFrame containing the sentences, predictions and labels. (pd.DataFrame)\n    model_name: name of the model for the plot title (str)\n    \"\"\"\n\n    def map_response(value):\n        if value.startswith('YES'):\n            return 1\n        elif value.startswith('NO'):\n            return 0\n\n    predictions_df[f'{model_name}_mapped'] = predictions_df[model_name].apply(map_response)\n    predictions_df = predictions_df[~predictions_df[model_name].isin(['YES', 'NO'])]\n    predictions_df = predictions_df[predictions_df['label_sexist'] != predictions_df[f'{model_name}_mapped']].reset_index(drop=True)\n    if len(predictions_df) == 0:\n      print('No answer with explanation given')\n    for rows in predictions_df.iterrows():\n        true_label = 'YES' if rows[1][\"label_sexist\"] == 1 else 'NO'\n        print(f'{rows[1][\"text\"]} \\nground truth: {true_label} \\nmodel pred: {rows[1][model_name]}')\n        print('='*80)\n\n\n\nPhi3\nAs we can see below, Phi3 model never gives an answer followed by an explanation, no analysis can be performed.\n\ndisplay_sentence_explained(responses_df.copy(), model_name= 'phi3_zero_shot')\n\nNo answer with explanation given\n\n\n\ndisplay_sentence_explained(responses_df.copy(), model_name= 'phi3_few_shot')\n\nNo answer with explanation given\n\n\n\n\nMistral\nLooking at the answers given by Mistral, and the queried sentences, we notice that all the sentences are quite difficult to understand even for a human. Indeed, the explanation given is not completly wrong, but it seems like the model is not able to catch up the undertone meaning of the sentences.\n\ndisplay_sentence_explained(responses_df.copy(), model_name= 'mistral_zero_shot')\n\nTell that to Weev you lying shitskin son of a bitch. \nground truth: YES \nmodel pred: NO (The text does not contain sexism. It is an insulting and offensive language, but it does not specifically target a person based on their gender.)\n================================================================================\n\n\n\ndisplay_sentence_explained(responses_df.copy(), model_name= 'mistral_few_shot')\n\nGood - the grid girls can finally get back into F1 \nground truth: NO \nmodel pred: YES (This statement objectifies women and reduces them to their physical appearance, which is a form of sexism.)\n================================================================================\nThe new Europeans are into all kinds of sex.... \nground truth: NO \nmodel pred: YES (This statement seems to stereotype a group of people based on their nationality, which could be considered as a form of ethnic prejudice, not necessarily sexism, but it's important to note that ethnic prejudice can intersect with sexism.)\n================================================================================\nThis is about false rape charges and how wives use the laws to torture and get money from the brides family.....highly recommend to watch full video ....its mind blowing \nground truth: NO \nmodel pred: YES (The text seems to stereotype all wives, implying that they are prone to false rape charges and using the law to torture and extort money, which is a generalization and potentially harmful towards women.)\n================================================================================\nMy state? I'm not Jewish. They literally won't stop until they destroy Israel and kill every Jewish person. Have you even read the Qur'an? It was created to give them a sanctuary if Nazi Germany type countries ever rose up again. \nground truth: NO \nmodel pred: YES (The text contains a stereotype and generalization about a religious group, which can be considered as a form of prejudice, not necessarily sexism.)\n================================================================================\n[URL] Obviously written by an incel \nground truth: NO \nmodel pred: YES (Assuming the URL leads to a text that contains sexist content)\n================================================================================\nDamn...this pisses me off to no end! These mother fuckers need to start gracing the halls of maximum security federal facilities! \nground truth: NO \nmodel pred: YES (The text uses derogatory language and shows a generalized anger towards an unspecified group, which could be interpreted as a form of prejudice or discrimination, although not explicitly sexist.)\n================================================================================\nKarma's a bitch! \nground truth: YES \nmodel pred: NO (The text does not contain sexism as it is a common idiomatic expression and does not target a specific gender.)\n================================================================================\n..oh Obama, dont even start with this kenyan hominid. I always pondered..His mother is a jew, he claims to be muslim....and fagg? yet jews claimed him as his own...oh about faggs...Obama put in risk lots of people with his \"Laws\" and \"freedom\". \nground truth: NO \nmodel pred: YES (The text contains derogatory language towards multiple groups, including a racial slur and homophobic slur, which can be considered as hate speech.)\n================================================================================\nFull on crotch grab from Biden. Even the baby's mom isn't holding him that way. \nground truth: NO \nmodel pred: YES (This statement implies a disrespectful and objectifying comment towards the woman, which can be considered sexist.)\n================================================================================\nI would love to suck her tits \nground truth: NO \nmodel pred: YES (This statement objectifies a woman and is disrespectful, therefore it is sexist.)\n================================================================================\nI hate her. \nground truth: NO \nmodel pred: YES (The text does not specify the gender of the person being hated, but the use of the generic \"her\" could be interpreted as sexist if it's used in a derogatory or disrespectful manner without a clear context)\n================================================================================\nRrrrrr im going to fuck u in the ass for this \nground truth: NO \nmodel pred: YES (This text contains sexist and violent language)\n================================================================================\nI try to act confident but girls dont even look at me \nground truth: NO \nmodel pred: YES (This statement implies a generalization about all girls, which can be considered sexist.)\n================================================================================"
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#word-exploration",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#word-exploration",
    "title": "Sexism Detection",
    "section": "Word exploration",
    "text": "Word exploration\nLet’s explore whether any error patterns correlated with specific words emerge from the analysis.\n\ndef plot_wrong_word(predictions_df: pd.DataFrame,\n                    model_name: str,\n                    top_wrongest: int = 20) -&gt; None:\n\n    \"\"\"\n    Shows the most frequent words in wrong classified sentences.\n\n    Input:\n    predictions_df: a DataFrame containing the sentences, predictions and labels. (pd.DataFrame)\n    model_name: name of the model for the plot title (str)\n    top_wrongest: number of words to show (int)\n    \"\"\"\n\n    def find_wrongest(df: pd.DataFrame, label: int, ax: matplotlib.axes.Axes) -&gt; None:\n        df = df[df[\"label_sexist\"] == label]\n        all_words = Counter(\n            word for row in df[\"text\"] for word in row.split() if word not in stop_words\n        )\n        df_wrong = df[df['label_sexist'] != df[model_name]]\n        wrongest_words = Counter(\n            word for row in df_wrong[\"text\"] for word in row.split() if word not in stop_words\n        )\n\n        wr_words, wr_counts = zip(*wrongest_words.most_common(top_wrongest)[:0:-1])\n        ax.barh(wr_words, wr_counts, color=\"skyblue\", label='Wrong Sentences')\n        counts = [all_words[word] for word in wr_words]\n        ax.barh(wr_words, counts, color=\"skyblue\",alpha=0.4, label='All Sentences')\n        ax.set_title(f\"Wrongest words in label {label}\")\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        ax.legend()\n\n    def map_response(value):\n        if value.startswith('YES'):\n            return 1\n        elif value.startswith('NO'):\n            return 0\n\n    predictions_df[f'{model_name}'] = predictions_df[model_name].apply(map_response)\n    stop_words = list(stopwords.words(\"english\"))\n\n    fig, axs = plt.subplots(1,2, figsize=(10,5))\n    fig.suptitle(f'{model_name} charts', weight=\"bold\")\n    find_wrongest(predictions_df, 0, axs[0])\n    find_wrongest(predictions_df, 1, axs[1])\n    plt.tight_layout()\n    plt.show()\n\n\nPhi3\nIn the zero-shot setting, the model appears to show a possible bias towards associating terms like “women” and “woman” with errors in label 0 (non-sexist).\n\nplot_wrong_word(responses_df.copy(), model_name= 'phi3_zero_shot')\n\n\n\n\n\n\n\n\nIn the few-shot setting certain patterns seem to persist, particularly for label 0 errors. This could indicate that the examples provided during few-shot prompting, although helpful, may not fully address the model’s tendency to misclassify based on specific terms.\n\nplot_wrong_word(responses_df.copy(), model_name= 'phi3_few_shot')\n\n\n\n\n\n\n\n\n\n\nMistral\nFrom Mistral zero shot word exploration we can see: - In label 0 all the woman-related occurrences (women, girls, woman, female) have almost a 100% wrong ratio. But, remembering the confusion matrix above, this result is not so meaningful, since almost all the sentences are wrongly classified. We can hypotesise that almost all the words have a 100% wrong ratio. - In label 1, the number of misclassified sentences is very low (only 3), which makes it difficult to draw meaningful conclusions or identify clear patterns for this category.\n\nplot_wrong_word(responses_df.copy(), model_name= 'mistral_zero_shot')\n\n\n\n\n\n\n\n\nFrom Mistral few shot word exploration we can see: - in label 0 the words that are most frequently misclassified include the woman-related ones (women, woman, girls, female). This could highlight some biases in the model that almost always associate a woman-related word to sexism. The presence of terms like “fat” and “white” also raises questions about the potential biases in the model, as these terms could be linked to specific stereotypes or contexts the model misinterprets. - in label 1 the diversity of terms could suggest a broader range of possible contexts or tasks where the model struggles, but thinking about the confusion matrix this could be probably related to the low number of errors (10).\n\nplot_wrong_word(responses_df.copy(), model_name= 'mistral_few_shot')"
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#few-shot-experiments",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#few-shot-experiments",
    "title": "Sexism Detection",
    "section": "Few Shot Experiments",
    "text": "Few Shot Experiments\nWe would like to analyse how much the predictions of the models, and consequently their performances, were correlated with the examples injected through the model prompt.\n\ndef plot_variability(mode : str, demonstration_list : list = [], num_per_class : list = [], to_save : bool = True ) -&gt; None:\n\n    \"\"\"\n    Computes the metrics with different examples per classes.\n\n    Input:\n    mode: whether we are computing the quality or quantity variability\n    demonstration_list: the list containting the demonstrations to be used\n    num_per_class: a list containing the number of elements per class to use in the few shot prompting, to use whether demonstration_list is empty\n    to_save: boolean attribute, specify whther the table will be saved in your local environment\n    \"\"\"\n    if demonstration_list == [] and num_per_class == []:\n      assert ValueError('At least one between demonstration_list and num_per_class should be not default')\n\n    n_experiments = len(demonstration_list) if demonstration_list != [] else len(num_per_class)\n\n    def compute_metric_list(model, tokenizer, ax, model_name):\n\n        metric_list = {'fail_ratio' : [], 'accuracy' : [], 'examples' : []}\n\n        for n_example in tqdm(range(n_experiments), desc=\"Generating responses\"):\n\n            with capture_output():\n                if demonstration_list != []:\n                  data_loader = prepare_prompts_few_shot(test_data, prompt_template= few_shot_prompt,\n                                                         tokenizer= tokenizer, demonstration_samples= demonstration_list[n_example])\n                  metric_list['examples'].append(demonstration_list[n_example])\n                else:\n                  data_loader = prepare_prompts_few_shot(test_data, prompt_template= few_shot_prompt,\n                                                         tokenizer= tokenizer, num_per_class = num_per_class[n_example])\n                y_pred = generate_responses(model, data_loader)\n\n            response = tokenizer.batch_decode(y_pred, skip_special_tokens=True)\n            raw_response = np.array([extract_response(item) for item in response])\n\n            metrics = compute_metrics(raw_response, ground_truth)\n            metric_list['accuracy'].append(metrics[\"accuracy_score\"])\n            metric_list['fail_ratio'].append(metrics[\"fail_ratio\"])\n\n\n            #json files checkpoints\n            if to_save:\n              if model == model_1:\n                  if mode == 'quality':\n                      with open(f\"metric_list_quality_phi_{n_example}.json\", \"w\") as json_file:\n                          json.dump(metric_list, json_file, indent=4)\n                  else:\n                      with open(f\"metric_list_quantity_phi_{n_example}.json\", \"w\") as json_file:\n                          json.dump(metric_list, json_file, indent=4)\n              else:\n                  if mode == 'quality':\n                      with open(f\"metric_list_quality_mistral_{n_example}.json\", \"w\") as json_file:\n                          json.dump(metric_list, json_file, indent=4)\n                  else:\n                      with open(f\"metric_list_quantity_mistral_{n_example}.json\", \"w\") as json_file:\n                          json.dump(metric_list, json_file, indent=4)\n\n\n        bars = ax.bar(range(1, n_experiments+1), metric_list['accuracy'])\n        x = np.arange(1, len(metric_list['accuracy']) + 1)\n        for bar in bars:\n            yval = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n        if mode == 'quality':\n            ax.set_xticks([])\n            ax.set_xlabel('Experiments')\n        else:\n            ax.set_xticks(np.arange(1, n_experiments+1, 1))\n            ax.set_xlabel('Number of examples')\n        ax.set_yticks(np.arange(0, 1.1, 0.1))\n        ax.set_ylabel('Accuracy score')\n        ax.set_title(f'{model_name}')\n\n    fig, axs = plt.subplots(1, 2, figsize= (12, 5))\n\n    if mode == 'quality':\n        plt.suptitle(f'Accuracy variability choosing 2 different examples per class')\n    else:\n        plt.suptitle(f'Accuracy variability increasing the number of examples per class')\n\n    compute_metric_list(model_1, tokenizer_1, axs[0], 'Phi3')\n    compute_metric_list(model_2, tokenizer_2, axs[1], 'Mistral')\n\n    plt.show()\n\n\nQualitative Analysis\nThe first analysis investigates how the model’s performance is influenced by the examples provided in the few-shot setup. In this experiment, the model generates predictions ten times, using a different set of examples in the prompt for each run. In each case, two examples per class are randomly selected and remain consistent throughout that specific run. This approach allows us to observe how the choice of examples impacts the model’s output.\nFrom the plot below, we can observe that for both models, their performance fluctuates significantly depending on the examples given as input. This suggests that the choice of few-shot examples can heavily influence the model’s predictions, highlighting the sensitivity of the few-shot approach to input selection.\nSuch variability underscores the importance of carefully curating examples for few-shot learning tasks to ensure robust and consistent performance.\n\nn_experiments = 10\ndemonstration_quality_var = [build_few_shot_demonstrations(demonstrations, num_per_class= 2) for idx in range(n_experiments)]\n\nplot_variability(mode= 'quality', demonstration_list= demonstration_quality_var)\n\nGenerating responses: 100%|██████████| 10/10 [34:03&lt;00:00, 204.31s/it]\nGenerating responses: 100%|██████████| 10/10 [1:12:14&lt;00:00, 433.48s/it]\n\n\n\n\n\n\n\n\n\nTo address this issue, we decide to modify the examples provided in each prompt by randomly selecting them during each run. This approach ensures that within a single run, each of the 300 prompts contains a different set of examples, resulting in a total of 1,200 unique examples sampled from the demonstration dataset. By introducing this variability, we aim to reduce the model’s sensitivity to specific fixed examples. We conduct 10 separate experiments to maintain consistency with the previous analysis. This experimental setup allows us to make more robust comparisons and avoid drawing false conclusions based on isolated runs.\nFrom the plots below, it is evident that, although some degree of variability in performance persists, the fluctuations are significantly reduced compared to when using a fixed set of examples across all prompts. This indicates that varying the examples contributes to a more stable model performance and reduces its sensitivity to specific demonstrations.\nFurthermore, it is important to note that none of the trials achieve peak performance. This observation suggests that the choice of examples is central in few-shot prompting, as the examples directly influence the model’s ability to generalize and adapt to the task. To achieve the best possible results, examples should be carefully selected.\nBased on these findings, we decide to consistently adopt the strategy of varying the examples in subsequent experiments.\n\nn_experiments= 10\n\nplot_variability(mode= 'quality', num_per_class= [2]*n_experiments, to_save = False)\n\nGenerating responses: 100%|██████████| 10/10 [34:57&lt;00:00, 209.75s/it]\nGenerating responses: 100%|██████████| 10/10 [1:12:39&lt;00:00, 436.00s/it]\n\n\n\n\n\n\n\n\n\n\n\nQuantitative Analysis\nWe also conducted experiments to investigate the impact of varying the number of examples per class on model performance. By testing a range spanning from 1 to 5 examples per class, we observed that there was no clear correlation between the number of examples and the overall accuracy. Interestingly, the results suggest that using two or three examples per class tends to yield the most stable and optimal performance.\nThis finding indicates that beyond a certain point, increasing the number of examples may not significantly enhance accuracy and could even introduce diminishing returns in performance. Therefore, selecting two or three examples per class appears to strike the right balance between providing sufficient context and avoiding unnecessary complexity in the prompt.\n\nn_experiments = 5\nnum_per_class_list= list(range(1, n_experiments+1))\n\nplot_variability(mode= 'quantity', num_per_class= num_per_class_list)\n\nGenerating responses: 100%|██████████| 5/5 [20:04&lt;00:00, 240.92s/it]\nGenerating responses: 100%|██████████| 5/5 [39:57&lt;00:00, 479.44s/it]"
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#cot",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#cot",
    "title": "Sexism Detection",
    "section": "CoT",
    "text": "CoT\nWe decide to explore the Chain of Thought technique. Our objective is to guide the model reasoning trying to increase its performances. We try this technique for both Phi3 and Mistral v3.\nWe add some instruction in the prompt template, giving some general reasoning tools to classify the sentences. Due to the high variability related to the examples found out in the previous section, we decide to not give the model specific examples with explanation, but just some general rules.\n\nprompt_cot = [\n    {\n        'role': 'system',\n        'content': 'You are an annotator for sexism detection.'\n    },\n    {\n        'role': 'user',\n        'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n        Think step by step. A sentence is more likely to contain sexism when:\n        A woman is associated to an object; a person is discriminated on the basis of its sex; a person, especially a woman, is denigrated due to sexual\n        comments.\n        TEXT:\n        {text}\n\n        ANSWER:\n        \"\"\"\n    }\n]\n\nIn the following sections we introduce the full_pipeline function which avoids multiple repetitions of the same code.\n\ndef full_pipeline(prompt : str, prompting_technique_name : str, performances : dict) -&gt; dict:\n\n    \"\"\"\n    Implements the full pipeline from the preparation of the prompts to the metrics saving.\n\n    Input:\n    prompt: the type of prompt\n    prompting_technique_name: the name of the prompting technique\n    performances: dictionary in which store the computed metrics\n    \"\"\"\n    for model, tokenizer in zip((model_1, model_2), (tokenizer_1, tokenizer_2)):\n\n        model_name = 'Phi3' if model == model_1 else 'Mistral'\n\n        if prompting_technique_name == 'few_shot':\n            data_loader = prepare_prompts_few_shot(test_data, prompt, tokenizer, num_per_class = 2)\n        else:\n            data_loader = prepare_prompts(test_data, prompt, tokenizer)\n\n        model_response = generate_responses(model, data_loader)\n        model_response = tokenizer.batch_decode(model_response, skip_special_tokens=True)\n        model_response = [extract_response(item) for item in model_response]\n\n        performances[f'{model_name}_{prompting_technique_name}'] = compute_metrics(model_response, ground_truth)\n\n    return performances\n\n\nperformances = full_pipeline(prompt_cot, 'CoT', {})\n\n\n\n\n&lt;|system|&gt;\nYou are an annotator for sexism detection.&lt;|end|&gt;\n&lt;|user|&gt;\nYour task is to classify input text as containing sexism or not. Respond only YES or NO.\n        Think step by step. A sentence is more likely to contain sexism when:\n        A woman is associated to an object; a person is discriminated on the basis of its sex; a person, especially a woman, is denigrated due to sexual\n        comments.\n        TEXT:\n        The boys must be gaming because there goes the wifi.\n\n        ANSWER:\n        &lt;|end|&gt;\n&lt;|assistant|&gt;\n\n\n\nGenerating responses: 100%|██████████| 300/300 [02:32&lt;00:00,  1.97it/s]\n\n\n\n\n\n&lt;s&gt;[INST] You are an annotator for sexism detection.\n\nYour task is to classify input text as containing sexism or not. Respond only YES or NO.\n        Think step by step. A sentence is more likely to contain sexism when:\n        A woman is associated to an object; a person is discriminated on the basis of its sex; a person, especially a woman, is denigrated due to sexual\n        comments.\n        TEXT:\n        The boys must be gaming because there goes the wifi.\n\n        ANSWER:\n        [/INST]\n\n\nGenerating responses: 100%|██████████| 300/300 [04:33&lt;00:00,  1.10it/s]\n\n\n\nshow_performances_table(performances, save_file= True, file_name= 'CoT_results')\n\n╒═════════════╤══════════════════╤══════════════╤════════════╕\n│             │   accuracy_score │   fail_ratio │   f1-score │\n╞═════════════╪══════════════════╪══════════════╪════════════╡\n│ Phi3_CoT    │            0.73  │            0 │      0.723 │\n├─────────────┼──────────────────┼──────────────┼────────────┤\n│ Mistral_CoT │            0.647 │            0 │      0.608 │\n╘═════════════╧══════════════════╧══════════════╧════════════╛\n\n\nPhi3 exhibits a more substantial performance boost with this technique, achieving results that even surpass its few-shot performance. In contrast, Mistral does not exhibit similar improvements, as its performance remains lower than that of its few-shot implementation. Specifically, when randomly selecting different examples per class in the few-shot setup, Mistral achieves an average accuracy of 0.7, which it fails to exceed with the current approach."
  },
  {
    "objectID": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#a1-dataset",
    "href": "assignment_2/assignment_2_baiocchi_dibuo_petrilli.html#a1-dataset",
    "title": "Sexism Detection",
    "section": "A1 dataset",
    "text": "A1 dataset\nIn this subsection, our goal is to evaluate the two models using the dataset provided in Assignment 1. To achieve this, we use the test set to query the models while utilizing the training set as the demonstration set for few-shot prompting. The models are evaluated using the three prompting techniques discussed earlier.\n\nurl = 'https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/'\n\ntest_df = pd.read_json(url + 'test.json').T\ntrain_df = pd.read_json(url + 'training.json').T\n\nWe preprocess the dataset as follows: - we keep only the english tweets - we remove the emoticons, urls, users mentions and special charcaters\n\ndef determine_label(row: pd.Series) -&gt; str:\n\n    \"\"\"\n    Converts soft labels into hard labels (strings).\n\n    :param row: row of the dataset to convert (pd.Series)\n\n    :return\n        - string representing the hard label\n    \"\"\"\n\n    num_yes = row[\"labels_task1\"].count(\"YES\")\n    num_no = row[\"labels_task1\"].count(\"NO\")\n    if num_yes == num_no:\n        return None\n    return \"YES\" if num_yes &gt; num_no else \"NO\"\n\ndef corpus(df: pd.DataFrame, multilingual:bool = False) -&gt; pd.DataFrame:\n\n    \"\"\"\n    Converts soft labels into hard labels (int) and drops irrelevant rows/columns.\n\n    Input:\n    df: dataset split to convert (pd.DataFrame)\n    multilingual: whether to keep spanish tweets (bool)\n\n    Output:\n        - refined dataset\n    \"\"\"\n\n    df[\"hard_label_task1\"] = df.apply(determine_label, axis=1)\n    df = df.dropna(subset=[\"hard_label_task1\"])\n    if not multilingual:\n      df = df[df[\"lang\"] != \"es\"]\n    df = df[[\"id_EXIST\", \"lang\", \"tweet\", \"hard_label_task1\"]]\n    df.loc[df[\"hard_label_task1\"] == \"YES\", \"hard_label_task1\"] = 1\n    df.loc[df[\"hard_label_task1\"] == \"NO\", \"hard_label_task1\"] = 0\n    df.reset_index(drop=True, inplace=True)\n    return df\n\ntest_df = corpus(test_df)\ntrain_df = corpus(train_df)\n\n\nurl = 'https://raw.githubusercontent.com/muan/unicode-emoji-json/main/data-by-emoji.json'\nresponse = requests.get(url)\njson_data = response.json()\n\n\nemoji_list = list(json_data.keys())\nregex_pattern = \"|\".join(re.escape(word) for word in emoji_list)\nregex = re.compile(regex_pattern)\n\n\ndef data_cleaning(tweet: str) -&gt; str:\n    tweet = re.sub(regex.pattern,' ',tweet)\n    tweet = re.sub(r'\\B\\@(\\w)+',' ',tweet)\n    #tweet = re.sub(r'\\B\\#(\\w)+',' ',tweet)\n    tweet = re.sub(r'https?://\\S*', ' ', tweet)\n    tweet = re.sub(r\"[^a-zA-Z]\", ' ', tweet)\n    tweet = tweet.lower()\n    return tweet\n\ntest_df['tweet'] = test_df['tweet'].apply(data_cleaning)\ntrain_df['tweet'] = train_df['tweet'].apply(data_cleaning)\n\n\ntest_df = test_df.rename(columns={'tweet' : 'text', 'hard_label_task1' : 'label_sexist'})\ntrain_df = train_df.rename(columns={'tweet' : 'text', 'hard_label_task1' : 'label_sexist'})\ntest_df.head()\n\n\n\n\n\n\n\n\nid_EXIST\nlang\ntext\nlabel_sexist\n\n\n\n\n0\n400178\nen\nst day at the pool on a beautiful sunday in n...\n0\n\n\n1\n400179\nen\ni like your outfit too except when i dress up...\n1\n\n\n2\n400180\nen\nsame though the angst just comes and ...\n0\n\n\n3\n400181\nen\nfuck that cunt tried to vote her out mult...\n1\n\n\n4\n400182\nen\nu gotta say some shit like i ll fuck that c...\n1\n\n\n\n\n\n\n\n\ntrain_df['label_sexist'] = train_df['label_sexist'].map(lambda x : 'YES' if x == 1 else 'NO')\n\n\ntest_data = Dataset.from_pandas(test_df)\nground_truth = np.array(test_data['label_sexist'])\nmetrics = {'zero_shot' : [], 'few_shot' : [], 'CoT' : []}\n\n\nZero shot prompting\n\nperformances = full_pipeline(zero_shot_prompt, 'zero_shot', {})\n\n\n\n\n&lt;|system|&gt;\nYou are an annotator for sexism detection.&lt;|end|&gt;\n&lt;|user|&gt;\nYour task is to classify input text as containing sexism or not. Respond only YES or NO.\n\n        TEXT:\n         st day at the pool on a beautiful sunday in ny  too bad few dared to go in due to the frigid water temp  we lasted    min and figured we proved our point     \n\n        ANSWER:\n        &lt;|end|&gt;\n&lt;|assistant|&gt;\n\n\n\nGenerating responses: 100%|██████████| 286/286 [02:11&lt;00:00,  2.18it/s]\n\n\n\n\n\n&lt;s&gt;[INST] You are an annotator for sexism detection.\n\nYour task is to classify input text as containing sexism or not. Respond only YES or NO.\n\n        TEXT:\n         st day at the pool on a beautiful sunday in ny  too bad few dared to go in due to the frigid water temp  we lasted    min and figured we proved our point     \n\n        ANSWER:\n        [/INST]\n\n\nGenerating responses: 100%|██████████| 286/286 [03:36&lt;00:00,  1.32it/s]\n\n\n\n\nFew Shot prompting\n\nperformances = full_pipeline(few_shot_prompt, 'few_shot', performances)\n\n\n\n\n&lt;|system|&gt;\nYou are an annotator for sexism detection.&lt;|end|&gt;\n&lt;|user|&gt;\nYour task is to classify input text as containing sexism or not. Respond only YES or NO.\n\n        EXAMPLES:\n        TEXT: So you are mainly attracted to Asian women who have white fever and 10 ex white boyfriends?\nANSWER: YES\nTEXT: Now dat rubber rounds are badass they will mame dat shithead for life possibly make him / her disabled\nANSWER: NO\nTEXT: No way that dumb bitch could put a sentence like that together... Someone else wrote it, she memorized it...\nANSWER: YES\nTEXT: Yep, in modern America when you molest little boys, you get a more lenient sentence (none) than stating verifiable facts about the holocaust in most of Europe. This is so something to celebrate. 👏. 👏. 👏.\nANSWER: NO\n\n        TEXT:\n         st day at the pool on a beautiful sunday in ny  too bad few dared to go in due to the frigid water temp  we lasted    min and figured we proved our point     \n\n        ANSWER:\n        &lt;|end|&gt;\n&lt;|assistant|&gt;\n\n\n\nGenerating responses: 100%|██████████| 286/286 [03:18&lt;00:00,  1.44it/s]\n\n\n\n\n\n&lt;s&gt;[INST] You are an annotator for sexism detection.\n\nYour task is to classify input text as containing sexism or not. Respond only YES or NO.\n\n        EXAMPLES:\n        TEXT: The SMF Social media female. Everything this type of female does is for her social media.\nANSWER: YES\nTEXT: ROFLMAO.... She's a little thing so ducking to her level is ill advised\nANSWER: NO\nTEXT: Yeah with Mgtow Day approaching (Feb 14) I already women being childish, it'll create a surge for sure.\nANSWER: YES\nTEXT: Don't mess with granny!\nANSWER: NO\n\n        TEXT:\n         st day at the pool on a beautiful sunday in ny  too bad few dared to go in due to the frigid water temp  we lasted    min and figured we proved our point     \n\n        ANSWER:\n        [/INST]\n\n\nGenerating responses: 100%|██████████| 286/286 [06:37&lt;00:00,  1.39s/it]\n\n\n\n\nChain of Thought prompting\n\nperformances = full_pipeline(prompt_cot, 'CoT', performances)\n\n\n\n\n&lt;|system|&gt;\nYou are an annotator for sexism detection.&lt;|end|&gt;\n&lt;|user|&gt;\nYour task is to classify input text as containing sexism or not. Respond only YES or NO.\n        Think step by step. A sentence is more likely to contain sexism when:\n        A woman is associated to an object; a person is discriminated on the basis of its sex; a person, especially a woman, is denigrated due to sexual\n        comments.\n        TEXT:\n         st day at the pool on a beautiful sunday in ny  too bad few dared to go in due to the frigid water temp  we lasted    min and figured we proved our point     \n\n        ANSWER:\n        &lt;|end|&gt;\n&lt;|assistant|&gt;\n\n\n\nGenerating responses: 100%|██████████| 286/286 [02:25&lt;00:00,  1.96it/s]\n\n\n\n\n\n&lt;s&gt;[INST] You are an annotator for sexism detection.\n\nYour task is to classify input text as containing sexism or not. Respond only YES or NO.\n        Think step by step. A sentence is more likely to contain sexism when:\n        A woman is associated to an object; a person is discriminated on the basis of its sex; a person, especially a woman, is denigrated due to sexual\n        comments.\n        TEXT:\n         st day at the pool on a beautiful sunday in ny  too bad few dared to go in due to the frigid water temp  we lasted    min and figured we proved our point     \n\n        ANSWER:\n        [/INST]\n\n\nGenerating responses: 100%|██████████| 286/286 [04:08&lt;00:00,  1.15it/s]\n\n\n\n\nResults\n\nshow_performances_table(performances, save_file= True, file_name= 'Results_A1_assignment')\n\n╒═══════════════════╤══════════════════╤══════════════╤════════════╕\n│                   │   accuracy_score │   fail_ratio │   f1-score │\n╞═══════════════════╪══════════════════╪══════════════╪════════════╡\n│ Phi3_zero_shot    │            0.72  │            0 │      0.721 │\n├───────────────────┼──────────────────┼──────────────┼────────────┤\n│ Mistral_zero_shot │            0.759 │            0 │      0.757 │\n├───────────────────┼──────────────────┼──────────────┼────────────┤\n│ Phi3_few_shot     │            0.699 │            0 │      0.684 │\n├───────────────────┼──────────────────┼──────────────┼────────────┤\n│ Mistral_few_shot  │            0.801 │            0 │      0.799 │\n├───────────────────┼──────────────────┼──────────────┼────────────┤\n│ Phi3_CoT          │            0.766 │            0 │      0.765 │\n├───────────────────┼──────────────────┼──────────────┼────────────┤\n│ Mistral_CoT       │            0.79  │            0 │      0.791 │\n╘═══════════════════╧══════════════════╧══════════════╧════════════╛\n\n\nThe Mistral_few_shot model achieves the highest accuracy at 0.801, outperforming all other models. The Phi3_few_shot model has the lowest accuracy at 0.699, suggesting that it may be less effective for the task when compared to Mistral-based models. Models that involve zero-shot or CoT techniques (e.g., Phi3_zero_shot and Mistral_CoT) show accuracy in the mid-range, with Mistral-based models generally outperforming Phi3-based ones.\nAll models report a fail ratio of 0, meaning no model experienced failure during the task. This is an important indicator that the models are stable and did not encounter issues like errors or incomplete predictions during evaluation.\nThe Mistral-based models (both zero_shot, few_shot, and CoT) tend to outperform the Phi3-based models across all three metrics (accuracy, fail ratio, f1-score). Few-shot learning appears to provide the best performance across the board, suggesting that the few-shot approach is particularly effective for these tasks."
  },
  {
    "objectID": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html",
    "href": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html",
    "title": "Sexism Detection",
    "section": "",
    "text": "Credits: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\nKeywords: Sexism Detection, Multi-class Classification, RNNs, Transformers, Huggingface"
  },
  {
    "objectID": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#problem-definition",
    "href": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#problem-definition",
    "title": "Sexism Detection",
    "section": "Problem Definition",
    "text": "Problem Definition\nThe systems have to decide whether or not a given tweet contains or describes sexist expressions or behaviors (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behavior).\n\nExamples:\nText: Can’t go a day without women womening\nLabel: Sexist\nText: ‘’Society’s set norms! Happy men’s day though!#weareequal’’\nLabel: Not sexist\n#[Task 1 - 1.0 points] Corpus\nWe have preparared a small version of EXIST dataset in our dedicated Github repository.\nCheck the A1/data folder. It contains 3 .json files representing training, validation and test sets.\nThe three sets are slightly unbalanced, with a bias toward the Non-sexist class.\n\n\nDataset Description\n\nThe dataset contains tweets in both English and Spanish.\nThere are labels for multiple tasks, but we are focusing on Task 1.\nFor Task 1, soft labels are assigned by six annotators.\nThe labels for Task 1 represent whether the tweet is sexist (“YES”) or not (“NO”).\n\n\n\nExample\n\"203260\": {\n    \"id_EXIST\": \"203260\",\n    \"lang\": \"en\",\n    \"tweet\": \"ik when mandy says “you look like a whore” i look cute as FUCK\",\n    \"number_annotators\": 6,\n    \"annotators\": [\"Annotator_473\", \"Annotator_474\", \"Annotator_475\", \"Annotator_476\", \"Annotator_477\", \"Annotator_27\"],\n    \"gender_annotators\": [\"F\", \"F\", \"M\", \"M\", \"M\", \"F\"],\n    \"age_annotators\": [\"18-22\", \"23-45\", \"18-22\", \"23-45\", \"46+\", \"46+\"],\n    \"labels_task1\": [\"YES\", \"YES\", \"YES\", \"NO\", \"YES\", \"YES\"],\n    \"labels_task2\": [\"DIRECT\", \"DIRECT\", \"REPORTED\", \"-\", \"JUDGEMENTAL\", \"REPORTED\"],\n    \"labels_task3\": [\n      [\"STEREOTYPING-DOMINANCE\"],\n      [\"OBJECTIFICATION\"],\n      [\"SEXUAL-VIOLENCE\"],\n      [\"-\"],\n      [\"STEREOTYPING-DOMINANCE\", \"OBJECTIFICATION\"],\n      [\"OBJECTIFICATION\"]\n    ],\n    \"split\": \"TRAIN_EN\"\n  }\n}\n\n\nInstructions\n\nDownload the A1/data folder.\nLoad the three JSON files and encode them as pandas dataframes.\nGenerate hard labels for Task 1 using majority voting and store them in a new dataframe column called hard_label_task1. Items without a clear majority will be removed from the dataset.\nFilter the DataFrame to keep only rows where the lang column is 'en'.\nRemove unwanted columns: Keep only id_EXIST, lang, tweet, and hard_label_task1.\nEncode the hard_label_task1 column: Use 1 to represent “YES” and 0 to represent “NO”.\n\n\n!pip install datasets --quiet\n!pip install evaluate --quiet\n!pip install simplejson --quiet\n!python -m spacy download es_core_news_sm\n\nCollecting es-core-news-sm==3.7.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/12.9 MB 68.1 MB/s eta 0:00:00\nRequirement already satisfied: spacy&lt;3.8.0,&gt;=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (3.0.12)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (1.0.5)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (1.0.11)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (3.0.9)\nRequirement already satisfied: thinc&lt;8.3.0,&gt;=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (8.2.5)\nRequirement already satisfied: wasabi&lt;1.2.0,&gt;=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (1.1.3)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (2.5.0)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: weasel&lt;0.5.0,&gt;=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (0.4.1)\nRequirement already satisfied: typer&lt;1.0.0,&gt;=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (0.15.1)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (4.67.1)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (2.10.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (75.1.0)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (24.2)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (3.5.0)\nRequirement already satisfied: numpy&gt;=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (1.26.4)\nRequirement already satisfied: language-data&gt;=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes&lt;4.0.0,&gt;=3.2.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (1.3.0)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (2.27.1)\nRequirement already satisfied: typing-extensions&gt;=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,&lt;3.0.0,&gt;=1.7.4-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (4.12.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (2.2.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (2024.12.14)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc&lt;8.3.0,&gt;=8.2.2-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (0.7.11)\nRequirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc&lt;8.3.0,&gt;=8.2.2-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (0.1.5)\nRequirement already satisfied: click&gt;=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer&lt;1.0.0,&gt;=0.3.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (8.1.7)\nRequirement already satisfied: shellingham&gt;=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer&lt;1.0.0,&gt;=0.3.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (1.5.4)\nRequirement already satisfied: rich&gt;=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer&lt;1.0.0,&gt;=0.3.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (13.9.4)\nRequirement already satisfied: cloudpathlib&lt;1.0.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel&lt;0.5.0,&gt;=0.1.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (0.20.0)\nRequirement already satisfied: smart-open&lt;8.0.0,&gt;=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel&lt;0.5.0,&gt;=0.1.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (7.1.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (3.0.2)\nRequirement already satisfied: marisa-trie&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data&gt;=1.2-&gt;langcodes&lt;4.0.0,&gt;=3.2.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (1.2.1)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich&gt;=10.11.0-&gt;typer&lt;1.0.0,&gt;=0.3.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich&gt;=10.11.0-&gt;typer&lt;1.0.0,&gt;=0.3.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (2.18.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open&lt;8.0.0,&gt;=5.2.1-&gt;weasel&lt;0.5.0,&gt;=0.1.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (1.17.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&gt;=10.11.0-&gt;typer&lt;1.0.0,&gt;=0.3.0-&gt;spacy&lt;3.8.0,&gt;=3.7.0-&gt;es-core-news-sm==3.7.0) (0.1.2)\n✔ Download and installation successful\nYou can now load the package via spacy.load('es_core_news_sm')\n⚠ Restart to reload dependencies\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n\n\n\nimport os\nimport json\nfrom tqdm import tqdm\nimport simplejson as sj\nfrom pathlib import Path\nfrom functools import reduce\nimport urllib\nfrom collections import Counter\nfrom typing import Dict, List, Callable, Tuple, Optional\nimport copy\nimport random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.special import softmax\nimport seaborn as sns\n\nimport tensorflow as tf\nimport keras\nimport torch\n\nimport spacy\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport gensim\nimport gensim.downloader as gloader\n\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    f1_score,\n)\nfrom keras.layers import Input, Embedding, Bidirectional, LSTM, Dense\nfrom keras.metrics import F1Score, Precision, Recall\nfrom keras.utils import plot_model\n\nimport transformers\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    TFAutoModelForSequenceClassification,\n    AutoTokenizer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n    Trainer,\n    EarlyStoppingCallback,\n)\n\nimport requests\nimport re\nimport datasets\nfrom datasets import Dataset\nfrom sklearn.metrics import f1_score, accuracy_score, roc_curve, roc_auc_score, precision_recall_curve, PrecisionRecallDisplay, auc\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nnltk.download(\"punkt_tab\")\nnltk.download(\"wordnet\")\nnltk.download(\"averaged_perceptron_tagger_eng\")\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\ndef set_reproducibility(seed: int) -&gt; None:\n    \"\"\"\n    Fixes any possible source of randomness.\n\n    :param seed: seed to ensure reproducibility (int)\n    \"\"\"\n    keras.utils.set_random_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    tf.config.experimental.enable_op_determinism()\n    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"  ### can make training slower\n\n\nset_reproducibility(seed=42)\n\n\nDownload the A1/data folder.\nLoad the three JSON files and encode them as pandas dataframes.\n\n\nurl = \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/\"\n\ntrain_df = pd.read_json(url + \"training.json\").T\nvalidation_df = pd.read_json(url + \"validation.json\").T\ntest_df = pd.read_json(url + \"test.json\").T\n\n\nprint(f\"Training set shape: {train_df.shape}\")\nprint(f\"Validation set shape: {validation_df.shape}\")\nprint(f\"Test set shape: {test_df.shape}\")\n\nTraining set shape: (6920, 11)\nValidation set shape: (726, 11)\nTest set shape: (312, 11)\n\n\n\nGenerate hard labels for Task 1 using majority voting and store them in a new dataframe column called hard_label_task1. Items without a clear majority will be removed from the dataset.\nFilter the DataFrame to keep only rows where the lang column is 'en'.\nRemove unwanted columns: Keep only id_EXIST, lang, tweet, and hard_label_task1.\nEncode the hard_label_task1 column: Use 1 to represent “YES” and 0 to represent “NO”.\n\n\ndef determine_label(row: pd.Series) -&gt; Optional[str]:\n    \"\"\"\n    Converts soft labels into hard labels (strings).\n\n    :param row: row of the dataset to convert (pd.Series)\n\n    :return\n        - string representing the hard label\n    \"\"\"\n    num_yes = row[\"labels_task1\"].count(\"YES\")\n    num_no = row[\"labels_task1\"].count(\"NO\")\n    if num_yes == num_no:\n        return None\n    return \"YES\" if num_yes &gt; num_no else \"NO\"\n\n\ndef corpus(df: pd.DataFrame, multilingual:bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Converts soft labels into hard labels (int) and drops irrelevant rows/columns.\n\n    :param df: dataset split to convert (pd.DataFrame)\n    :param multilingual: whether to keep spanish tweets (bool)\n\n    :return\n        - refined dataset\n    \"\"\"\n    df[\"hard_label_task1\"] = df.apply(determine_label, axis=1)\n    df = df.dropna(subset=[\"hard_label_task1\"])\n    if not multilingual:\n      df = df[df[\"lang\"] != \"es\"]\n    df = df[[\"id_EXIST\", \"lang\", \"tweet\", \"hard_label_task1\"]]\n    df.loc[df[\"hard_label_task1\"] == \"YES\", \"hard_label_task1\"] = 1\n    df.loc[df[\"hard_label_task1\"] == \"NO\", \"hard_label_task1\"] = 0\n    df.reset_index(drop=True, inplace=True)\n    return df\n\n\ntrain_df = corpus(train_df)\nvalidation_df = corpus(validation_df)\ntest_df = corpus(test_df)\n\n\nvalidation_df\n\n\n  \n    \n\n\n\n\n\n\nid_EXIST\nlang\ntweet\nhard_label_task1\n\n\n\n\n0\n400001\nen\n@Mike_Fabricant “You should smile more, love. ...\n0\n\n\n1\n400002\nen\n@BBCWomansHour @LabWomenDec @EverydaySexism Sh...\n1\n\n\n2\n400003\nen\n#everydaysexism Some man moving my suitcase in...\n1\n\n\n3\n400004\nen\n@KolHue @OliverJia1014 lol gamergate the go to...\n0\n\n\n4\n400005\nen\n@ShelfStoriesGBL To me this has the same negat...\n0\n\n\n...\n...\n...\n...\n...\n\n\n153\n400172\nen\n@leesu44 @elishabroadway @markbann57 @SeaeyesT...\n1\n\n\n154\n400174\nen\nIt is is impossible for a man to become a woma...\n1\n\n\n155\n400175\nen\nIf Gaga decided to sing 18 versions of Free Wo...\n0\n\n\n156\n400176\nen\nThis is your reminder that you can be child-fr...\n0\n\n\n157\n400177\nen\njust completed my last final, i’m officially a...\n0\n\n\n\n\n158 rows × 4 columns"
  },
  {
    "objectID": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#performances",
    "href": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#performances",
    "title": "Sexism Detection",
    "section": "Performances",
    "text": "Performances\n\n# get predictions as probability distributions\nmodel_test_preds_soft = best_model.predict(test_sentences)\ntransformer_preds_soft = softmax(torch.tensor(y_pred_transformer), axis=-1)\n\n# get predictions as integer labels (0 -&gt; not sexist, 1 -&gt; sexist)\nmodel_test_preds_hard = np.argmax(model_test_preds_soft, axis=1)\ntransformer_preds_hard = np.argmax(transformer_preds_soft, axis=-1)\n\n9/9 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step\n\n\nUtility function to make plots\n\ndef plot_cm(y_pred: np.ndarray, y_true: np.ndarray, model_name: str, axis: np.ndarray) -&gt; None:\n    \"\"\"\n    Plots both confusion matrix and classification report of the given model.\n\n    :param y_pred: model predictions (np.ndarray)\n    :param y_true: true labels (np.ndarray)\n    :param model_name: name of the model to display (str)\n    :param axis: axis to plot on (np.ndarray)\n    \"\"\"\n    cr = classification_report(y_true, y_pred, output_dict=True)\n    cr = pd.DataFrame(cr).T\n    sns.heatmap(cr.iloc[:, :].astype(float), annot=True, fmt=\".2f\",\n                cmap=\"Blues\", cbar=False,\n                xticklabels=cr.columns, yticklabels=cr.index,\n                linewidths=0.5, linecolor=\"gray\", ax=axis[0])\n    axis[0].set_facecolor(\"white\")\n    axis[0].set_title(f'Classification Report: {model_name}', fontsize=14, weight=\"bold\")\n\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(cmap=\"Blues\", colorbar=False, ax=axis[1])\n    axis[1].set_facecolor(\"white\")\n    axis[1].set_title(f'Confusion Matrix: {model_name}', fontsize=14, weight=\"bold\")\n    axis[1].set_xlabel(\"Predicted Labels\", fontsize=12)\n    axis[1].set_ylabel(\"True Labels\", fontsize=12)\n\n    for ax in axis:\n        ax.tick_params(axis='x', labelsize=10)\n        ax.tick_params(axis='y', labelsize=10)\n\n\ndef plot_precision_recall(y_preds: np.ndarray, y_true: np.ndarray, model_name: str, ax: matplotlib.axes.Axes) -&gt; None:\n    \"\"\"\n    Plots precison-recall curve of the given model.\n\n    :param y_preds: model predictions (np.ndarray)\n    :param y_true: true labels (np.ndarray)\n    :param model_name: name of the model to display (str)\n    :param ax: axis to plot on (matplotlib.axes.Axes)\n    \"\"\"\n    # Precision recall curve\n    y_preds = y_preds[:,1]\n    precision, recall, _ = precision_recall_curve(y_true, y_preds)\n    prevalence_pos_label = np.sum(y_true) / len(y_true)\n    disp = PrecisionRecallDisplay(precision=precision,recall=recall,prevalence_pos_label=prevalence_pos_label)\n    auc_score = auc(recall,precision)\n    disp.plot(ax,name=f'Model (AUC = {auc_score:.2f}',plot_chance_level=True)\n    ax.fill_between(recall, precision, alpha=0.2)\n    ax.text(\n      0.6,\n      0.2,\n      f\"AUC = {auc_score:.2f}\",\n      fontsize=12,\n      color=\"blue\",\n      bbox=dict(facecolor=\"white\", alpha=0.7),\n    )\n    ax.set_xlabel(\"False Positive Rate (FPR)\",fontsize=12)\n    ax.set_ylabel(\"True Positive Rate (TPR)\",fontsize=12)\n    ax.set_title(f\"{model_name} Curve\",fontsize=14, weight=\"bold\")\n    ax.legend()\n\nAs we can see from the classification reports below there’s a performance gap between the Custom Model and the Transformer. The majority of the errors made by the Custom Model occur on label 1, while the Transformer seems to be more balanced as pointed out by the confusion matrices.\n\n# plots\nfig, ax = plt.subplots(2,2,figsize=(9,9))\nplot_cm(model_test_preds_hard, test_labels, \"Best_model\",(ax[0][0],ax[1][0]))\nplot_cm(transformer_preds_hard, test_labels, \"Trasformer\",(ax[0][1],ax[1][1]))\nplt.tight_layout()\n\n\n\n\n\n\n\n\nAs we can see from the Precision-Recall curves on label 1 the Transformer achieves a higher AUC score (0.88) compared to the Custom Model (0.80), indicating better overall precision and recall balance across thresholds.\n\nfig,ax = plt.subplots(1,2,figsize=(10,6))\nfig.suptitle('Precision Recall curves',fontsize=14, weight=\"bold\")\nplot_precision_recall(model_test_preds_soft, test_labels, \"Best_model\", ax[0])\nplot_precision_recall(transformer_preds_soft, test_labels, \"Trasformer\", ax[1])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nUtility function used for decoding sentences to display.\n\ndef custom_decode(sentences: np.ndarray) -&gt; List[List[str]]:\n    \"\"\"\n    Converts back token ids of the custom model to words.\n\n    :param sentences: sentences to decode (np.ndarray)\n\n    :return\n        - decoded sentences (list)\n    \"\"\"\n    decoded_sentences = []\n    for sentence in sentences:\n        decoded_sentence = []\n        for word in sentence:\n            if word != 0:\n                decoded_sentence.append(idx_to_word[word])\n        decoded_sentences.append(decoded_sentence)\n    return decoded_sentences\n\n\ndecoded_test = custom_decode(test_sentences)"
  },
  {
    "objectID": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#dataset-imbalance",
    "href": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#dataset-imbalance",
    "title": "Sexism Detection",
    "section": "Dataset Imbalance",
    "text": "Dataset Imbalance\nLet’s show how the labels are distributed across the dataset.\n\ndef plot_data_distribution(df: pd.DataFrame, ax: matplotlib.axes.Axes, title: str) -&gt; None:\n    \"\"\"\n    Shows the distribution of a dataset split.\n\n    :param df: split of the dataset (pd.DataFrame)\n    :param ax: axis to plot on (matplotlib.axes.Axes)\n    :param title: title of the plot (str)\n    \"\"\"\n    label_1 = len(df[df[\"hard_label_task1\"] == 1])\n    label_0 = len(df[df[\"hard_label_task1\"] == 0])\n    bar_colors = [\"#6baed6\", \"#2171b5\"]\n    bars = ax.bar([\"label_1\", \"label_0\"], [label_1, label_0], color=bar_colors)\n    percentages = [(label_1 / len(df))*100, (label_0 / len(df))*100]\n    for bar, percentage in zip(bars, percentages):\n        ax.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height(),\n            f\"{percentage:.2f}%\",\n            ha=\"center\",\n            va=\"bottom\",\n        )\n    ax.set_title(title)\n    ax.set_ylabel(\"label count\")\n\nAll the three splits seem to have similar label distribution, even though the train set is slightly more imbalanced. Perhaps this imbalance could be the reason why the Custom Model tends to have a bias towards non sexism as previously shown by the confusion matrix.\n\nfig, ax = plt.subplots(1, 3, figsize=(12, 5))\nplot_data_distribution(train_df, ax[0], \"train\")\nplot_data_distribution(validation_df, ax[1], \"validation\")\nplot_data_distribution(test_df, ax[2], \"test\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#sentence-level-analysis",
    "href": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#sentence-level-analysis",
    "title": "Sexism Detection",
    "section": "Sentence-level analysis",
    "text": "Sentence-level analysis\nLet’s now see which are the common errors at sentence level.\n\ndef display_common_errors(model_preds: np.ndarray, transformer_preds: np.ndarray, true_labels: np.ndarray, sentences: List[List[str]]) -&gt; None:\n  \"\"\"\n  Shows the common errors between the custom model and the transformer.\n\n  :param model_preds: predictions of the custom model (np.ndarray)\n  :param transformer_preds: predictions of the transformer (np.ndarray)\n  :param true_labels: true labels (np.ndarray)\n  :param sentences: input sentences (list)\n  \"\"\"\n  common_errors = pd.DataFrame(columns=[\"sentence\", \"true_label\", \"model_prediction\",'transformer_pred'])\n  transformer_errors = pd.DataFrame(columns=[\"sentence\", \"true_label\", \"model_prediction\",'transformer_pred','diff_pred'])\n  model_errors = pd.DataFrame(columns=[\"sentence\", \"true_label\", \"model_prediction\",'transformer_pred','diff_pred'])\n  sentences = sentences.apply(lambda x: copy.deepcopy(x))\n\n\n  model_hards = np.argmax(model_preds, axis=1)\n  transformer_hards = np.argmax(transformer_preds, axis=1)\n\n  for i in range(len(true_labels)):\n    if true_labels[i][0] != transformer_hards[i]:\n      if model_hards[i] == transformer_hards[i]:\n        common_errors.loc[len(common_errors)] = [\n                  \" \".join(sentences[i]),\n                  true_labels[i][0],\n                  model_preds[i][true_labels[i][0]],\n                  transformer_preds[i][true_labels[i][0]],\n              ]\n      else:\n        transformer_errors.loc[len(transformer_errors)] = [\n                  \" \".join(sentences[i]),\n                  true_labels[i][0],\n                  model_preds[i][true_labels[i][0]],\n                  transformer_preds[i][true_labels[i][0]],\n                  abs(model_preds[i][true_labels[i][0]] - transformer_preds[i][true_labels[i][0]]),\n              ]\n    elif model_hards[i] != transformer_hards[i]:\n      for j in range(len(sentences[i])):\n        if sentences[i][j] in oov_terms or sentences[i][j] not in word_listing:\n          if sentences[i][j] in oov_terms:\n            sentences[i][j] = f\"&lt;span style='color:red'&gt;{sentences[i][j]}&lt;/span&gt;\"\n          else:\n            sentences[i][j] = f\"&lt;span style='color:blue'&gt;{sentences[i][j]}&lt;/span&gt;\"\n\n      model_errors.loc[len(model_errors)] = [\n                \" \".join(sentences[i]),\n                true_labels[i][0],\n                model_preds[i][true_labels[i][0]],\n                transformer_preds[i][true_labels[i][0]],\n                abs(model_preds[i][true_labels[i][0]] - transformer_preds[i][true_labels[i][0]]),\n          ]\n\n  common_errors = common_errors.sort_values(by='true_label', ascending=False)\n  transformer_errors = transformer_errors.sort_values(by='diff_pred', ascending=False)\n  model_errors = model_errors.sort_values(by='diff_pred', ascending=False)\n\n  return common_errors, transformer_errors, model_errors\n\nBelow we display the sentences that both the Custom Model and the Tranformer got wrong along with the corresponding prediction values. The prediction value represents the probability that the model assigns to the true label, i.e smaller values imply larger errors made by the model.\n\ncommon_errors, transformer_errors, model_errors = display_common_errors(model_test_preds_soft, transformer_preds_soft, test_labels, test_tokenized)\nprint(f'Number of common errors between the Custom Model and the Transformer: {len(common_errors)}')\ncommon_errors = common_errors.style.set_properties(**{'text-align':'left'})\ndisplay(common_errors)\n\nNumber of common errors between the Custom Model and the Transformer: 30\n\n\n\n\n\n\n\n \nsentence\ntrue_label\nmodel_prediction\ntransformer_pred\n\n\n\n\n1\nthat gold digger be nice but the picture and set up for them be elite pinwork be like the revealing of the mona lisa always a work of art\n1\n0.045073\n0.109728\n\n\n2\ndear fan can we not harass talented actor of color stop be racist douche nozzle this be why we can t have nice thing moses be a badass inquisitor and y all can t handle her badassary\n1\n0.284516\n0.038928\n\n\n4\ncall to clean up parliament s laddish culture after mp s porn shame via\n1\n0.271655\n0.365000\n\n\n9\ni don t need a man to tell me what to stand up for thank you\n1\n0.314866\n0.255073\n\n\n8\ngo weak rn play the guitar manspreading i be on my knee\n1\n0.044828\n0.229011\n\n\n7\nmansplaining the concept of a pee drawer to my partner who want me\n1\n0.332612\n0.196164\n\n\n13\ncyborg politics be the struggle for language and the struggle against perfect communication against the one code that translate all mean perfectly the central dogma of phallogocentrism\n1\n0.018916\n0.012902\n\n\n12\nhave i miss something or have phallocentrism become the late cult\n1\n0.315829\n0.036020\n\n\n10\nwhy bother go to mar if life there be just go to be more of the same toxic sexism racism and bigotry the same horrible oppression unfairness and inequality that make earth a living hell for so many\n1\n0.058804\n0.308408\n\n\n20\ni pack my hubby case when we be go to a wedding in england tell him to pop his underpants in get to england he put his kilt outfit on and everyone discover what a true scot doesn t wear under his kilt if i don t do it it doesn t get do men\n1\n0.056732\n0.182320\n\n\n18\nbounceeyyyy boobieeeessssss\n1\n0.392917\n0.040618\n\n\n17\nbut you can enter my heart anytime baby i want u to sodomize me emotionally huhuhu\n1\n0.482891\n0.361583\n\n\n16\nyea back when i didn t even have one to slap unfortunate\n1\n0.306543\n0.096619\n\n\n15\naccord to the human right commission in people experience sexual harassment at work we have create an online course provide strategy to prevent and respond should incidents occur start your course at\n1\n0.093599\n0.074334\n\n\n29\nthis genration be no more then selfish cunt more matter how genuine you be you do all the right thing and they still make you look like a cunt it s only get bad too and people ask me why i don t bother with anyone\n1\n0.081977\n0.310637\n\n\n11\nnotice something else tho almost every single one be a man that s such a fucking problem that this be how some men believe they should let out their anger and hatred this society need to do good it do not even benefit the men patriarchy misogyny fail everyone\n0\n0.455971\n0.125236\n\n\n3\nyup i hate when men rape and kill woman\n0\n0.054037\n0.002256\n\n\n5\nstop don t lie to these people lady don t let this man trick you out of the great organ of your life cause what\n0\n0.104192\n0.004390\n\n\n0\nsex a in gender harassment be what they be infer\n0\n0.215036\n0.066964\n\n\n6\nlady don t have a miscarriage in louisiana if you do in addition to probably have your friend and family sue by some opportunistic yokel you ll be charge with murder\n0\n0.152081\n0.002646\n\n\n19\ntoday s woman eye makeup and those nail too\n0\n0.300865\n0.099775\n\n\n14\ncall a man bald be sexual harassment via\n0\n0.184400\n0.059021\n\n\n21\noh so now u can t understand woman s word\n0\n0.060663\n0.006125\n\n\n22\nthe question be do i wear the very short skirt that literally show my as with kneehighs or opt out for a short\n0\n0.441833\n0.081327\n\n\n24\nfeminism v womanism hudson weems identifies far difference between womanism and feminism be womanism be family orient and focus on race class and gender while feminism be female orient and strictly focus on biological sex relate issue woman face\n0\n0.143654\n0.187458\n\n\n23\nmy new favorite meme be the pregnant woman drive in the oklahoma hov lane tell the state trooper who pull her over that she be drive with another person\n0\n0.196056\n0.113502\n\n\n25\nyeah woman be get just a bad i ve notice it anyway i m sorry people be send you stuff like that it can be so gross\n0\n0.172709\n0.012971\n\n\n26\na vasectomy be a simple surgery do in an office hospital or clinic hour later recovery be pretty much complete tubal ligation recovery take week longer if it s do follow a c section or childbirth many woman can t afford to take week off work\n0\n0.247540\n0.201829\n\n\n27\nwoman shouldn t have to share their trauma story for you to feel empathy for the way they re feel today stand with them we deserve a choice junta si podemos\n0\n0.250281\n0.080697\n\n\n28\nuh they re allow to call this out make you look like a cunt fortunately i ll block you so fast you wont get your screenshot\n0\n0.418653\n0.012207\n\n\n\n\n\nHere we display the sentences that only the Transformer got wrong, along with the absolute differences between prediction values. The sentences are sorted by these differences.\n\nprint(f'Number of Transformer-only errors: {len(transformer_errors)}')\ntransformer_errors = transformer_errors.style.set_properties(**{'text-align':'left'})\ndisplay(transformer_errors)\n\nNumber of Transformer-only errors: 10\n\n\n\n\n\n\n\n \nsentence\ntrue_label\nmodel_prediction\ntransformer_pred\ndiff_pred\n\n\n\n\n5\ndude look like a lady aerosmith\n1\n0.936352\n0.188096\n0.748256\n\n\n6\nstay on penis masturbate the penis forever\n0\n0.830019\n0.092392\n0.737627\n\n\n3\nwell let me ask for the guy do you ever just grab your testicle because you can\n0\n0.823763\n0.152067\n0.671696\n\n\n0\ni mean i do but wouldn t it be fun to get gangbanged together\n0\n0.980279\n0.325955\n0.654325\n\n\n1\nappreciate be include along with on the uvalde gunman s history of cyber gender harassment regrettably unshocking\n0\n0.638995\n0.007495\n0.631501\n\n\n7\nit s in the teaxass code book of penis envy idiot that it s gun over everything sad thing be it s spread and the more we try to stop it the faster it spread like the diseased idiot that support it\n0\n0.838803\n0.217703\n0.621100\n\n\n2\nhow do we reach a point where blonks be have the audacity to talk about yoongis rap do y all see who you stan be y all ok your girl can t write can t deliver can t come close to bts s lyricism tf be y all get brave for fashion deal\n0\n0.594202\n0.019369\n0.574833\n\n\n4\nstop say i say this cu she have short hair some of her gay look come from blonde ponytail era know ur herstory\n1\n0.856455\n0.330240\n0.526215\n\n\n8\nsharia law in the religion of islam allows abortion before the th montha for rape incest mental physical health or life of mother be at risk after the th month abortion allow only to save the life of the mother\n0\n0.505552\n0.284134\n0.221419\n\n\n9\nisw publish the fake crap of any of the map they be just a front for the u state department they be fake news victoria nuland be the woman directly involve in the armed overthrow of the ukraine government in\n0\n0.538044\n0.371504\n0.166539\n\n\n\n\n\nConversely the sentences below represent the errors made only by the Custom Model. Additionally we have highlighted the UNK terms in blue and the OOV in red, showing that many of these words are closely related to sexism. Probably such words, which are highly informative for this task, caused the Custom Model to fail on those sentences.\n\nprint(f'Number of Custom Model-only errors: {len(model_errors)}')\nmodel_errors = model_errors.style.set_properties(**{'text-align':'left'})\ndisplay(model_errors)\n\nNumber of Custom Model-only errors: 40\n\n\n\n\n\n\n\n \nsentence\ntrue_label\nmodel_prediction\ntransformer_pred\ndiff_pred\n\n\n\n\n29\nsound like the typa nigga to let his bos sodomize him cause the job come with perk stfu keep that dick in ya mouth nd stay way the fuck out there wherever u nd that ignorant as comment be think of\n1\n0.073411\n0.987215\n0.913804\n\n\n9\ny all be husband and wife and best friend you guy have so much fun it s awesome you keep each other laughing and smile\n0\n0.079544\n0.990332\n0.910788\n\n\n32\nnever know you all share tits how do we stop be pal again yeah\n1\n0.079930\n0.972010\n0.892079\n\n\n28\nder i literally take it last night slutbag\n1\n0.059965\n0.949125\n0.889161\n\n\n10\nwait till ur husband mess up u gon na love the scouser death threat\n1\n0.086108\n0.975064\n0.888955\n\n\n21\neat out both hole and then slide my dick in you and pound you till you squirt\n1\n0.118759\n0.984013\n0.865254\n\n\n1\nsuika get gangbanged to death\n1\n0.113801\n0.971719\n0.857918\n\n\n38\nyo i swear sagittarius men and woman be the funny\n0\n0.089753\n0.922790\n0.833037\n\n\n4\nthe violent antifeminism of a far right movement that see principally a vessel for breed a new white generation express itself in a fixation on a return to traditional gender role worth every minute to read take min out of your sun\n1\n0.050172\n0.864687\n0.814515\n\n\n37\nyou don t know who invent womanism huh okay risk skin cancer i guess best of luck to you\n1\n0.201679\n0.990023\n0.788343\n\n\n5\nisa hot girl summer now pop yall shit\n0\n0.198477\n0.983003\n0.784526\n\n\n15\ncan you confront your misogyny misandry be hardly a problem in the world by comparison and if it exist it a reaction to abuse\n1\n0.196442\n0.971214\n0.774772\n\n\n22\nit s ironic that the majority of the people happy about this in the comment be people who can t get pregnant\n1\n0.167558\n0.917408\n0.749850\n\n\n14\nwhy do you feel unsafe share a bathroom with a transwoman seem like some bigotry you need to unpack yourself not far marginalize an already marginalize group\n1\n0.076321\n0.824592\n0.748271\n\n\n11\naughhh i still get an exam tomorrow i hate woman\n1\n0.159302\n0.905700\n0.746397\n\n\n25\nit just show how money rule the world i m so disappointed that ripley s would even allow it out of the building and to let that reality skank get it be an incredible insult i be wait for an sub full of armed robber to overtake her in the end khloe look well\n1\n0.254709\n0.936507\n0.681798\n\n\n23\nhave a dream that i be at a party with my friend and bill gate show up and i say to my friend look at his pregnant as belly\n0\n0.304590\n0.986050\n0.681460\n\n\n17\non the audio from when his finger be cut off even the people that be there say she do it she say she s totally against cocaine but do cocaine try to say the nurse be basically lie in her note about her do drug everyone be lie but her\n0\n0.266323\n0.945932\n0.679609\n\n\n12\nthey tell u we re brood mare amp must die so cell clump can live carry our rapist baby amp want u to protest civilly yes he s mansplaining protest nicely while we be move toward enslavement polite little ladies no\n1\n0.323620\n0.964314\n0.640694\n\n\n27\njealous plss talk about his new album u wrinkly slutbag\n1\n0.042669\n0.682589\n0.639920\n\n\n39\nlittle woman atlanta star m juicy reveal health struggle after have a stroke\n0\n0.376461\n0.991794\n0.615333\n\n\n2\nmaybe something involve her get gangbanged\n1\n0.381396\n0.995329\n0.613932\n\n\n30\nso you stereotype someone base on their appearance be what make you some form of enlightened individual a oppose to a throwback racist cretin get it judge people by the race and appearance accord to this guy\n0\n0.363724\n0.967839\n0.604115\n\n\n33\nshould focus on money freedom and not retirement today s woman may still think about a not have to work read why gt\n1\n0.341708\n0.938272\n0.596564\n\n\n0\nst day at the pool on a beautiful sunday in ny too bad few dare to go in due to the frigid water temp we last min and figure we prove our point\n0\n0.422451\n0.995615\n0.573164\n\n\n8\nbest way to do that be from behind her with your hand on her hip\n1\n0.442914\n0.988667\n0.545753\n\n\n16\nmost woman who don t like bath go to girls school\n1\n0.443296\n0.989010\n0.545713\n\n\n19\nah here she be work hard to promote the patriarchy exploit the light music club girl like porn victim by make them drink tea eat cake and occasionally play cute song or something also trick young woman into like the show how dare she\n1\n0.452815\n0.996730\n0.543915\n\n\n3\nkabul islamic emirate of afghanistan have announce the end of co education in the country cite gender harassment female student will attend university on monday wednesday and saturday while tuesday thursday and sunday be fix for male\n1\n0.453141\n0.977071\n0.523930\n\n\n6\nmy baby call me mommy sha for the first time today twice y all don t understand how hype that make me baby girl have autism and get her to talk without be prompt have be a challenge she s come so far\n0\n0.469968\n0.990878\n0.520910\n\n\n26\ni kinda get her point but a slag be too far off for me\n1\n0.395405\n0.897156\n0.501750\n\n\n36\nso why tf ya call it femism shouldn t it be womanism if that s the case\n1\n0.486762\n0.988478\n0.501716\n\n\n20\nfuck s sake be it possible to have a furry space that s not explicitly content restrict and doesn t immediately become yet another showcase for size queen phallocentrism\n1\n0.179012\n0.665819\n0.486806\n\n\n34\nan unarmed black woman drive a car with a broken tail light be more dangerous than a white supremacist terrorist who murder people\n1\n0.280515\n0.694021\n0.413506\n\n\n31\nsuperwoman syndrome it s all too real and debilitate for this gal\n1\n0.458902\n0.816766\n0.357864\n\n\n7\nhe try to expose this gold digger but it backfire day fianc e b via\n0\n0.264883\n0.560497\n0.295614\n\n\n13\nwonwoo cross legs amp jun manspreading\n1\n0.374546\n0.648841\n0.274295\n\n\n24\ni don t care what gender race religion sexual orientation a person be as long a they be qualify for that position that be say be she qualify base on experience education or be she choose solely because she be black and gay to please progressive wake just say\n1\n0.320049\n0.591192\n0.271143\n\n\n18\na a diverse and pluralistic country america s law should not be base on a distorted and patriarchal interpretation of christianity furthermore the government should have no role whatsoever in a woman s private healthcare decision\n0\n0.329812\n0.504869\n0.175056\n\n\n35\ni be non binary and also lean into my femininity most of the time i be content with be afab amp be see a a woman amp embrace it other time i simply want to be a genderless thing that exist\n0\n0.344296\n0.512631\n0.168335\n\n\n\n\n\nLet’s now analyze potential patterns of error in the model’s predictions. We will start by plotting the length of the sentences that were incorrectly classified, this analysis can help us identify whether the model struggles with sentences of specific lengths.\n\ndef plot_len_sentences(\n    model_predictions: np.ndarray,\n    true_labels: np.ndarray,\n    sentences: List[List[str]],\n    model_name: str) -&gt; None:\n    \"\"\"\n    Plots the length of the wrong classified sentences for each label and calculates correlation.\n\n    :param model_predictions: predictions of the model (np.ndarray)\n    :param true_labels: true labels (np.ndarray)\n    :param sentences: input sentences (list)\n    :param model_name: name of the model for the plot title (str)\n    \"\"\"\n    df = pd.DataFrame({\"sentences\": list(sentences)})\n    df[\"true_label\"] = true_labels\n    df[\"pred_label\"] = model_predictions.astype(int)\n\n    df_wrong = df[df[\"true_label\"] != df[\"pred_label\"]]\n    len_sentences_wrong = Counter([len(row) for row in df_wrong[\"sentences\"]])\n    len_sentences = Counter([len(row) for row in df[\"sentences\"]])\n\n    all_lengths, all_counts = zip(*len_sentences.items())\n    wrong_lengths, wrong_counts = zip(*len_sentences_wrong.items())\n\n    plt.bar(wrong_lengths, wrong_counts, color=\"tab:blue\", label=\"Wrong Sentences\")\n    plt.bar(all_lengths, all_counts, alpha=0.3, color=\"tab:blue\", label=\"All Sentences\")\n    plt.title(f\"Length of {model_name} wrong sentences\")\n    plt.xlabel(\"Sentence Length\")\n    plt.ylabel(\"Count\")\n    plt.legend()\n    plt.show()\n\n    wrong_dict = dict(len_sentences_wrong)\n    all_dict = dict(len_sentences)\n\n    lengths = list(set(all_dict.keys()).union(set(wrong_dict.keys())))\n    wrong_counts_aligned = [wrong_dict.get(length, 0) for length in lengths]\n    all_counts_aligned = [all_dict.get(length, 0) for length in lengths]\n\n    correlation = np.corrcoef(wrong_counts_aligned, all_counts_aligned)[0, 1]\n    print(f\"Correlation between wrong sentence lengths and total lengths: {correlation:.2f}\")\n\nComparing the length distribution of the missclassified sentences with that of all sentences we noticed a quite high correlation between the two. This means that sentence length does not significantly influence the predictions of either the Custom Model or the Transformer.\n\nplot_len_sentences(model_test_preds_hard, test_labels, decoded_test, 'Custom Model')\n\n\n\n\n\n\n\n\nCorrelation between wrong sentence lengths and total lengths: 0.71\n\n\n\nplot_len_sentences(transformer_preds_hard, test_labels, decoded_test, 'Transformer')\n\n\n\n\n\n\n\n\nCorrelation between wrong sentence lengths and total lengths: 0.50"
  },
  {
    "objectID": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#word-level-analysis",
    "href": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#word-level-analysis",
    "title": "Sexism Detection",
    "section": "Word-level analysis",
    "text": "Word-level analysis\nSince it looks like there are no clear error patterns at sentence level, we will delve into a more fine-grained analysis: a word-level analysis. This analysis will help us understand whether certain types of words systematically affect the model’s predictions.\n\ndef plot_unk_stats(model_predictions: np.ndarray,\n                   true_labels: np.ndarray,\n                   sentences: List[List[str]]) -&gt; Tuple[List[str],List[str]]:\n  '''\n  Displays the statistic releated to the UNK token.\n\n  :param model_predictions: predictions of the model (np.ndarray)\n  :param true_labels: true labels (np.ndarray)\n  :param sentences: input sentences (list)\n  '''\n\n  def update_stats(df:pd.DataFrame) -&gt; None:\n    all_words = Counter(\n      word if word not in oov_terms else 'OOV' for row in df[\"sentences\"] for word in row\n    )\n    df_wrong = df[df['true_label'] != df['pred_label']]\n    wrongest_words = Counter(\n      word if word not in oov_terms else 'OOV' for row in df_wrong[\"sentences\"] for word in row\n    )\n\n    count_total_unk, count_total_oov = all_words['UNK'], all_words['OOV']\n    count_wrong_unk, count_wrong_oov = wrongest_words['UNK'], wrongest_words['OOV']\n\n    unk_per_sentences = np.mean([sum([1 for word in sentence if word=='UNK']) for sentence in df[\"sentences\"]])\n    unk_per_wrong_sentences = np.mean([sum([1 for word in sentence if word=='UNK']) for sentence in df_wrong[\"sentences\"]])\n    unk_stats.loc[len(unk_stats)] = [len(df), len(df_wrong), count_total_unk, count_wrong_unk, count_total_oov, count_wrong_oov]\n\n\n  column_names = pd.MultiIndex.from_product([['Sentences', 'UNK', 'OOV'], ['Total', 'Wrong']])\n  unk_stats = pd.DataFrame(columns = column_names, dtype = int)\n  df = pd.DataFrame({\"sentences\": list(sentences)})\n  df[\"true_label\"] = true_labels\n  df[\"pred_label\"] = model_predictions\n\n  df_0 = df[df[\"true_label\"] == 0]\n  df_1 = df[df[\"true_label\"] == 1]\n  update_stats(df_0)\n  update_stats(df_1)\n  print('Statistics of UNK and OOV terms of Custom Model\\n')\n  display(unk_stats)\n\nThe table below shows statistics related to the presence of UNK and OOV tokens in the sentences, distinguishing between all sentences (total) and those incorrectly classified. Although there are differences in the counts, the proportion of UNK and OOV tokens in wrong predictions does not suggest a strong influence on the Custom Model’s errors. Specifically:\n\nUNK tokens appear frequently in both total and wrong sentences, but their distribution does not seem to correlate with misclassifications.\nOOV tokens are relatively rare overall, but a large number of them (11 out of 20) appear in misclassified sentences with label 1. Indeed, as we have seen in the earlier analysis section, these terms are closely related to sexism and are important for the task.\n\n\nplot_unk_stats(model_test_preds_hard, test_labels, decoded_test)\n\nStatistics of UNK and OOV terms of Custom Model\n\n\n\n\n  \n    \n\n\n\n\n\n\nSentences\nUNK\nOOV\n\n\n\nTotal\nWrong\nTotal\nWrong\nTotal\nWrong\n\n\n\n\n0\n160\n27\n340\n35\n10\n0\n\n\n1\n126\n43\n218\n60\n20\n11\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nSince UNK terms do not appear to be correlated with the Custom Model’s errors, we will now focus on analyzing the frequency of the most common terms, including OOV terms. This analysis aims to identify the words most strongly correlated with the model’s misclassifications.\n\ndef plot_wrong_word(model_predictions: np.ndarray,\n                    true_labels: np.ndarray,\n                    sentences: List[List[str]],\n                    model_name: str,\n                    top_wrongest: int = 20) -&gt; None:\n    \"\"\"\n    Shows the most frequent words in wrong classified sentences.\n\n    :param model_predictions: predictions of the model (np.ndarray)\n    :param true_labels: true labels (np.ndarray)\n    :param model_name: name of the model for the plot title (str)\n    :param sentences: input sentences (list)\n    \"\"\"\n    def find_wrongest(df: pd.DataFrame, label: int, ax: matplotlib.axes.Axes) -&gt; None:\n        df = df[df[\"true_label\"] == label]\n        all_words = Counter(\n            word for row in df[\"sentences\"] for word in row if word not in stop_words\n        )\n        df_wrong = df[df['true_label'] != df['pred_label']]\n        wrongest_words = Counter(\n            word for row in df_wrong[\"sentences\"] for word in row if word not in stop_words\n        )\n\n        wr_words, wr_counts = zip(*wrongest_words.most_common(top_wrongest)[:0:-1])\n        ax.barh(wr_words, wr_counts, color=\"skyblue\", label='Wrong Sentences')\n        counts = [all_words[word] for word in wr_words]\n        ax.barh(wr_words, counts, color=\"skyblue\",alpha=0.4, label='All Sentences')\n        ax.set_title(f\"Wrongest words in label {label}\")\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        ax.legend()\n\n    stop_words = list(stopwords.words(\"english\")) + list(stopwords.words(\"spanish\")) + ['n', 'ser']\n    df = pd.DataFrame({\"sentences\": list(sentences)})\n    df[\"true_label\"] = true_labels\n    df[\"pred_label\"] = model_predictions.astype(int)\n\n    fig, axs = plt.subplots(1,2, figsize=(10,5))\n    fig.suptitle(f'{model_name} charts', weight=\"bold\")\n    find_wrongest(df, 0, axs[0])\n    find_wrongest(df, 1, axs[1])\n    plt.tight_layout()\n    plt.show()\n\nEach of the following chart highlights the top words that contributed to the model’s errors for the respective label. Common stop words (e.g., “the”, “and”, “is”) are removed to focus on more meaningful terms that might influence the model’s predictions.\nFor each word, the chart shows: * Darker Bars: Frequency of the word in misclassified sentences. * Lighter Bars (Transparent): Overall frequency of the word in all sentences of the given label.\nHere are the key findings from the Custom Model charts:\n\nLabel 0: As shown in the left chart, most of the words are closely related to sexism (e.g., ‘woman’, ‘sex’, ‘feminism’, ‘harassment’). In particular, the word ‘woman’ is not only the most frequently misclassified word but also the one with the highest misclassification rate compared to its overall occurrence in sentences.\nLabel 1: As shown in the right chart, there is no single word particularly correlated with misclassifications for label 1, for instance ‘woman’ is by far the most frequent word in all sentences, it has a relatively low proportion of misclassifications compared to its total occurrences.\n\nFinal observations:\nThis analysis suggests that the model may exhibit a bias, associating the word ‘woman’ strongly with sexism, regardless of the actual context. This bias could be influencing the model’s overall performance.\n\nplot_wrong_word(model_test_preds_hard, test_labels, decoded_test, 'Custom Model')\n\n\n\n\n\n\n\n\nHere are the key findings from the Transformer charts:\nLabel 0: Similar to the Custom Model, most of the words are closely related to sexism. However, in the Transformer Model, the word ‘woman’ is again the most frequently misclassified word but with a slightly lower misclassification rate compared to its overall occurrence in sentences, suggesting the Transformer may handle this word marginally better than the Custom Model.\nLabel 1: Unlike Label 0, there is no clear word strongly correlated with misclassifications. Words such as ‘like’, ‘look’, ‘get’ contribute the most to errors, but they are general-purpose words rather than terms specifically related to sexism. This indicates that, for label 1, the Transformer’s errors are not driven by any specific thematic words. The word ‘woman’ does not appear in the chart for label 1, meaning that it does not play an important role in the misclassifications for this label.\nFinal Observations:\nThis analysis reveals that the Transformer exhibits the same bias as the Custom Model, associating the word ‘woman’ strongly with sexism, regardless of the actual context.\n\nplot_wrong_word(transformer_preds_hard, test_labels, decoded_test, 'Transformer')\n\n\n\n\n\n\n\n\nThis consistent pattern across both models suggests that the bias may not originate from the models themselves but rather from the dataset.\nIf the dataset overrepresents the word ‘woman’ in contexts related to sexism, the models are likely to learn an inaccurate association between the word and sexist content. This imbalance might cause the models to incorrectly classify sentences containing ‘woman’, even when the context is neutral.\n\nCorrelation Analysis: Woman, OOV and UNK tokens\nAfter identifying error patterns involving the word ‘woman’, OOV tokens, and UNK tokens, we aim to test their significance by calculating their correlation with misclassification errors. This analysis will help determine whether these factors are strongly associated with errors or if their impact is less meaningful. Understanding these correlations is crucial for validating the observed patterns.\n\ndef plot_correlation(\n                     labels: np.ndarray,\n                     predictions: np.ndarray,\n                     sentences: List[List[str]],\n                     model_name: str,\n                     oov_terms: List[str] = []) -&gt; None:\n  \"\"\"\n  Computes and plots correlations to highlight relevant patterns.\n\n  :param labels: true labels (np.ndarray)\n  :param predictions: predictions of the model (np.ndarray)\n  :param sentences: input sentences (list)\n  :param model_name: name of the model for the plot title (str)\n  :param oov_terms: list of OOV terms (list)\n  \"\"\"\n  def compute_correlation(label: int, ax: matplotlib.axes.Axes) -&gt; None:\n\n    report = pd.DataFrame(\n    columns=[\n        \"woman\",\n        \"oov\",\n        \"unk\",\n        \"is_wrong\",\n      ]\n    )\n\n    for enum, sentence in enumerate(sentences):\n\n      if labels[enum] == label:\n\n        unk_count = 0\n        oov_count = 0\n        woman_count = 0\n\n        for word in sentence:\n          if word == \"UNK\":\n              unk_count += 1\n          if word in oov_terms:\n              oov_count += 1\n          if word == 'woman':\n              woman_count += 1\n\n        is_wrong = 1\n        if labels[enum] == predictions[enum]:\n          is_wrong = 0\n\n        report.loc[len(report)] = [\n            woman_count,\n            oov_count,\n            unk_count,\n            is_wrong,\n        ]\n\n    if model_name == 'Transformer':\n      report = report[['woman','is_wrong']]\n\n    corr = report.corr()\n    sns.heatmap(corr, cmap=\"Blues\", annot=True, ax = ax,vmin=0,vmax=1);\n    ax.set_title(f'Label {label}')\n\n  fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n  fig.suptitle(f'{model_name} Correlation Matrices', weight=\"bold\")\n  compute_correlation(0, ax[0])\n  compute_correlation(1, ax[1])\n  plt.show()\n\nThe correlation matrix confirms the pattern seen earlier: the word ‘woman’ is positively correlated with errors in label 0 and negatively correlated with errors in label 1. This shows that the Custom Model tends to associate ‘woman’ with sexism in label 0 and is less likely to misclassify sentences with ‘woman’ in label 1.\nOn the other hand, UNK and OOV terms show no significant correlation with errors.\n\nplot_correlation(test_labels, model_test_preds_hard, decoded_test,'Custom Model', oov_terms)\n\n\n\n\n\n\n\n\nThe correlation matrix for the Transformer shows similar results to the Custom Model, with a slightly lower positive correlation with errors in label 0.\nIt does not include UNK and OOV terms because the Transformer model processes the input directly using its own tokenization. This structural difference simplifies the analysis, focusing only on the word ‘woman’ and its correlation with errors.\n\nplot_correlation(test_labels, transformer_preds_hard, decoded_test, 'Transformer')\n\n\n\n\n\n\n\n\n\n# get OOV terms in wrong sentences of label 1\nwrong_oov_1 = [word  for i, sentence in enumerate(decoded_test) for word in sentence if test_labels[i] != model_test_preds_hard[i] and test_labels[i] == 1 and word in oov_terms]\nprint(f'OOV terms in misclassified sentences of label 1: {set(wrong_oov_1)}')\n\nOOV terms in misclassified sentences of label 1: {'slutbag', 'gangbanged', 'phallogocentrism', 'phallocentrism', 'manspreading', 'mansplaining'}\n\n\nActually we think that the weak correlation between OOV terms and misclassified sentences on label 1 is due to the fact that, as we have seen from the previous analyses, the most informative OOV terms were exclusively found in the misclassified sentences of label 1. So we will try to recompute the correlation focusing only on those terms (the ones in the list above).\n\nplot_correlation(test_labels, model_test_preds_hard, decoded_test,'Custom Model', wrong_oov_1)\n\n\n\n\n\n\n\n\nAs expected the correlation between OOV terms and misclassified sentences has increased, meaning that these terms are a source of error for label 1 sentences.\nGiven the two main error patterns identified in the previous analyses, we propose the following solutions:\n\n‘woman’ pattern: addressing this issue would require refining the dataset, providing more diverse contextual examples, and/or applying bias mitigation strategies during training.\nOOV pattern: a possible solution could be trying to assign to these terms an embedding close to semantically similar words already present in our vocabulary, or implementing subword tokenization techniques."
  },
  {
    "objectID": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#mulitilingual-model",
    "href": "assignment_1/assignment_1_baiocchi_dibuo_petrilli.html#mulitilingual-model",
    "title": "Sexism Detection",
    "section": "Mulitilingual Model",
    "text": "Mulitilingual Model\nThis section explores Spanish tweets using three different models: the two previously discussed models and a multilingual transformer. Since the test split of the dataset does not include Spanish tweets, we decided to evaluate performance on the validation set.\n\nCustom Model\n\nurl = \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/\"\n\ntrain_df = pd.read_json(url + \"training.json\").T\nvalidation_df = pd.read_json(url + \"validation.json\").T\n\n\ntrain_df = corpus(train_df,multilingual=True)\nvalidation_df = corpus(validation_df,multilingual=True)\n\n\nvalidation_df\n\n\n  \n    \n\n\n\n\n\n\nid_EXIST\nlang\ntweet\nhard_label_task1\n\n\n\n\n0\n300002\nes\n@anacaotica88 @MordorLivin No me acuerdo de lo...\n1\n\n\n1\n300003\nes\n@cosmicJunkBot lo digo cada pocos dias y lo re...\n0\n\n\n2\n300004\nes\nAlso mientras les decia eso la señalaba y deci...\n1\n\n\n3\n300005\nes\nAnd all people killed, attacked, harassed by ...\n0\n\n\n4\n300006\nes\nOn this #WorldPressFreedomDay I’m thinking of ...\n0\n\n\n...\n...\n...\n...\n...\n\n\n643\n400172\nen\n@leesu44 @elishabroadway @markbann57 @SeaeyesT...\n1\n\n\n644\n400174\nen\nIt is is impossible for a man to become a woma...\n1\n\n\n645\n400175\nen\nIf Gaga decided to sing 18 versions of Free Wo...\n0\n\n\n646\n400176\nen\nThis is your reminder that you can be child-fr...\n0\n\n\n647\n400177\nen\njust completed my last final, i’m officially a...\n0\n\n\n\n\n648 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nOnce we introduced Spanish tweets into the dataset, the labels became more balanced than before, especially in the validation set.\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 5))\nplot_data_distribution(train_df, ax[0], \"train\")\nplot_data_distribution(validation_df, ax[1], \"validation\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f'Number of english tweet in train set: {len(train_df[train_df[\"lang\"] == \"en\"])}')\nprint(f'Number of spanish tweet in train set: {len(train_df[train_df[\"lang\"] == \"es\"])}\\n')\n\nprint(f'Number of english tweet in validation set: {len(validation_df[validation_df[\"lang\"] == \"en\"])}')\nprint(f'Number of spanish tweet in validation set: {len(validation_df[validation_df[\"lang\"] == \"es\"])}')\n\nNumber of english tweet in train set: 2870\nNumber of spanish tweet in train set: 3194\n\nNumber of english tweet in validation set: 158\nNumber of spanish tweet in validation set: 490\n\n\n\nPREPROCESSING_PIPELINE = [\n    remove_emojis,\n    remove_hashtags,\n    remove_mentions,\n    remove_urls,\n    remove_special_chars,\n    lower,\n]\ntrain_df[\"tweet\"] = train_df[\"tweet\"].apply(lambda txt: text_prepare(txt))\nvalidation_df[\"tweet\"] = validation_df[\"tweet\"].apply(lambda txt: text_prepare(txt))\n\n\ntrain_df.to_csv('train_multilingual.csv')\nvalidation_df.to_csv('validation_multilingual.csv')\n\nFor the lemmatization part, we could not use the WordNet Lemmatizer as it is designed exclusively for English. Therefore, we decided to adopt the Spacy Lemmatizer, which supports both English and Spanish sentences.\n\nnlp_en = spacy.load(\"en_core_web_sm\")\nnlp_es = spacy.load(\"es_core_news_sm\")\n\ndef tokenize_tweet(row: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Tokenizes and lemmatizes the given tweet based on the specified language.\n\n    :param row: row of a DataFrame (pd.Series)\n\n    :return: tokenized and lemmatized tweet (List[str])\n    \"\"\"\n    nlp = nlp_en if row['lang'] == \"en\" else nlp_es\n    doc = nlp(row['tweet'])\n    row['tweet'] = [token.lemma_ for token in doc if not token.is_space]\n    return row\n\ntrain_df = train_df.apply(tokenize_tweet,axis=1)\nvalidation_df = validation_df.apply(tokenize_tweet,axis=1)\n\n\nidx_to_word, word_to_idx, word_listing = build_vocabulary(train_df)\nprint(f\"\\n[Debug] Index -&gt; Word vocabulary size: {len(idx_to_word)}\")\nprint(f\"[Debug] Word -&gt; Index vocabulary size: {len(word_to_idx)}\")\nprint(f\"[Debug] Some words: {[(idx_to_word[idx], idx) for idx in np.arange(20)]}\")\n\n100%|██████████| 6064/6064 [00:00&lt;00:00, 118996.81it/s]\n\n\n\n[Debug] Index -&gt; Word vocabulary size: 19132\n[Debug] Word -&gt; Index vocabulary size: 19132\n[Debug] Some words: [('PAD', 0), ('UNK', 1), ('ignorar', 2), ('al', 3), ('otro', 4), ('ser', 5), ('uno', 6), ('capullo', 7), ('el', 8), ('problema', 9), ('con', 10), ('este', 11), ('youtuber', 12), ('denunciar', 13), ('acoso', 14), ('cuando', 15), ('no', 16), ('afectar', 17), ('a', 18), ('gente', 19)]\n\n\n\n\n\nWe decided to use GloVe-Twitter, a specialized version of GloVe, because it includes a larger vocabulary compared to the previously used version, particularly providing embeddings for many Spanish words.\n\nembedding_dimension = 200\nembedding_model = gloader.load(f\"glove-twitter-{embedding_dimension}\")\n\n\nembedding_matrix = build_embedding_matrix(\n    embedding_model, embedding_dimension, word_to_idx, len(word_to_idx)\n)\nprint(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n\n100%|██████████| 19132/19132 [00:00&lt;00:00, 130848.23it/s]\n\n\nEmbedding matrix shape: (19132, 200)\n\n\n\n\n\n\noov_terms = check_OOV_terms(embedding_model, word_listing)\noov_percentage = float(len(oov_terms)) * 100 / len(word_listing)\nprint(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")\n\nTotal OOV terms: 3495 (18.27%)\n\n\n\nvalidation_df[\"tweet\"] = validation_df[\"tweet\"].apply(put_unk)\n\n\nmax_train_length = max(len(tweet) for tweet in train_df[\"tweet\"])\nmax_validation_length = max(len(tweet) for tweet in validation_df[\"tweet\"])\nmax_sequence_lenght = max(max_train_length, max_validation_length)\n\nprint(f\"Max sequence lenght in train set: {max_train_length}\")\nprint(f\"Max sequence lenght in validation set: {max_validation_length}\")\nprint(f\"Max sequence lenght: {max_sequence_lenght}\")\n\nMax sequence lenght in train set: 62\nMax sequence lenght in validation set: 61\nMax sequence lenght: 62\n\n\n\ntrain_df[\"tweet\"] = train_df[\"tweet\"].apply(pad_tweet)\nvalidation_df[\"tweet\"] = validation_df[\"tweet\"].apply(pad_tweet)\n\n\ntrain_df[\"tweet\"] = train_df[\"tweet\"].apply(word_to_num)\nvalidation_df[\"tweet\"] = validation_df[\"tweet\"].apply(word_to_num)\n\n\ntrain_sentences, train_labels = format_data(\n    train_df[\"tweet\"], train_df[\"hard_label_task1\"]\n)\nvalidation_sentences, validation_labels = format_data(\n    validation_df[\"tweet\"], validation_df[\"hard_label_task1\"]\n)\n\n\ncategorical_train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=2)\ncategorical_validation_labels = tf.keras.utils.to_categorical(\n    validation_labels, num_classes=2\n)\n\n\nbest_seed = 2\nbest_model = get_model(add_lstm=False, vocab_size=len(word_listing))\n\n\nset_reproducibility(best_seed)\n\nif best_model.name == \"Baseline\":\n    multilingual_model = get_model(add_lstm=False, vocab_size=len(word_listing))\nelse:\n    multilingual_model = get_model(add_lstm=True, vocab_size=len(word_listing))\n\nmultilingual_model, _, _, _, _= train_loop(multilingual_model)\n\n\nvalidation_preds = np.argmax(multilingual_model(validation_sentences), axis=1)\nprint(classification_report(validation_labels, validation_preds, digits=3))\ncm = confusion_matrix(validation_labels, validation_preds)\ndisp = ConfusionMatrixDisplay(cm)\ndisp.plot();\n\n              precision    recall  f1-score   support\n\n           0      0.718     0.806     0.759       319\n           1      0.786     0.693     0.737       329\n\n    accuracy                          0.748       648\n   macro avg      0.752     0.749     0.748       648\nweighted avg      0.753     0.748     0.748       648\n\n\n\n\n\n\n\n\n\n\n\ndef count_misclassified_sentences(df: pd.DataFrame, preds: List[int],labels: List[int]) -&gt; Tuple[int, int, int]:\n  count_total = 0\n  count_en = 0\n  count_es = 0\n\n  for i, pred in enumerate(preds):\n    if pred != labels[i]:\n      count_total += 1\n      if df.iloc[i]['lang'] == 'en':\n        count_en += 1\n      else:\n        count_es += 1\n  return count_total, count_en, count_es\n\n\ncount_total, count_en, count_es = count_misclassified_sentences(validation_df,validation_preds,validation_labels)\nprint(f'Total misclassified sentences: {count_total}')\nprint(f'Misclassified sentences in English: {count_en}')\nprint(f'Misclassified sentences in Spanish: {count_es}')\n\nTotal misclassified sentences: 163\nMisclassified sentences in English: 28\nMisclassified sentences in Spanish: 135\n\n\n\ndecoded_validation = custom_decode(validation_sentences)\nplot_unk_stats(validation_preds, validation_labels, decoded_validation)\n\nStatistics of UNK and OOV terms of Custom Model\n\n\n\n\n  \n    \n\n\n\n\n\n\nSentences\nUNK\nOOV\n\n\n\nTotal\nWrong\nTotal\nWrong\nTotal\nWrong\n\n\n\n\n0\n319\n62\n735\n149\n204\n37\n\n\n1\n329\n101\n648\n228\n212\n64\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nAs we can see, the most common words in the misclassified sentences are predominantly Spanish, due to the fact that Spanish sentences are more prevalent than English ones in the validation split. Moreover, we observe a similar pattern to the previous error analysis with the word ‘woman’. Specifically, the Spanish equivalent, ‘mujer’, is the most frequent word in misclassified sentences with label 0 and, while it is the overall most frequent word in label 1, it appears rarely in misclassified sentences for that label.\n\nplot_wrong_word(validation_preds, validation_labels, decoded_validation, 'Custom Model')\n\n\n\n\n\n\n\n\n\n\nTransformer\nNow we will fine-tune the same transformer architecture used earlier in the monolingual section, adopting the same preprocessing procedure and training hyperparameters. As expected, the performance on the validation set dropped slightly, likely because this architecture was mostly pretrained on English text.\n\nmodel_name = \"cardiffnlp/twitter-roberta-base-hate\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, ignore_mismatched_sizes=True\n)\n\n\ndata = datasets.load_dataset(\n    \"csv\",\n    data_files={\n        \"train\": \"train_multilingual.csv\",\n        \"validation\": \"validation_multilingual.csv\",\n    }\n)\ndata = data.map(preprocess_text, batched=True)\ndata = data.rename_column(\"hard_label_task1\", \"label\")\n\n\n\n\n\n\n\n\n\n\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n\n\n\n\n\n\nfor name, param in model.named_parameters():\n  if 'classifier' in name or 'encoder.layer.10' in name or 'encoder.layer.11' in name or 'encoder.layer.9' in name:\n    param.requires_grad = True\n  else:\n    param.requires_grad = False\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"test_dir\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    metric_for_best_model=\"loss\",\n    greater_is_better=False,\n    seed=best_seed,\n    logging_strategy='epoch'\n)\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=data[\"train\"],\n    eval_dataset=data[\"validation\"],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n\nvalidation_info = trainer.predict(data[\"validation\"])\ny_pred_transformer, y_true = validation_info.predictions, validation_info.label_ids\nvalidation_metrics = compute_metrics([y_pred_transformer, y_true])\nprint(\n    f'Transformer pre-finetuning validation f1_score {validation_metrics[\"f1\"]:.4f}, accuracy {validation_metrics[\"acc\"]:.4f}'\n)\n\n\n\n\nTransformer pre-finetuning validation f1_score 0.3731, accuracy 0.5093\n\n\n\nvalidation_pred_transformer = np.argmax(softmax(y_pred_transformer,axis=1),axis=1)\n\ncount_total, count_en, count_es = count_misclassified_sentences(validation_df,validation_pred_transformer,y_true)\nprint(f'Total misclassified sentences: {count_total}')\nprint(f'Misclassified sentences in English: {count_en} out of {len(validation_df[validation_df[\"lang\"] == \"en\"])}')\nprint(f'Misclassified sentences in Spanish: {count_es} out of {len(validation_df[validation_df[\"lang\"] == \"es\"])}')\n\nTotal misclassified sentences: 318\nMisclassified sentences in English: 58 out of 158\nMisclassified sentences in Spanish: 260 out of 490\n\n\n\ntrainer.train()\n\n\n    \n      \n      \n      [4548/7580 08:09 &lt; 05:26, 9.29 it/s, Epoch 6/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nModel Preparation Time\nF1\nAcc\n\n\n\n\n1\n0.524200\n0.501840\n0.003700\n0.760674\n0.760802\n\n\n2\n0.455300\n0.508893\n0.003700\n0.760426\n0.762346\n\n\n3\n0.422200\n0.486943\n0.003700\n0.808525\n0.808642\n\n\n4\n0.379600\n0.500273\n0.003700\n0.795968\n0.796296\n\n\n5\n0.346000\n0.551903\n0.003700\n0.817899\n0.817901\n\n\n6\n0.311200\n0.662030\n0.003700\n0.802401\n0.802469\n\n\n\n\n\n\nTrainOutput(global_step=4548, training_loss=0.40640132303497933, metrics={'train_runtime': 489.5987, 'train_samples_per_second': 123.857, 'train_steps_per_second': 15.482, 'total_flos': 1693760974439040.0, 'train_loss': 0.40640132303497933, 'epoch': 6.0})\n\n\n\nvalidation_info = trainer.predict(data[\"validation\"])\ny_pred_transformer, y_true = validation_info.predictions, validation_info.label_ids\nvalidation_metrics = compute_metrics([y_pred_transformer, y_true])\nprint(\n    f'Transformer best validation f1_score {validation_metrics[\"f1\"]:.4f}, accuracy {validation_metrics[\"acc\"]:.4f}'\n)\n\n\n\n\nTransformer best validation f1_score 0.8085, accuracy 0.8086\n\n\n\nvalidation_pred_transformer = np.argmax(softmax(y_pred_transformer,axis=1),axis=1)\n\ncount_total, count_en, count_es = count_misclassified_sentences(validation_df,validation_pred_transformer,y_true)\nprint(f'Total misclassified sentences: {count_total}')\nprint(f'Misclassified sentences in English: {count_en} out of {len(validation_df[validation_df[\"lang\"] == \"en\"])}')\nprint(f'Misclassified sentences in Spanish: {count_es} out of {len(validation_df[validation_df[\"lang\"] == \"es\"])}')\n\nTotal misclassified sentences: 124\nMisclassified sentences in English: 17 out of 158\nMisclassified sentences in Spanish: 107 out of 490\n\n\n\nprint(classification_report(validation_labels, validation_pred_transformer, digits=3))\ncm = confusion_matrix(validation_labels, validation_pred_transformer)\ndisp = ConfusionMatrixDisplay(cm)\ndisp.plot();\n\n              precision    recall  f1-score   support\n\n           0      0.812     0.796     0.804       319\n           1      0.806     0.821     0.813       329\n\n    accuracy                          0.809       648\n   macro avg      0.809     0.808     0.809       648\nweighted avg      0.809     0.809     0.809       648\n\n\n\n\n\n\n\n\n\n\nIn the chart below we observe the same pattern previously identified in the custom model regarding the word ‘mujer’. In this case ‘mujer’ does not appear in the right chart, indicating that it is rarely misclassified in label 1 sentences.\n\nplot_wrong_word(validation_pred_transformer, validation_labels, decoded_validation, 'Transformer')\n\n\n\n\n\n\n\n\n\n\nTransformer Multilingual\nNow we will compare the previous models with the following one, which is also a Transformer-based architecture but pretrained on multilingual corpora. Since this model was originally trained for multiclass classification (3 labels), we replaced and fine-tuned the final classification head to adapt it to our binary classification task.\n\nmodel_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=2, ignore_mismatched_sizes=True\n)\n\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual and are newly initialized because the shapes did not match:\n- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\ndata = datasets.load_dataset(\n    \"csv\",\n    data_files={\n        \"train\": \"train_multilingual.csv\",\n        \"validation\": \"validation_multilingual.csv\",\n    }\n)\ndata = data.map(preprocess_text, batched=True)\ndata = data.rename_column(\"hard_label_task1\", \"label\")\n\n\n\n\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n\n\n\n\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=data[\"train\"],\n    eval_dataset=data[\"validation\"],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n\ntrainer.train()\n\n\n    \n      \n      \n      [3032/7580 13:42 &lt; 20:34, 3.68 it/s, Epoch 4/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1\nAcc\n\n\n\n\n1\n0.472000\n0.404488\n0.856193\n0.856481\n\n\n2\n0.366600\n0.599462\n0.825397\n0.825617\n\n\n3\n0.270100\n0.573474\n0.836418\n0.836420\n\n\n4\n0.156300\n1.179024\n0.801351\n0.804012\n\n\n\n\n\n\nTrainOutput(global_step=3032, training_loss=0.31624828731164456, metrics={'train_runtime': 822.6802, 'train_samples_per_second': 73.71, 'train_steps_per_second': 9.214, 'total_flos': 779186946007680.0, 'train_loss': 0.31624828731164456, 'epoch': 4.0})\n\n\n\nvalidation_info = trainer.predict(data[\"validation\"])\ny_pred_transformer, y_true = validation_info.predictions, validation_info.label_ids\nvalidation_metrics = compute_metrics([y_pred_transformer, y_true])\nprint(\n    f'Transformer validation f1_score {validation_metrics[\"f1\"]:.4f}, accuracy {validation_metrics[\"acc\"]:.4f}'\n)\n\n\n\n\nTransformer validation f1_score 0.8562, accuracy 0.8565\n\n\n\nvalidation_pred_transformer = np.argmax(softmax(y_pred_transformer,axis=1),axis=1)\n\ncount_total, count_en, count_es = count_misclassified_sentences(validation_df,validation_pred_transformer,y_true)\nprint(f'Total misclassified sentences: {count_total}')\nprint(f'Misclassified sentences in English: {count_en} out of {len(validation_df[validation_df[\"lang\"] == \"en\"])}')\nprint(f'Misclassified sentences in Spanish: {count_es} out of {len(validation_df[validation_df[\"lang\"] == \"es\"])}')\n\nTotal misclassified sentences: 93\nMisclassified sentences in English: 21 out of 158\nMisclassified sentences in Spanish: 72 out of 490\n\n\n\nprint(classification_report(validation_labels, validation_pred_transformer, digits=3))\ncm = confusion_matrix(validation_labels, validation_pred_transformer)\ndisp = ConfusionMatrixDisplay(cm)\ndisp.plot();\n\n              precision    recall  f1-score   support\n\n           0      0.877     0.824     0.850       319\n           1      0.839     0.888     0.863       329\n\n    accuracy                          0.856       648\n   macro avg      0.858     0.856     0.856       648\nweighted avg      0.858     0.856     0.856       648\n\n\n\n\n\n\n\n\n\n\n\nplot_wrong_word(validation_pred_transformer, validation_labels, decoded_validation, 'Transformer')\n\n\n\n\n\n\n\n\nAs previously shown, the model’s performance is significantly better than that of the two previous models. Additionally, unlike the other models, the ratio of misclassified sentences is consistent across languages. Lastly we observe the same ‘mujer’ error pattern in the charts above."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Seminar Speech/seminar_speech.html",
    "href": "nlp-course-material/2024-2025/Seminar Speech/seminar_speech.html",
    "title": "Seminar: The Language of Sound: A Journey Through Speech Processing and Multimodality",
    "section": "",
    "text": "Credits: Eleonora Mancini\nKeywords: Speech Processing, Speech Foundation Models"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Seminar Speech/seminar_speech.html#contact",
    "href": "nlp-course-material/2024-2025/Seminar Speech/seminar_speech.html#contact",
    "title": "Seminar: The Language of Sound: A Journey Through Speech Processing and Multimodality",
    "section": "Contact",
    "text": "Contact\nFor any doubt, question, issue or help, you can always contact us at the following email addresses:\n\nEleonora Mancini -&gt; e.mancini@unibo.it"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Seminar Speech/seminar_speech.html#audio-processing-with-librosa",
    "href": "nlp-course-material/2024-2025/Seminar Speech/seminar_speech.html#audio-processing-with-librosa",
    "title": "Seminar: The Language of Sound: A Journey Through Speech Processing and Multimodality",
    "section": "Audio Processing with Librosa",
    "text": "Audio Processing with Librosa\nIn this section, we will use Librosa to load an audio file, play it, and visualize the waveform, spectrogram, and mel spectrogram.\nNote that the same operations can be performed using torchaudio.\nSteps: 1. Load the audio file using Librosa. 2. Play the audio. 3. Plot the waveform of the audio. 4. Compute and display the spectrogram. 5. Compute and display the mel spectrogram.\nBefore proceeding, make sure you have the required libraries installed:\n\n!pip install librosa matplotlib numpy \n\n\n# Import necessary libraries\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import Audio\n\n\nLoad an Audio File\n\naudio_path = 'audio_files/hello_sound.wav'  # Replace with your file path\ny, sr = librosa.load(audio_path, sr=None)  # y is the audio signal, sr is the sample rate\n\n\n\nReproduce the Audio\n\nAudio(audio_path)\n\n\n\nPlot the Waveform\n\nplt.figure(figsize=(10, 4))\nlibrosa.display.waveshow(y, sr=sr)\nplt.title('Waveform')\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\nplt.show()\n\n\n\nCompute and Display the Spectrogram\n\n\nUnderstanding the STFT in Librosa\n\nKey Concepts:\n\nSTFT and Segments:\nWhen computing an STFT, the signal is divided into several short segments, each of length n_fft. The FFT is then computed for each segment. To avoid losing information, these segments often overlap, meaning the distance between consecutive segments is typically less than n_fft. This overlap is controlled by the hop_length, which is the number of audio samples between the starts of consecutive FFTs.\nHop Length in Samples:\nThe hop_length is defined in samples. For example:\n\nIf you have 1000 audio samples and hop_length = 100, you get 10 feature frames.\n\nIf n_fft &gt; hop_length, padding may be applied to ensure all samples are processed.\n\nFrame Rate:\nThe rate at which feature frames are generated can be calculated as:\n\\[\\text{frame\\_rate} = \\frac{\\text{sample\\_rate}}{\\text{hop\\_length}}\\]\nFor example, with:\n\nsample_rate = 22050 Hz\n\nDefault hop_length = 512\n\nThe frame rate is:\n\\[\\frac{22050}{512} \\approx 43 \\, \\text{frames per second}\\]\nOutput Dimensions:\nFor an audio clip of 10 seconds at 22050 Hz, the resulting spectrogram will have dimensions of approximately:\n\nFrequency bins: Defined by the Mel filterbank (e.g., 128 Mel bins).\n\nFeature frames: Determined by the audio duration and the hop_length.\n\nExample:\n\nAudio duration = 10s\n\nSample rate = 22050 Hz\n\nhop_length = 512\n\nNumber of feature frames:\n\\[\\text{frames} = \\frac{\\text{audio\\_samples}}{\\text{hop\\_length}} = \\frac{22050 \\times 10}{512} \\approx 430\\]\nResulting spectrogram dimensions:\n\\[(128, 430)\\]\nwhere 128 is the number of Mel bins, and 430 is the number of feature frames.\nNote: Padding can slightly alter these dimensions.\n\n\n\n\n\nWith and Without Mel Bins\n\nWithout Mel Bins:\n\nFrequency Representation:\nThe spectrogram represents the raw linear frequency scale directly from the STFT. The number of frequency bins is:\n\\[\\text{n\\_fft} / 2 + 1\\]\nFor example, if ( = 1024 ), there are ( 1024 / 2 + 1 = 513 ) frequency bins.\nResolution:\nAll frequencies are equally spaced, which does not match the logarithmic nature of human hearing. This is suitable for applications requiring precise frequency information, such as pitch detection or music transcription, but less interpretable for tasks involving human perception.\nOutput Dimensions:\nSpectrogram dimensions will be ((/2+1, )), where num_frames depends on the audio duration and hop_length.\n\n\n\n\nWith Mel Bins:\n\nFrequency Representation:\nFrequencies are mapped to the Mel scale, which is perceptually motivated. Lower frequencies have finer resolution, and higher frequencies are compressed.\nReduced Dimensionality:\nA Mel filterbank reduces the number of frequency bins to a fixed number (e.g., 128 or 40). This reduces computational complexity and focuses on perceptually relevant features.\nOutput Dimensions:\nSpectrogram dimensions become ((, )), where num_mel_bins is user-defined.\nApplications:\nMel-scaled spectrograms are better suited for tasks like speech recognition, emotion detection, and other tasks involving human perception.\n\n\n\nD = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)  # Compute the STFT and convert to dB\n\nplt.figure(figsize=(10, 6))\nlibrosa.display.specshow(D, x_axis='time', y_axis='log', sr=sr)\nplt.colorbar(format='%+2.0f dB')\nplt.title('Spectrogram')\nplt.show()\n\n\n\n\nCompute and display the Mel Spectrogram\n\nS = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)  # Mel spectrogram\n\n# Convert to decibels\nS_db = librosa.power_to_db(S, ref=np.max)\n\nplt.figure(figsize=(10, 6))\nlibrosa.display.specshow(S_db, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\nplt.colorbar(format='%+2.0f dB')\nplt.title('Mel Spectrogram')\nplt.show()\n\n\n\nWhy Do We Need to Perform AmplitudeToDB?\nIn audio signal processing, converting the amplitude of an audio signal into a decibel (dB) scale is a common practice for a few key reasons:\n\nHuman Hearing Perception: The human auditory system perceives sound in a logarithmic manner. This means that we do not perceive differences in sound intensity (loudness) in a linear way. A sound that is 10 times more intense than another is not perceived as 10 times louder. Instead, it is perceived in a more compressed scale. The decibel scale closely matches this perception, making it more intuitive when working with audio data.\nDynamic Range Compression: Audio signals, especially raw waveforms, often have a wide dynamic range. That is, they contain both very quiet and very loud sections. A signal’s dynamic range can span several orders of magnitude. By converting amplitudes into dB, we compress this range, making it easier to visualize and analyze without losing important details. This is particularly useful for operations like spectrograms, where low-energy components would otherwise be hard to distinguish from noise.\nLogarithmic Representation: The decibel scale is logarithmic, which helps compress very large values and make the differences more noticeable in a manageable range. For instance, very large or very small values (in terms of amplitude) are normalized into a range that is easier to visualize or process. This can help reveal patterns in the data that would be difficult to detect in a purely linear scale.\nStandard Practice in Audio Analysis: Many audio and speech processing algorithms (such as in speech recognition, audio classification, or music information retrieval) are designed to work with logarithmic representations of audio, like spectrograms in decibels. Converting to dB ensures consistency and comparability across various tools, datasets, and research.\n\n\n\nThe Process: AmplitudeToDB in Detail\nWhen you perform an operation like the Short-Time Fourier Transform (STFT) or Mel-frequency Cepstral Coefficients (MFCCs) on an audio signal, the result is typically a magnitude spectrogram. This magnitude is in linear units, representing raw amplitudes.\nTo convert the raw amplitude spectrogram to a logarithmic scale, we use AmplitudeToDB, which transforms the amplitude values into decibel (dB) values using the formula:\n\\(𝑑𝐵=10×log_{10}(P)\\)\nWhere: - \\(P\\) is the power or amplitude of the signal. This transformation helps improve the perceptual relevance of the data and aids in better visualization of the frequency content.\n\n\nAlternatives to AmplitudeToDB\nWhile AmplitudeToDB is a widely used and standard method for converting amplitude to decibels (dB), there are several alternative approaches or transformations that can be used depending on the context:\n\n1. Logarithmic Scaling (Without AmplitudeToDB)\nYou can apply a direct logarithmic transformation to the magnitude spectrogram or waveform. This method typically operates on the power of the signal, rather than just the amplitude. For example:\n[ = ( + ) ]\nHere, ( ) is a small constant added to avoid taking the logarithm of zero.\nThis approach can approximate decibel scaling but lacks the specific scaling factor (e.g., ( 10 ) or ( 20 )) applied by AmplitudeToDB. It is less common than using AmplitudeToDB, which is specifically designed for audio data.\n\n\n4. Mel Spectrogram\nInstead of converting to dB after computing the spectrogram, you can directly compute a Mel spectrogram. This method applies a non-linear transformation to map frequencies to the Mel scale, which better aligns with human auditory perception.\nThe Mel spectrogram can also be combined with logarithmic scaling to produce Mel-frequency cepstral coefficients (MFCCs), which are widely used in speech and audio processing tasks."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Seminar Speech/seminar_speech.html#inference-example-using-pre-trained-wav2vec2-model",
    "href": "nlp-course-material/2024-2025/Seminar Speech/seminar_speech.html#inference-example-using-pre-trained-wav2vec2-model",
    "title": "Seminar: The Language of Sound: A Journey Through Speech Processing and Multimodality",
    "section": "1. Inference Example: using Pre-trained Wav2Vec2 Model",
    "text": "1. Inference Example: using Pre-trained Wav2Vec2 Model\nIn this section, we will demonstrate how to perform inference using a pre-trained speech model, specifically Wav2Vec2. We will load an audio file, process it, and then use the model to transcribe the speech to text. This will showcase how to quickly leverage powerful models from Hugging Face for automatic speech recognition (ASR) without needing to train the model from scratch.\n\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nimport torchaudio\nimport torch\n\n\nLoad pre-trained Wav2Vec2 model and processor\n\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n\n\n\nLoad and resample audio with PyTorch\nTo handle audio files that may have a sampling rate different from the one used in the pre-trained Wav2Vec2 model (which was trained on 16 kHz audio), we need to add a resampling step to convert the audio to the correct sampling rate before passing it to the model.\n\n# Function to load audio\ndef load_and_resample_audio(file_path, target_sample_rate=16000):\n    waveform, sample_rate = torchaudio.load(file_path)  # Load audio file\n    if sample_rate != target_sample_rate:\n        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n        waveform = resampler(waveform)  # Resample to target sample rate\n    return waveform\n\n\n\nDefine function to transcribe speech\n\ndef transcribe_audio(waveform):\n    # Step 1: Preprocess the waveform (audio) using the processor\n    # The processor tokenizes and prepares the input waveform to be fed into the model.\n    # It expects the audio to have a sampling rate of 16 kHz and converts the waveform into a tensor.\n    # 'padding=True' ensures that the inputs are padded to the correct length.\n    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n    \n    # Step 2: Print the shape of the input tensor\n    # This prints the shape of the processed audio tensor to see its dimensions before feeding it into the model.\n    print(inputs.input_values.shape)\n    \n    # Step 3: Forward pass through the model\n    # With torch.no_grad() to prevent gradients from being calculated (since we are not training the model).\n    # This step passes the processed input values through the model to get the logits (raw output scores).\n    with torch.no_grad():\n        logits = model(input_values=inputs.input_values).logits\n        # logits are the unnormalized predictions made by the model.\n        # They will be used to generate the final predicted transcription.\n        print(logits)\n    \n    # Step 4: Decode predictions\n    # `logits` contain the output predictions of the model. We use `torch.argmax` to get the index of the maximum logit.\n    # This gives us the predicted token IDs (word/phoneme IDs) for the input sequence.\n    predicted_ids = torch.argmax(logits, dim=-1)  # The 'dim=-1' indicates we are taking the max value across the last dimension.\n    \n    # Step 5: Convert the predicted token IDs to text\n    # The processor.decode function converts the token IDs back into a human-readable transcription (string).\n    # Here, we only decode the first (and typically the only) batch in the tensor.\n    transcription = processor.decode(predicted_ids[0])\n    \n    # Step 6: Return the transcription\n    return transcription\n\n\n\nPerform inference\n\nfile_path = \"audio_files/hello_sound.wav\"  # Update with your audio file path\nwaveform, sr = load_and_resample_audio(file_path)\ntranscription = transcribe_audio(waveform)\nprint(\"Transcription:\", transcription)"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Seminar Speech/seminar_speech.html#use-sfms-to-generate-representations-extracting-embeddings",
    "href": "nlp-course-material/2024-2025/Seminar Speech/seminar_speech.html#use-sfms-to-generate-representations-extracting-embeddings",
    "title": "Seminar: The Language of Sound: A Journey Through Speech Processing and Multimodality",
    "section": "2. Use SFMs to Generate Representations: Extracting Embeddings",
    "text": "2. Use SFMs to Generate Representations: Extracting Embeddings\nIn this section, we will demonstrate how to extract embeddings from a pre-trained speech model that can be used for downstream tasks such as speech classification. We will use the Wav2Vec2 model to extract meaningful audio representations (embeddings). These embeddings can be then directly input into a separate classifier for tasks like speaker identification or emotion recognition in speech.\n\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nimport torch\nimport torchaudio\n\n\n# Load pre-trained Wav2Vec2 model and processor\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n\n\n# Function to load audio\ndef load_and_resample_audio(file_path, target_sample_rate=16000):\n    waveform, sample_rate = torchaudio.load(file_path)  # Load audio file\n    if sample_rate != target_sample_rate:\n        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n        waveform = resampler(waveform)  # Resample to target sample rate\n    return waveform\n\n\n# Function to extract embeddings from the model\ndef extract_embeddings(inputs):\n\n    # Get embeddings (hidden states) from the model\n    with torch.no_grad():\n        outputs = model(input_values=inputs, output_hidden_states=True)\n    \n    # Extract hidden states (embeddings) from the last layer\n    hidden_states = outputs.hidden_states[-1]\n    embeddings = torch.mean(hidden_states, dim=1)  # Optionally, take the mean of the hidden states for each time step\n    return embeddings\n\n\n# Example usage for embedding extraction\nfile_path = \"audio_files/hello_sound.wav\"  # Update with your audio file path\nwaveform = load_and_resample_audio(file_path)\nembeddings = extract_embeddings(waveform)\n\n# Now embeddings can be used as input to a downstream classification model\nprint(\"Extracted embeddings:\", embeddings.shape)"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html",
    "title": "Tutorial 3",
    "section": "",
    "text": "Credits: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\nKeywords: Transformers, Huggingface, Prompting"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#part-0-5-mins",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#part-0-5-mins",
    "title": "Tutorial 3",
    "section": "PART 0 ($$5 mins)",
    "text": "PART 0 ($$5 mins)\n\nDownloading a dataset.\nEncoding a a dataset."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#part-i-30-mins",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#part-i-30-mins",
    "title": "Tutorial 3",
    "section": "PART I ($$30 mins)",
    "text": "PART I ($$30 mins)\n\nText encoding with transformers.\nModel definition.\nModel training and evaluation with huggingface APIs."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#part-ii-30-mins",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#part-ii-30-mins",
    "title": "Tutorial 3",
    "section": "PART II ($$30 mins)",
    "text": "PART II ($$30 mins)\n\nPrompting 101\nSentiment analysis with prompting\nAdvanced prompting"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#preliminaries",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#preliminaries",
    "title": "Tutorial 3",
    "section": "Preliminaries",
    "text": "Preliminaries\nFirst of all, we need to import some useful packages that we will use during this hands-on session.\n\n# system packages\nfrom pathlib import Path\nimport shutil\nimport urllib\nimport tarfile\nimport sys\n\n# data and numerical management packages\nimport pandas as pd\nimport numpy as np\n\n# useful during debugging (progress bars)\nfrom tqdm import tqdm\n\n\n!pip install transformers\n!pip install datasets\n!pip install accelerate -U\n!pip install evaluate\n!pip install bitsandbytes\n\n\nimport torch\ntorch.cuda.is_available()\n\n\n!nvidia-smi\n\n\nfrom notebook.services.config import ConfigManager\ncm = ConfigManager()\ncm.update('livereveal', {\n        'width': 2560,\n        'height': 1440,\n        'scroll': True,\n})"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#text-encoding-with-transformers.",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#text-encoding-with-transformers.",
    "title": "Tutorial 3",
    "section": "1. Text encoding with Transformers.",
    "text": "1. Text encoding with Transformers.\nIn tutorial 1, we have seen how to define standard machine learning models to address sentiment classification.\nHowever, we know that Transformer-based models are one of the strongest baselines when assessing a task or benchmarking on a novel corpus.\nBefore defining our transformer-based classifier, we need to encode text inputs into numerical format.\nAs in Tutorial 1, we are going to tokenize input texts to perform token indexing.\n\n1.1 Encoding the dataset\nFirst, we are going to use datasets library to encode our dataset into a handy wrapper for computational speedup.\n\nfrom datasets import Dataset\n\n# Slicing for showcasing purposes only!\ntrain_df = df.loc[df['split'] == \"train\"].sample(frac=1.0)[:5000]\ntest_df = df.loc[df['split'] == \"test\"].sample(frac=1.0)[:1000]\n\ntrain_data = Dataset.from_pandas(train_df)\ntest_data = Dataset.from_pandas(test_df)\n\nLet’s inspect the newly defined Dataset instances\n\nprint(train_data)\nprint(test_data)\n\n\n\n1.2 Tokenization\nTransformers typically use SentencePiece tokenizer to perform sub-word level tokenization.\nIn particular, the transformers library offers the AutoTokenizer class to quickly retrieve our chosen transformer’s ad-hoc tokenizer.\n\nfrom transformers import AutoTokenizer\n\nmodel_card = 'distilbert-base-uncased'\n\ntokenizer = AutoTokenizer.from_pretrained(model_card)\n\nThe model_card variable defines the path where to look for our pre-trained model.\nYou can check huggingface’s hub model hub to pick the model card according to your preference.\nWe proceed on tokenizing movie reviews text with our tokenizer.\n\ndef preprocess_text(texts):\n    return tokenizer(texts['text'], truncation=True)\n\ntrain_data = train_data.map(preprocess_text, batched=True)\ntest_data = test_data.map(preprocess_text, batched=True)\n\nLet’s inspect the preprocess Dataset instances\n\nprint(train_data)\nprint(test_data)\n\n\nprint(train_data['input_ids'][50])\n\n\nprint(train_data['attention_mask'][50])\n\nWe can perform some quick sanity check to evaluate the tokenization process\n\noriginal_text = train_data['text'][50]\ndecoded_text = tokenizer.decode(train_data['input_ids'][50])\n\nprint(original_text)\nprint()\nprint()\nprint(decoded_text)\n\n\n\n1.3 Vocabulary\nWe do not necessarily need to build a vocabulary since transformers already come with their own!\nHowever, it is still possible to add new tokens to the vocabulary to adapt the model to the given use case.\ntokenizer.add_tokens(new_tokens=new_tokens)\nThe transformer vocabulary will update its unusued vocabulary indexes with newly provided tokens.\n\n\n1.4 Special tokens\nPay attention to used special tokens and their corresponding token ids.\nEach transformer models has its own special tokens ([CLS], [SEP], [PAD], [EOS], etc…).\nThus, the same special token may be mapped to different token ids in distinct transformer models.\n\n\n1.5 Text cleaning\nWe didn’t perform any kind of text cleaning before performing text encoding.\nThis is usually because transformer tokenizers have their own text cleaning process to perform tokenization.\nThus, models may be sensitive to custom operations!\n\nexample_text = \"couldn't\"\nencoded_example = tokenizer.encode_plus(example_text, add_special_tokens=False)\nprint(encoded_example.tokens())\n\n\nexample_text = \"At one point,some kids are wandering through the deeper levels, exploring.\"\nencoded_example = tokenizer.encode_plus(example_text, add_special_tokens=False)\nprint(encoded_example.tokens())\n\n\nExample\nbert-base-uncased is trained with text in lower format.\nCheck model cards on huggingface to know more about the models you use and inspect their text encoding pipeline to understand how they behave.\n\n\nHomework 📖\nExperiment with different model cards.\nExperiment with text cleaning and evaluate its impact on classification."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#model-definition",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#model-definition",
    "title": "Tutorial 3",
    "section": "2. Model definition",
    "text": "2. Model definition\nWe are now ready to define our transformer-based classifier.\n\n2.1 Data Formatting\nWe first need to format input data to be fed as mini-batches in a training/evaluation procedure.\n\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nThe DataCollatorWithPadding receives a batch of\n(input_ids, attention_mask, token_type_ids, label)\ntuples and dynamically pads input_ids, attention_mask and token_type_ids to maximum sequence in the batch.\nIntuitively, this operation saves a lot of memory compared to padding to global maximum sequence, while it introduces a reasonable computational overhead.\n\n\nNote\nThe above example is just one way out of many to perform dynamic batch padding: it really depends on which data structures you are using.\n\n\n2.2 Model definition\nDefining a transformer-based model with huggingface is pretty straightforward!\nSince we are dealing with text classification, we can use off-the-shelf AutoModelForSequenceClassification.\n\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_card,\n                                                           num_labels=2,\n                                                           id2label={0: 'NEG', 1: 'POS'},\n                                                           label2id={'NEG': 0, 'POS': 1})\n\nLet’s first check the loaded model architecture.\n\nprint(model)\n\nThat’s it!\nThat’s the simplicity of huggingface’s APIs.\nThe model is ready to use for classification.\n\n\n2.3 Custom architectures\nThere are plenty of pre-defined model architectures \\(\\rightarrow\\) auto classes\nIn more complex scenarios, we may want to define a custom architecture where the pre-trained model is part of it.\nIn these cases, the way you do it strongly depends on the underlying neural library.\nHowever, there exist several high-level APIs depending on your needs."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#model-training-and-evaluation",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#model-training-and-evaluation",
    "title": "Tutorial 3",
    "section": "3. Model training and evaluation",
    "text": "3. Model training and evaluation\nWe are now ready to define the training and evaluation procedures to test our model on the IMDB dataset.\nIn particular, we are going to use Trainer APIs to efficiently perform training.\n\n3.1 Metrics\nFirst, we define classification metrics for evaluation.\n\nfrom sklearn.metrics import f1_score, accuracy_score\n\ndef compute_metrics(output_info):\n    predictions, labels = output_info\n    predictions = np.argmax(predictions, axis=-1)\n    \n    f1 = f1_score(y_pred=predictions, y_true=labels, average='macro')\n    acc = accuracy_score(y_pred=predictions, y_true=labels)\n    return {'f1': f1, 'acc': acc}\n\n\n\nHugginface’s metrics\nHuggingface’s offers the evaluate package that contains several evaluation metrics (e.g., accuracy, f1, squad-f1, etc…)\n\nimport evaluate\n\nacc_metric = evaluate.load('accuracy')\nf1_metric = evaluate.load('f1')\n\ndef compute_metrics(output_info):\n    predictions, labels = output_info\n    predictions = np.argmax(predictions, axis=-1)\n    \n    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n    acc = acc_metric.compute(predictions=predictions, references=labels)\n    return {**f1, **acc}\n    \n\n\n\n3.2 Training Arguments\nThe Trainer object can be extensively customized.\nFeel free to check the documentation on training arguments.\nWe first rename the sentiment column to label as the default input to AutoModelForSequenceClassification.\n\ntrain_data = train_data.rename_column('sentiment', 'label')\ntest_data = test_data.rename_column('sentiment', 'label')\n\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"test_dir\",                 # where to save model\n    learning_rate=2e-5,                   \n    per_device_train_batch_size=8,         # accelerate defines distributed training\n    per_device_eval_batch_size=8,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",           # when to report evaluation metrics/losses\n    save_strategy=\"epoch\",                 # when to save checkpoint\n    load_best_model_at_end=True,\n    report_to='none'                       # disabling wandb (default)\n)\n\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n\n\nTraining schema with collator\n\n\n\n\ntrainer.train()\n\n\n\n3.3 Evaluation\nWe now evaluate the trained model on the test set.\n\ntest_prediction_info = trainer.predict(test_data)\ntest_predictions, test_labels = test_prediction_info.predictions, test_prediction_info.label_ids\n\nprint(test_predictions.shape)\nprint(test_labels.shape)\n\n\ntest_metrics = compute_metrics([test_predictions, test_labels])\nprint(test_metrics)\n\n\n\nSome cleaning before PART II\nLet’s clean the memory and GPU before switching to instruction-tuned models.\n\nimport gc\n\nmodel = None\ndel model\ntrainer = None\ndel trainer\n\nwith torch.no_grad():\n    torch.cuda.empty_cache()\n\ngc.collect()"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#prompting-101",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#prompting-101",
    "title": "Tutorial 3",
    "section": "1. Prompting 101",
    "text": "1. Prompting 101\nPrompting is a technique used to adapt a model to a variety of tasks without requiring fine-tuning.\nClassify the text into neutral, negative or positive.\nText: {text}\nSentiment:\nThe model receives the above input prompt and performs text classification via completion.\nClassify the text into neutral, negative or positive.\nText: {text}\nSentiment: {label}\nIn natural language, prompting is a very delicate process since natural language is expressive, flexible, and, ambiguous.\nA certain concept can be expressed in several ways:\n\nThese ways are semantically equivalent\nMay lead to significant model performance drifts\n\n\n1.1 Sensitivity Factors\nThere are two main factors to consider when performing prompt-based learning.\n\nPrompt Engineering\nEventually we have to iteratively find the best performing prompt.\nThis can either done\n\nManually\nAutomatically (via an ad-hoc model).\n\n\n\nGeneration hyper-parameters\nFinding the optimal text generation strategy is a critical point for achieving satisfying performance.\nThese strategies affects how the model iteratively selects tokens during generation to avoid phenomena like repetitions, rare words, coherence with input text, and style.\n\n[Deterministic] Greedy \\(\\rightarrow\\) the most preferred (i.e., highest likelihood) token wins\n[Deterministic] Beam search\n[Stochastic] Top-k sampling\n[Stochastic] Nucleus sampling (or top-p sampling)\nContrastive search \\(\\leftarrow\\) recommended\n\n\n\n\n1.2 Model types\nThere are a lot of different large language models and it is quite easy to be confused.\nEssentially, we have:\n\nBase models (either encoders or encode-decoders): very good at text completion.\nInstruct-based models: base models specifically fine-tuned to address instructions.\nChat-based models: models specifically fine-tuned to chat.\n\n\nExample\nIn Huggingface, the distinct is easily formatted as:\n\nllama2-7b \\(\\rightarrow\\) base model\nllama2-7b-instruct \\(\\rightarrow\\) instruct-based model\nllama-7b-chat \\(\\rightarrow\\) chat-based model\n\n\n\n\n1.3 Preliminaries\nWe are going to download LLMs from Huggingface.\nMany of these open-source LLMs require you to accept their “Community License Agreement” to download them.\nIn summary:\n\nIf not already, create an account of Huggingface (~2 mins)\nCheck a LLM model card page (e.g., Mistral v3) and accept its “Community License Agreement”.\nGo to your account -&gt; Settings -&gt; Access Tokens -&gt; Create new token -&gt; “Repositories permissions” -&gt; add the LLM model card you want to use.\nSave the token (we’ll need it later)\n\nOnce we have created an account and an access token, we need to login to Huggingface via code.\n\nType your token and press Enter\nYou can say No to Github linking\n\n\n!huggingface-cli login\n\nAfter login, you can download all models associated with your access token in addition to those that are not protected by an access token."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#sentiment-analysis-with-prompting",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#sentiment-analysis-with-prompting",
    "title": "Tutorial 3",
    "section": "2. Sentiment analysis with prompting",
    "text": "2. Sentiment analysis with prompting\nLet’s consider our task once again to evaluate prompt-based models.\n\n2.1 Model pipeline\nFirst, we have to define the model pipeline to digest input prompts.\n\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n\nmodel_card = \"microsoft/Phi-3.5-mini-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_card)\ntokenizer.pad_token = tokenizer.eos_token\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"&lt;|eot_id|&gt;\")\n]\n\n\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_card,\n    return_dict=True,\n    quantization_config=bnb_config,\n    device_map='auto'\n)\n\n\ngeneration_config = model.generation_config\ngeneration_config.max_new_tokens = 100\ngeneration_config.eos_token_id = tokenizer.eos_token_id\ngeneration_config.pad_token_id = tokenizer.eos_token_id\ngeneration_config.temperature = None\ngeneration_config.num_return_sequences = 1\n\n\nHomework 📖\nExperiment with different model cards (either base or chat-base models)\n\n\n\n2.2 Prompt Template\nWe first define the prompt template to format data samples.\n\nprompt = [\n    {\n        'role': 'system',\n        'content': 'You are an annotator for sentiment analysis.'\n    },\n    {\n        'role': 'user',\n        'content': \"\"\"Classify the text into negative or positive.\n        Respond only POSITIVE or NEGATIVE.\n\n        TEXT: \n        {text}\n\n        SENTIMENT:\n        \"\"\"\n    }\n]\nprompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n\nLet’s inspect the formatted prompt.\n\nprint(prompt)\n\n\n\n2.2 Inference\nWe are now ready to feed prompts to our model and evaluate its performance.\nLet’s start with an example.\n\nexample_text = \"This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\"\nformatted_example = prompt.format(text=example_text)\nparsed_example = tokenizer(formatted_example, return_tensors='pt').to('cuda')\ngenerated = model.generate(input_ids=parsed_example['input_ids'],\n                           attention_mask=parsed_example['attention_mask'],\n                           generation_config=generation_config,\n                           do_sample=False)\noutput = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\nprint(output)\n\nNow we try with the whole test set.\n\ntest_df = df.loc[df['split'] == \"test\"].sample(frac=1.0)[:100]\ntest_data = Dataset.from_pandas(test_df)\n\n\nfrom transformers import DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\n\ndef prepend_prompt(example):\n    example['text'] = prompt.format(text=example['text'])\n    return example\n\ndef collate_fn(batch):\n    texts = tokenizer.batch_encode_plus([it['text'] for it in batch], return_tensors='pt', padding=True, truncation=True)\n    sentiment = th.tensor([it['sentiment'] for it in batch])\n    return texts, sentiment\n\ntest_data = test_data.map(prepend_prompt)\ntest_data = test_data.select_columns(['text', 'sentiment'])\ndata_loader = DataLoader(test_data,\n                         batch_size=1,\n                         shuffle=False,\n                         collate_fn=collate_fn)\n\nBefore running the inference loop, we define a function to parse generated outputs into classification labels.\n\nimport re\n\ndef extract_response(response):\n    match = [m for m in re.finditer('SENTIMENT:', response)][-1]\n    parsed = response[match.end():].strip()\n    return parsed\n\ndef convert_response(response):\n    return [0, 1] if 'positive' in response.casefold() else [1, 0]\n\n\nraw_responses = []\npredictions = []\nwith th.inference_mode():\n    for batch_x, batch_y in tqdm(data_loader, desc=\"Generating responses\"):\n        response = model.generate(\n            input_ids=batch_x['input_ids'].to(model.device),\n            attention_mask=batch_x['attention_mask'].to(model.device),\n            generation_config=generation_config,\n            do_sample=False,\n            use_cache=True\n        )\n        raw_response = tokenizer.batch_decode(response, skip_special_tokens=True)\n        raw_response = [extract_response(item) for item in raw_response]\n        raw_responses.extend(raw_response)\n        batch_predictions = [convert_response(item) for item in raw_response]\n        predictions.extend(batch_predictions)\n\nWe now compute classification metrics.\n\npredictions = np.array(predictions)\nground_truth = np.array(test_data['sentiment'])\nmetrics = compute_metrics([predictions, ground_truth])\nprint(metrics)"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#advanced-prompting",
    "href": "nlp-course-material/2024-2025/Tutorial 3/Tutorial 3.html#advanced-prompting",
    "title": "Tutorial 3",
    "section": "3. Advanced Prompting",
    "text": "3. Advanced Prompting\nThere is no rule of thumb to perform well on prompting.\nSome may argue it is art, some others might say it is just engineering.\nHowever, here are some general recommendations:\n\nCheck how the pre-trained model you are using was trained!\nStart simple and then refine.\nInstructions at the start/end of the prompt \\(\\rightarrow\\) based on how most attention layers work.\nSeparate input text from instructions\nProvide clear description of the task: no ambiguity, text format, style, language, etc…\nEvaluate the prompt on several models\nUse advanced techniques: few-shot prompting, Chain-of-thought (CoT), Least-to-Most (LtM)\n\n\n3.1 From Zero- to Few-shot Prompting\nIn many situations, a prompt containing instructions is not sufficient for a model to behave properly.\nWe can improve the prompt by providing a few ground-truth examples showing how the model should behave.\nClassify the text into negative or positive. \nText: {example1}\nSentiment: {label1}\nText: {example2}\nSentiment: {label2}\nText: {example3}\nSentiment: {label3}\nText: {text}\nSentiment:\n\nExamples may be insufficient\nDepending on the task at hand, providing examples may be not sufficient for the model to understand the instructions.\nAlso, the model might ignore provided examples or it might still perform correctly despite using intentionally wrong examples!!\n\n\nLengthy prompts\nAdding examples increases the level of detail of prompt, while it may considerably increases its length.\nPay attention to what model_card you choose since your model may truncate input prompts!\nAdditionally, a lengthy prompt increases computation!\n\n\nExamples quality\nChoosing the right set of examples has an impact on model performance.\nIntuitively, we select examples to maximize (textual) diversity and cover the whole label distribution.\nIn practice, this may be harder than expected: models are sensitive to prompt formatting.\nLet’s try sentiment analysis again with Few-shot prompting!\n\nprompt = [\n    {\n        'role': 'system',\n        'content': 'You are an annotator for sentiment analysis.'\n    },\n    {\n        'role': 'user',\n        'content': \"\"\"Classify the text into negative or positive.\n        Respond only POSITIVE or NEGATIVE.\n        \n        Here are some examples you can look at.\n        EXAMPLES:\n        {examples}\n\n        Here is the text to classify.\n\n        TEXT: \n        {text}\n\n        SENTIMENT:\n        \"\"\"\n    }\n]\nprompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n\n\ndemonstrations = [\n    (\"Everything is so well done: acting, directing, visuals, settings, photography, casting. If you can enjoy a story of real people and real love - this is a winner.\", \"POSITIVE\"),\n    (\"This is one of the dumbest films, I've ever seen. It rips off nearly ever type of thriller and manages to make a mess of them all.\", \"NEGATIVE\")\n]\ndemonstrations = '\\n'.join([f'TEXT: {text}\\nSENTIMENT: {sentiment}' for (text, sentiment) in demonstrations])\n\n\nexample_text = \"This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\"\nformatted_example = prompt.format(text=example_text, examples=demonstrations)\nparsed_example = tokenizer(formatted_example, return_tensors='pt').to('cuda')\ngenerated = model.generate(input_ids=parsed_example['input_ids'],\n                           attention_mask=parsed_example['attention_mask'],\n                           generation_config=generation_config,\n                           do_sample=False)\noutput = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\nprint(output)\n\n\ndef prepend_prompt(example):\n    example['text'] = prompt.format(text=example['text'], examples=demonstrations)\n    return example\n\ntest_data = Dataset.from_pandas(test_df)\ntest_data = test_data.map(prepend_prompt)\ntest_data = test_data.select_columns(['text', 'sentiment'])\ndata_loader = DataLoader(test_data,\n                         batch_size=1,\n                         shuffle=False,\n                         collate_fn=collate_fn)\n\n\nraw_responses = []\npredictions = []\nwith th.inference_mode():\n    for batch_x, batch_y in tqdm(data_loader, desc=\"Generating responses\"):\n        response = model.generate(\n            input_ids=batch_x['input_ids'].to(model.device),\n            attention_mask=batch_x['attention_mask'].to(model.device),\n            generation_config=generation_config,\n            do_sample=False,\n            use_cache=True\n        )\n        raw_response = tokenizer.batch_decode(response, skip_special_tokens=True)\n        raw_response = [extract_response(item) for item in raw_response]\n        raw_responses.extend(raw_response)\n        batch_predictions = [convert_response(item) for item in raw_response]\n        predictions.extend(batch_predictions)\n\n\npredictions = np.array(predictions)\nground_truth = np.array(test_data['sentiment'])\nmetrics = compute_metrics([predictions, ground_truth])\nprint(metrics)\n\n\n\nHomework 📖\nExperiment with different few-shot examples and evaluate corresponding model performance.\n\n\n\n3.2 Chain-of-thought (CoT) Prompting\nProviding examples to improve task performance may fail in complex scenarios like reasoning tasks.\nCoT prompting forces the model to generate intermediate reasoning steps before providing the final output.\nCoT can either be achieved via\n\nFew-shot examples on how to perform reasoning\nDefining the prompt to force reasoning (e.g., let’s think step by step)\n\nLet’s try our sentiment analysis task with CoT prompting\n\nprompt = [\n    {\n        'role': 'system',\n        'content': 'You are an annotator for sentiment analysis.'\n    },\n    {\n        'role': 'user',\n        'content': \"\"\"Classify the text into negative or positive.\n        Respond with POSITIVE or NEGATIVE.\n        Think step by step.\n        \n        Here are some examples you can look at.\n        EXAMPLES:\n        {examples}\n\n        Here is the text to classify.\n\n        TEXT: \n        {text}\n\n        SENTIMENT:\n        \"\"\"\n    }\n]\nprompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n\n\nexample_text = \"This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\"\nformatted_example = prompt.format(text=example_text, examples=demonstrations)\nparsed_example = tokenizer(formatted_example, return_tensors='pt').to('cuda')\ngenerated = model.generate(input_ids=parsed_example['input_ids'],\n                           attention_mask=parsed_example['attention_mask'],\n                           generation_config=generation_config,\n                           do_sample=False)\noutput = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\nprint(output)\n\n\ntest_data = Dataset.from_pandas(test_df)\ntest_data = test_data.map(prepend_prompt)\ntest_data = test_data.select_columns(['text', 'sentiment'])\ndata_loader = DataLoader(test_data,\n                         batch_size=1,\n                         shuffle=False,\n                         collate_fn=collate_fn)\n\n\nraw_responses = []\npredictions = []\nwith th.inference_mode():\n    for batch_x, batch_y in tqdm(data_loader, desc=\"Generating responses\"):\n        response = model.generate(\n            input_ids=batch_x['input_ids'].to(model.device),\n            attention_mask=batch_x['attention_mask'].to(model.device),\n            generation_config=generation_config,\n            do_sample=False,\n            use_cache=True\n        )\n        raw_response = tokenizer.batch_decode(response, skip_special_tokens=True)\n        raw_response = [extract_response(item) for item in raw_response]\n        raw_responses.extend(raw_response)\n        batch_predictions = [convert_response(item) for item in raw_response]\n        predictions.extend(batch_predictions)\n\n\npredictions = np.array(predictions)\nground_truth = np.array(test_data['sentiment'])\nmetrics = compute_metrics([predictions, ground_truth])\nprint(metrics)\n\n\nHomework 📖\nExperiment with different CoT prompts to enforce intermediate reasoning steps.\nFor more details check this page.\n\n\n\n3.3 Prompting vs Fine-tuning\nAt last, we may wondering on which technique to use.\nIn short, prompting comes at hand when transferring a pre-trained model on a domain that has some affinities with those seen during training.\nIn other cases like:\n\nDifferent domain\nSensitive data\nLow-resource language\nDomain-specific model constraints\n\nFine-tuning is the preferred choice (to maximize improvements)"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html",
    "title": "Tutorial 1",
    "section": "",
    "text": "Credits: Andrea Galassi, Federico Ruggeri, Eleonora Mancini, Paolo Torroni\nKeywords: Data loading, Feature Extraction, Machine Learning, Text Classification, Sentiment Analysis"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#important",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#important",
    "title": "Tutorial 1",
    "section": "Important",
    "text": "Important\nMake sure to put all of us in cc when contacting us via mail!"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#part-i-25-mins",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#part-i-25-mins",
    "title": "Tutorial 1",
    "section": "PART I ($$25 mins)",
    "text": "PART I ($$25 mins)\n\nHow to download a dataset.\nHow to inspect a dataset."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#part-ii-25-mins",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#part-ii-25-mins",
    "title": "Tutorial 1",
    "section": "PART II ($$25 mins)",
    "text": "PART II ($$25 mins)\n\nHow to perform basic text pre-processing to identify input features."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#part-iii-25-mins",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#part-iii-25-mins",
    "title": "Tutorial 1",
    "section": "PART III ($$25 mins)",
    "text": "PART III ($$25 mins)\n\nHow to train a classifier on identified input features for sentiment classification.\nHow to evaluate model performance and inspect its predictions."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#task",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#task",
    "title": "Tutorial 1",
    "section": "Task",
    "text": "Task\nWe consider the task of document classification.\nSpecifically, we consider document sentiment analysis on movie reviews."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#preliminaries",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#preliminaries",
    "title": "Tutorial 1",
    "section": "Preliminaries",
    "text": "Preliminaries\nFirst of all, we need to import some useful packages that we will use during this hands-on session.\n\n# file management\nimport sys\nimport shutil\nimport urllib\nimport tarfile\nfrom pathlib import Path\n\n# dataframe management\nimport pandas as pd\n\n# data manipulation\nimport numpy as np\n\n# for readability\nfrom typing import Iterable\n\n# viz\nfrom tqdm import tqdm\n\n\nfrom notebook.services.config import ConfigManager\ncm = ConfigManager()\ncm.update('livereveal', {\n        'width': 1024,\n        'height': 768,\n        'scroll': True,\n})\n\n{'width': 1024, 'height': 768, 'scroll': True}"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#encode-the-dataset-into-a-pandas.dataframe",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#encode-the-dataset-into-a-pandas.dataframe",
    "title": "Tutorial 1",
    "section": "1.1 Encode the dataset into a pandas.DataFrame",
    "text": "1.1 Encode the dataset into a pandas.DataFrame\nWe encode the loaded dataset into a dataframe to better access to its content.\n\ndataframe_rows = []\n\nfor split in ['train', 'test']:\n    for sentiment in ['pos', 'neg']:\n        folder = dataset_folder.joinpath(dataset_name, split, sentiment)\n        for file_path in folder.glob('*.txt'):            \n            with file_path.open(mode='r', encoding='utf-8') as text_file:\n                text = text_file.read()\n                score = file_path.stem.split(\"_\")[1]\n                score = int(score)\n                file_id = file_path.stem.split(\"_\")[0]\n\n                num_sentiment = 1 if sentiment == 'pos' else 0\n\n                dataframe_row = {\n                    \"file_id\": file_id,\n                    \"score\": score,\n                    \"sentiment\": num_sentiment,\n                    \"split\": split,\n                    \"text\": text\n                }\n\n                dataframe_rows.append(dataframe_row)\n\n\nfolder = Path.cwd().joinpath(\"Datasets\", \"Dataframes\", dataset_name)\nif not folder.exists():\n    folder.mkdir(parents=True)\n\n# transform the list of rows in a proper dataframe\ndf = pd.DataFrame(dataframe_rows)\ndf = df[[\"file_id\", \n         \"score\",\n         \"sentiment\",\n         \"split\",\n         \"text\"]\n       ]\ndf_path = folder.with_name(dataset_name + \".pkl\")\ndf.to_pickle(df_path)"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#inspecting-the-dataset",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#inspecting-the-dataset",
    "title": "Tutorial 1",
    "section": "1.2 Inspecting the dataset",
    "text": "1.2 Inspecting the dataset\nWe first have a look at the dataframe structure.\n\nprint(\"Dataframe structure:\")\nprint(df)\nprint()\n\nprint(\"Total rows %d\" % (len(df)))\nprint()\n\nDataframe structure:\n      file_id  score  sentiment  split  \\\n0        3859      8          1  train   \n1        1733      7          1  train   \n2       12084      8          1  train   \n3       11231     10          1  train   \n4        5094      8          1  train   \n...       ...    ...        ...    ...   \n49995     620      2          0   test   \n49996    3300      1          0   test   \n49997    5127      2          0   test   \n49998    5070      2          0   test   \n49999   11027      4          0   test   \n\n                                                    text  \n0      Paul Verhoeven's De Vierde Man (The Fourth Man...  \n1      Much underrated camp movie on the level of Cob...  \n2      A toothsome little potboiler whose 65-minute l...  \n3      I really like Star Trek Hidden Frontier it is ...  \n4      If you a purist, don't waste your time - other...  \n...                                                  ...  \n49995  I couldn't. I was bored, not just because the ...  \n49996  There are shows and films I've seen and subseq...  \n49997  Flashdance is one of those awful, stupid movie...  \n49998  Another example of the women-in-prison genre. ...  \n49999  Carole Lombard and James Stewart gamely try to...  \n\n[50000 rows x 5 columns]\n\nTotal rows 50000\n\n\n\n\nprint(\"Distribution of scores: \")\nprint(df['score'].value_counts())\nprint()\n\nprint(\"Distribution of sentiment: \")\nprint(df['sentiment'].value_counts())\nprint()\n\nprint(\"Distribution of splits: \")\nprint(df['split'].value_counts())\nprint()\n\nDistribution of scores: \nscore\n1     10122\n10     9731\n8      5859\n4      5331\n3      4961\n7      4803\n9      4607\n2      4586\nName: count, dtype: int64\n\nDistribution of sentiment: \nsentiment\n1    25000\n0    25000\nName: count, dtype: int64\n\nDistribution of splits: \nsplit\ntrain    25000\ntest     25000\nName: count, dtype: int64\n\n\n\n\nprint(\"Distribution of sentiments in split: \")\nprint(df.groupby(['split','sentiment']).size())\nprint()\n\nprint(\"Distribution of scores in split: \")\nprint(df.groupby(['split','score']).size())\nprint()\n\nprint(\"Differences in score distribution in split: \")\nprint(df.groupby(['score','split',]).size())\nprint()\n\nDistribution of sentiments in split: \nsplit  sentiment\ntest   0            12500\n       1            12500\ntrain  0            12500\n       1            12500\ndtype: int64\n\nDistribution of scores in split: \nsplit  score\ntest   1        5022\n       2        2302\n       3        2541\n       4        2635\n       7        2307\n       8        2850\n       9        2344\n       10       4999\ntrain  1        5100\n       2        2284\n       3        2420\n       4        2696\n       7        2496\n       8        3009\n       9        2263\n       10       4732\ndtype: int64\n\nDifferences in score distribution in split: \nscore  split\n1      test     5022\n       train    5100\n2      test     2302\n       train    2284\n3      test     2541\n       train    2420\n4      test     2635\n       train    2696\n7      test     2307\n       train    2496\n8      test     2850\n       train    3009\n9      test     2344\n       train    2263\n10     test     4999\n       train    4732\ndtype: int64\n\n\n\n\nprint(\"Differences among score classes in the splits: \")\nprint(df.groupby(['sentiment','score','split']).size())\nprint()\n\nDifferences among score classes in the splits: \nsentiment  score  split\n0          1      test     5022\n                  train    5100\n           2      test     2302\n                  train    2284\n           3      test     2541\n                  train    2420\n           4      test     2635\n                  train    2696\n1          7      test     2307\n                  train    2496\n           8      test     2850\n                  train    3009\n           9      test     2344\n                  train    2263\n           10     test     4999\n                  train    4732\ndtype: int64\n\n\n\n\n1.2.1 Visualization\n\n# Preliminaries\nimport matplotlib.pyplot as plt\n\nWe first visualize sentiment class distribution.\n\ntrain_labels = df[df.split == 'train'].sentiment.values\ntest_labels = df[df.split == 'test'].sentiment.values\n\nplt.hist([train_labels,test_labels], bins=2, label=['train','test'], color=[\"red\", \"blue\"], align=\"mid\")\nplt.legend(loc='upper center')\nplt.title('Class distribution')\nplt.xlabel('Sentiment')\nplt.ylabel('# Samples')\nplt.show()\n\n\n\n\n\n\n\n\nThen we visualize sentiment scores.\n\ntrain_scores = df[df.split == 'train'].score.values\ntest_scores = df[df.split == 'test'].score.values\n\nplt.xticks(range(1,11))\nplt.hist([train_scores,test_scores], bins=10, label=['train','test'], color=[\"red\", \"blue\"], align=\"mid\")\nplt.legend(loc='upper center')\nplt.title('Score distribution')\nplt.xlabel('Score')\nplt.ylabel('# Samples')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplot1 = plt.figure(1)\nplt.hist(train_scores, 10, color=\"red\")\nplt.xlabel('Score')\nplt.ylabel('# Samples')\nplt.title(\"Train\")\n\nplot2 = plt.figure(2)\nplt.hist(test_scores, 10, color=\"blue\")\nplt.xlabel('Score')\nplt.ylabel('# Samples')\nplt.title(\"Test\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLastly, we check text length distribution.\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# necessary for being able to tokenize\nnltk.download('punkt_tab')\nnltk.download('punkt')\n\ntrain_texts = df[df.split == 'train'].text.values\ntest_texts = df[df.split == 'test'].text.values\n\ntrain_lengths = [len(word_tokenize(text)) for text in tqdm(train_texts)]\ntest_lengths = [len(word_tokenize(text)) for text in tqdm(test_texts)]\n\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /home/emancini/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package punkt to /home/emancini/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:24&lt;00:00, 1040.70it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:23&lt;00:00, 1062.79it/s]\n\n\n\nplot1 = plt.figure(1)\nplt.hist(train_lengths, color=\"red\")\nplt.xlabel('Word count')\nplt.ylabel('# Samples')\nplt.title(\"Train\")\n\nplot2 = plt.figure(2)\nplt.hist(test_lengths, 10, color=\"blue\")\nplt.xlabel('Word count')\nplt.ylabel('# Samples')\nplt.title(\"Test\")\n\nplt.show()"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#feature-extraction",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#feature-extraction",
    "title": "Tutorial 1",
    "section": "2. Feature Extraction",
    "text": "2. Feature Extraction\nWe are now ready to address the sentiment analysis task.\nWe consider the sentiment classification task for description purposes.\nWe first determine the inputs and outputs of our classifier.\nThen, we define how to encode the inputs and outputs for the classifier.\n\nInputs and Outputs\n\n\n\n\n\nInput features and Outputs\nWe will encode input text into a numerical format: bag-of-words.\n\n\n\nSentiment score prediction has the same schema where the output label is replaced by the sentiment score.\n\n\n2.1 Bag-of-Words Encoding\nThe scikit-learn library offers TfidfVectorizer, a class that performs both tokenization and the creation of the Bag-of-Words (BoW) representation of a corpus.\nIn particular, it computes tf-idf representation for each word token.\n\nHomework 📖\nThe class has plenty of options: it can be used also to count n-grams, excluding stop-words, and cutting off most and/or less frequent terms.\n\nfrom sklearn.feature_extraction.text import  TfidfVectorizer\n\n# select only the training sentences\ndf_train = df.loc[df['split'] == \"train\"]\n\ntrain_texts = df_train.text.values\n\nprint(\"Processing corpus\\n\")\nvectorizer =  TfidfVectorizer()\n\n# tokenization and creation of Bag of Words representation\nX_train = vectorizer.fit_transform(train_texts)\n\nprint(\"Shape of the matrix: (data points, features)\")\nprint(X_train.shape)\nprint()\n\n# targets for the training set\nY_train = df_train.sentiment.values \n\nProcessing corpus\n\nShape of the matrix: (data points, features)\n(25000, 74849)\n\n\n\nThe vectorizer fits on the provided texts to create an internal vocabulary.\nThe vocabulary is used to create the BoW representation.\nThe vocabulary is a dictionary that associates to each word a corresponding column in the feature matrix.\n\n\nHomework 📖\nDo not try to print the whole vocabulary: it is quite large ;)\nStill, it is very important to have a look at detected tokens!\nLet’s check the size of the built vocabulary.\n\nprint(\"Size of vocabulary:\")\nprint(len(vectorizer.vocabulary_))\nprint(type(vectorizer.vocabulary_))\n\nSize of vocabulary:\n74849\n&lt;class 'dict'&gt;\n\n\nSince the vocabulary is quite big, the BoW representation of input texts is a sparse matrix (see scipy.sparse for more info).\nSimply put, a sparse matrix is represented as a dictionary of non-zero entries, where the keys are the matrix coordinates.\n\nprint(X_train)\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 3445861 stored elements and shape (25000, 74849)&gt;\n  Coords    Values\n  (0, 48780)    0.07437683455808204\n  (0, 71089)    0.3411572759466054\n  (0, 16742)    0.15835630521672903\n  (0, 71321)    0.1462697637388363\n  (0, 40591)    0.13550935695548164\n  (0, 66339)    0.16064667311940728\n  (0, 25738)    0.19243155200670298\n  (0, 34585)    0.07065297031375764\n  (0, 46932)    0.04996565970832315\n  (0, 46680)    0.08382411219491039\n  (0, 43995)    0.03736041669968358\n  (0, 13719)    0.16761326444057742\n  (0, 66675)    0.09923901598559316\n  (0, 30118)    0.024960352134857557\n  (0, 22737)    0.042215166368718894\n  (0, 58613)    0.040550479122127045\n  (0, 34683)    0.05329332149376032\n  (0, 53839)    0.06969033593611378\n  (0, 72196)    0.022878734560446083\n  (0, 50357)    0.09119584023968826\n  (0, 64640)    0.07432696846166174\n  (0, 63422)    0.10457938564386521\n  (0, 13940)    0.09888625903361496\n  (0, 7197) 0.1295437826477587\n  (0, 59123)    0.1522889958449226\n  : :\n  (24999, 21636)    0.06631614886778509\n  (24999, 48801)    0.061364793658764\n  (24999, 45172)    0.058473979963732554\n  (24999, 35719)    0.05831348170850543\n  (24999, 18885)    0.10620592435331727\n  (24999, 58851)    0.058473979963732554\n  (24999, 374)  0.11244760665077666\n  (24999, 8334) 0.06028253547081038\n  (24999, 9882) 0.11902011255451533\n  (24999, 54984)    0.05529651920953299\n  (24999, 23192)    0.07332686403280356\n  (24999, 18907)    0.05713250248232416\n  (24999, 18434)    0.07151830852572572\n  (24999, 43672)    0.06209109097788821\n  (24999, 20497)    0.07440912222075717\n  (24999, 29168)    0.06590042199826807\n  (24999, 10268)    0.05610192287774723\n  (24999, 25439)    0.06159973466037476\n  (24999, 38006)    0.07894475056026122\n  (24999, 54306)    0.07440912222075717\n  (24999, 62587)    0.08127638413584957\n  (24999, 71672)    0.07440912222075717\n  (24999, 42758)    0.15427239010636679\n  (24999, 10416)    0.07713619505318339\n  (24999, 18432)    0.0845626370877189\n\n\nIt is possible to get the dense representation of a feature matrix.\nKeep in mind that the matrix will occupy a lot of memory.\n\nprint(X_train.toarray())\n\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n\n\n\n\nHomework 📖\nTry taking a subset of the matrix and inspect its dense representation.\nWe use our fitted Vectorizer to transform test texts as well.\nMake sure you invoke vectorizer.transform() and not vectorizer.fit_transform().\n\n# select only the test sentences\ndf_test = df.loc[df['split'] == \"test\"]\ntest_texts = df_test.text.values\n\nX_test = vectorizer.transform(test_texts)\nY_test = df_test.sentiment.values\n\nGOLDEN RULE: DO NOT EVER EVER EVER EVER MESS WITH THE TEST SET!!!!\nYou should never add or remove rows/instances/data points from the test set.\nYou should never alter the ground-truth labels.\nYou can process it to transform, add, or remove columns/features.\nYou can modify the train set more freely.\nBut keep your gross fingerprints from the test set: it is untouchable, whatever its quality.\n\n\n\n\nSo far, we have encoded the text as it is.\nWe can perform additional pre-processing to ‘normalize’ the provided text.\n\n\n\n2.2 Stemming\n‘’Stemming usually refers to a crude heuristic process that chops off the ends of words to reduce words to a common form.’’\n\n(Introduction to Information Retrieval ~ C. D. Manning)\n\nExample\nprogrammer, programs, programming \\(\\rightarrow\\) program\n\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import word_tokenize, sent_tokenize, WhitespaceTokenizer\n\nstemmer = SnowballStemmer(\"english\")\ntokenizer = WhitespaceTokenizer()\n\n\ndef stem_text(text: str):\n    words = [stemmer.stem(word) for word in tokenizer.tokenize(text)]\n    return \" \".join(words)\n\nst_train_texts = [stem_text(text) for text in tqdm(train_texts)]\nst_test_texts = [stem_text(text) for text in tqdm(test_texts)]\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:36&lt;00:00, 684.93it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:35&lt;00:00, 702.22it/s]\n\n\n\nprint(train_texts[0][:100])\nprint()\nprint(st_train_texts[0][:100])\n\nPaul Verhoeven's De Vierde Man (The Fourth Man) is one of the most compelling thrillers I have ever \n\npaul verhoeven de vierd man (the fourth man) is one of the most compel thriller i have ever seen. it\n\n\nWe encoded the pre-processed texts as before via the TfidfVectorizer.\n\nprint(\"Processing corpus\\n\")\nst_vectorizer =  TfidfVectorizer()\nst_X_train = st_vectorizer.fit_transform(st_train_texts)\nst_X_test = st_vectorizer.transform(st_test_texts)\n\nprint(\"Shape of the matrix: (data points, features)\")\nprint(st_X_train.shape)\nprint()\n\nProcessing corpus\n\nShape of the matrix: (data points, features)\n(25000, 70363)\n\n\n\n\n\n2.3 Lemmatization\n‘’Lemmatization usually refers to use a vocabulary and morphological analysis of words to return the base or dictionary form of a word, known as lemma’’\n\n(Introduction to Information Retrieval ~ C. D. Manning)\n\nExample\nsaw, seen, sees \\(\\rightarrow\\) see\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\n\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('averaged_perceptron_tagger_eng')\n\nlemmatizer = WordNetLemmatizer()\n\n[nltk_data] Downloading package omw-1.4 to /home/emancini/nltk_data...\n[nltk_data] Downloading package wordnet to /home/emancini/nltk_data...\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/emancini/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n\n\n\ndef get_wordnet_key(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:          \n        return 'n'\n\ndef lem_text(text: str):\n    tokens = tokenizer.tokenize(text)\n    tagged = pos_tag(tokens)\n    words = [lemmatizer.lemmatize(word, get_wordnet_key(tag)) for word, tag in tagged]\n    return \" \".join(words)\n\nlem_train_texts = [lem_text(text) for text in tqdm(train_texts, leave=True, position=0)]\nlem_test_texts = [lem_text(text) for text in tqdm(test_texts, leave=True, position=0)]\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /home/emancini/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n  0%|                                                                                                                                                           | 0/25000 [00:31&lt;?, ?it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [02:23&lt;00:00, 174.25it/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [02:18&lt;00:00, 180.01it/s]\n\n\n\nprint(train_texts[40][:100])\nprint()\nprint(lem_train_texts[40][:100])\n\nPolice Story is a stunning series of set pieces for Jackie Chan to show his unique talents and brave\n\nPolice Story be a stunning series of set piece for Jackie Chan to show his unique talent and bravery\n\n\nWe encoded the pre-processed texts as before via the TfidfVectorizer.\n\nprint(\"Processing corpus\\n\")\nlem_vectorizer =  TfidfVectorizer()\nlem_X_train = lem_vectorizer.fit_transform(lem_train_texts)\nlem_X_test = lem_vectorizer.transform(lem_test_texts)\n\nprint(\"Shape of the matrix: (data points, features)\")\nprint(lem_X_train.shape)\nprint()\n\nProcessing corpus\n\nShape of the matrix: (data points, features)\n(25000, 70649)"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#training-and-evaluating-a-classifier",
    "href": "nlp-course-material/2023-2024/Tutorial 1/tutorial1-2324.html#training-and-evaluating-a-classifier",
    "title": "Tutorial 1",
    "section": "3. Training and Evaluating a Classifier",
    "text": "3. Training and Evaluating a Classifier\nWe are now ready to try out some standard machine learning classifiers.\nBut first, we introduce some baselines.\n\nThe importance of defining good baselines\nGood baselines are always needed to consistently evaluate employed models.\nWhen proposing a novel model, good baselines are recent state-of-the-art models.\nWhen proposing a benchmark, good baselines are general-purpose off-the-shelf models.\n\n\n3.1 Baselines\nThe majority baseline always predicts the majority class.\nThe random baseline makes uniform random predictions.\nTheir performances will tell us how much difficult this task is.\n\nfrom sklearn.dummy import DummyClassifier\n\nmajority_classifier = DummyClassifier(strategy=\"prior\")\nuniform_classifier = DummyClassifier(strategy=\"uniform\")\n\nmajority_classifier.fit(X_train, Y_train)\nuniform_classifier.fit(X_train, Y_train)\n\nY_pred_train_maj = majority_classifier.predict(X_train)\nY_pred_test_maj = majority_classifier.predict(X_test)\nY_pred_train_uni = uniform_classifier.predict(X_train)\nY_pred_test_uni = uniform_classifier.predict(X_test)\n\n\nHomework 📖\nTry out a random baseline that samples predictions according to label distribution.\nWe now evaluate model predictions via some metrics.\n\nfrom sklearn.metrics import classification_report\n\ndef evaluate_classification(y_true, y_pred):\n    report = classification_report(y_true, y_pred, target_names=['neg', 'pos'])\n    print(report)\n\n\nevaluate_classification(y_true=Y_train, y_pred=Y_pred_train_maj)\nevaluate_classification(y_true=Y_test, y_pred=Y_pred_test_maj)\n\n              precision    recall  f1-score   support\n\n         neg       0.50      1.00      0.67     12500\n         pos       0.00      0.00      0.00     12500\n\n    accuracy                           0.50     25000\n   macro avg       0.25      0.50      0.33     25000\nweighted avg       0.25      0.50      0.33     25000\n\n              precision    recall  f1-score   support\n\n         neg       0.50      1.00      0.67     12500\n         pos       0.00      0.00      0.00     12500\n\n    accuracy                           0.50     25000\n   macro avg       0.25      0.50      0.33     25000\nweighted avg       0.25      0.50      0.33     25000\n\n\n\n/home/emancini/miniforge3/envs/nlp-teaching/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/emancini/miniforge3/envs/nlp-teaching/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/emancini/miniforge3/envs/nlp-teaching/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/emancini/miniforge3/envs/nlp-teaching/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/emancini/miniforge3/envs/nlp-teaching/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/home/emancini/miniforge3/envs/nlp-teaching/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n\n\n\nevaluate_classification(y_true=Y_train, y_pred=Y_pred_train_uni)\nevaluate_classification(y_true=Y_test, y_pred=Y_pred_test_uni) \n\n              precision    recall  f1-score   support\n\n         neg       0.50      0.51      0.51     12500\n         pos       0.50      0.50      0.50     12500\n\n    accuracy                           0.50     25000\n   macro avg       0.50      0.50      0.50     25000\nweighted avg       0.50      0.50      0.50     25000\n\n              precision    recall  f1-score   support\n\n         neg       0.50      0.49      0.50     12500\n         pos       0.50      0.50      0.50     12500\n\n    accuracy                           0.50     25000\n   macro avg       0.50      0.50      0.50     25000\nweighted avg       0.50      0.50      0.50     25000\n\n\n\n\n\n3.1.1 The importance of model evaluation\nIn most cases, accuracy is not a very informative metric.\nPrecision, recall, and F1 are more informative\nF1 is generally a good indicator. In particular, macro F1 in case the test set in unbalanced.\nWhether precision or recall are more important depends on the specific application, if the priority is to avoid false positives or false negatives.\nOther metrics do exists (e.g., Area Under ROC Curve), always check the literature to find the most informative way to measure something!\n\n\nHomework 📖\nTry sampling an unbalanced test set and run model evaluation.\nDo you see any differences in terms of metric values?\n\n# plot precision/recall curve and confusion matrix\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, ConfusionMatrixDisplay, PrecisionRecallDisplay\n\nPrecisionRecallDisplay.from_estimator(majority_classifier, X_test, Y_test)\nConfusionMatrixDisplay.from_estimator(majority_classifier, X_test, Y_test, normalize=None, cmap=plt.cm.Blues)\nConfusionMatrixDisplay.from_estimator(majority_classifier, X_test, Y_test, normalize='all', cmap=plt.cm.Blues, values_format=\".2f\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn most cases it is better to use the normalized version, especially with unbalanced distributions!\nLooking at confusion tables row-wise gives information about recall, looking column-wise gives information about precision.\n\n\n\n\n(wikipedia)\n\n\nPrecisionRecallDisplay.from_estimator(uniform_classifier, X_test, Y_test)\nConfusionMatrixDisplay.from_estimator(uniform_classifier, X_test, Y_test, normalize=None, cmap=plt.cm.Blues)\nConfusionMatrixDisplay.from_estimator(uniform_classifier, X_test, Y_test, normalize='all', cmap=plt.cm.Blues, values_format=\".2f\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 Bayes Classifier\nLet’s try out our first classifier: A Naive Bayes Classifier (NB).\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nnb_classifier = MultinomialNB()\n\nnb_classifier.fit(X_train, Y_train)\n\nY_pred_train_nb = nb_classifier.predict(X_train)\nY_pred_test_nb = nb_classifier.predict(X_test)\n\n\nevaluate_classification(y_true=Y_train, y_pred=Y_pred_train_nb)\nevaluate_classification(y_true=Y_test, y_pred=Y_pred_test_nb)\n\n              precision    recall  f1-score   support\n\n         neg       0.89      0.93      0.91     12500\n         pos       0.93      0.89      0.91     12500\n\n    accuracy                           0.91     25000\n   macro avg       0.91      0.91      0.91     25000\nweighted avg       0.91      0.91      0.91     25000\n\n              precision    recall  f1-score   support\n\n         neg       0.79      0.89      0.84     12500\n         pos       0.87      0.77      0.82     12500\n\n    accuracy                           0.83     25000\n   macro avg       0.83      0.83      0.83     25000\nweighted avg       0.83      0.83      0.83     25000\n\n\n\n\nPrecisionRecallDisplay.from_estimator(nb_classifier, X_test, Y_test)\nConfusionMatrixDisplay.from_estimator(nb_classifier, X_test, Y_test, cmap=plt.cm.Blues)\nConfusionMatrixDisplay.from_estimator(nb_classifier, X_test, Y_test, normalize='all', cmap=plt.cm.Blues, values_format=\".2f\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3 SVC Classifier\nWe consider another classifier: a Support Vector Machine (SVM).\n\nfrom sklearn.svm import LinearSVC\n\nsvc_classifier = LinearSVC()\n\nsvc_classifier.fit(X_train, Y_train)\n\nY_pred_train_svc = svc_classifier.predict(X_train)\nY_pred_test_svc = svc_classifier.predict(X_test)\n\n\nevaluate_classification(y_true=Y_train, y_pred=Y_pred_train_svc)\nevaluate_classification(y_true=Y_test, y_pred=Y_pred_test_svc)\n\n              precision    recall  f1-score   support\n\n         neg       0.99      0.99      0.99     12500\n         pos       0.99      0.99      0.99     12500\n\n    accuracy                           0.99     25000\n   macro avg       0.99      0.99      0.99     25000\nweighted avg       0.99      0.99      0.99     25000\n\n              precision    recall  f1-score   support\n\n         neg       0.87      0.89      0.88     12500\n         pos       0.89      0.87      0.88     12500\n\n    accuracy                           0.88     25000\n   macro avg       0.88      0.88      0.88     25000\nweighted avg       0.88      0.88      0.88     25000\n\n\n\n\nPrecisionRecallDisplay.from_estimator(svc_classifier, X_test, Y_test)\nConfusionMatrixDisplay.from_estimator(svc_classifier, X_test, Y_test, cmap=plt.cm.Blues)\nConfusionMatrixDisplay.from_estimator(svc_classifier, X_test, Y_test, normalize='all', cmap=plt.cm.Blues, values_format=\".2f\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4 Error Analysis\nLet’s try to understand what kind of errors are done by the model.\nWe can look at specific cases of error.\n\n# confidence of the model\nconfidence = svc_classifier.decision_function(X_test)\n\nfp_i = []\nfn_i = []\nfp_v = []\nfn_v = []\nfp_c = []\nfn_c = []\n\nfor index, value in enumerate(Y_test):\n    # false positives\n    if value==0 and Y_pred_test_svc[index]==1:\n        fp_i.append(index)\n        fp_v.append(value)\n        fp_c.append(confidence[index])\n    # false negatives\n    elif value==1 and Y_pred_test_svc[index]==0:\n        fn_i.append(index)\n        fn_v.append(value)\n        fn_c.append(confidence[index])\n\nLet’s print some examples of FN and FP and the confidence in their classification\n\nprint(\"FN\")\n\nfor index in [0, 3]:\n    print(test_texts[fn_i[index]])\n    print(fn_c[index])\n    print()\n\nFN\nI'd honestly give this movie a solid 7.5, but I clicked 10 to try to offset the 5 pages of imbecilic, unjustified 1-star reviews. This is an interesting story, all of the acting is good to very good (even Ms. Diaz, who is totally out of her usual grinning-bimbo role here, yet plays it well.) The sets are perfect and the cinematography is consistently appropriately creepy. It's a fine morality play and there is *no* reason to explain the origin of the god/supernatural being/alien/whatever that's \"running the show,\" so I'm glad the movie doesn't try. It's really irrelevant to the story, which is relatively long but quite compelling and summed up quite satisfyingly in the ending. &lt;br /&gt;&lt;br /&gt;Before you decide this movie is terrible (or really, anything under a ~7.5) read some of the dozens of 1-star \"hated it\" reviews that are rife with misspellings, lack of punctuation and capitalization, and juvenile criticisms. Maybe the trailer was misleading or something -- I didn't see it -- but some of these reviewers were apparently expecting Terminator 4 or Saw 5 (one reviewer actually compared this movie to Saw! How utterly inappropriate and unrelated!) &lt;br /&gt;&lt;br /&gt;Seriously, most of these reviews read like you-tube comments -- according to these \"critics\" this movie is too confusing yet too predictable, not enough action yet there's too much going on, too smart yet too dumb, explains too much yet leaves too much unexplained... oh -- and it's apparently a \"waist (sic) of time.\" Do consider the quality and source of the reviews before taking them to heart. I'm afraid these 1-star kids failed to understand the phrase \"altruism coefficient\" and were therefore utterly incapable of understanding the movie's premise (despite adamantly claiming that they \"get it\" right before explaining how confusing it was!) If you know what those two words mean you will have no trouble understanding (and enjoying) this movie.&lt;br /&gt;&lt;br /&gt;I really wish there were a reviewer reputation system here so I could be sure to ignore the rating of everyone who gave this movie 1 star forever.&lt;br /&gt;&lt;br /&gt;See it for yourself and enjoy the fine presentation of an interesting couple taking an interesting moral \"test\" and facing the consequences. It's a good time, in my opinion.\n-0.3232998334312244\n\nNo one would ever question that director Leos Carax is a genius, but what we wonder about is: is he an insane genius? So many people hated this film! I am normally the first person to accuse many French directors of making offensive, boring, disgusting and pretentious films (such as the horrible recent film 'L'Enfant' and the pointless and offensive 'Feux Rouges'). But strangely enough, I actually think that 'Pola X' is an amazing film, made with great skill and passion by a master of his craft, and containing remarkable performances. The film does carry melodrama to more extreme lengths than I believe I have ever seen on screen before. But then, Carax is extreme, that we know. The film also contains what I consider way over-the-top Trotskyite or Anarchist fantasies and wet-dreams, what with a mysterious group of young men training to fire machine guns at the bourgeoisie in between playing Scott Walker's rather fascinating music in a band which has its recording sessions in an abandoned warehouse filled with squatters and fires burning in old steel barrels. Guillaume Depardieu plays a rich young man in a château (whose step-mother is Catherine Deneuve, and he wanders into her bathroom while she is naked in the bath, by the way). But he suddenly 'snaps' completely when he discovers that his deceased father, a famous diplomat, had fathered an illegitimate daughter who had been effectively disposed of by Deneuve as an inconvenience. This is because the sister suddenly turns up as a kind of Romanian refugee with wild dishevelled hair, expressionless face, and little ability to speak French coherently. Depardieu then transforms himself into a 'class hero' of the far left and wants to kill or destroy his family for their hypocrisy and corruption, and lives in squalor and extreme poverty, while scorning a vast inheritance. He then commences an incestuous sexual relationship with his half-sister, which is shown in an explicit sex scene which has offended many people, though I have no objection to it, as I think people are far too hysterical about sex, especially in America, where apparently it never happens. The intensity of the acting and the filming make this unlikely scenario come off as an experience of powerful, if depressing, hyper-melodrama. The differences between Carax making an extreme film like this and the numerous extreme French films which I think are pretentious and disgusting are (1) that Carax is an excellent filmmaker, and (2) he is seriously attempting to explore a meaningful, if harrowing, extreme emotional condition, whereby a human being disintegrates and turns against his background. Many would say that the extreme elements in this film were gratuitous, but I don't agree. I believe Carax was genuine, and was not making an exploitation picture at all. It is very difficult to defend a man who goes that far and who, for all I know, may be a complete madman, but I believe he deserves defending for this remarkable cinematic achievement.\n-0.25690549395936313\n\n\n\n\nprint(\"FP\")\n\nfor index in [0, 6]:\n    print(test_texts[fp_i[index]])\n    print(fp_c[index])\n    print()\n\nFP\nThis movie is filled with so many idiotic moments, that you wonder how it ever got made. For example, they get into the sewers from the Capitol and while they're in the sewers you can see signs pointing to various government buildings, and then they come up in the middle of the street! I highly doubt that government buildings would provide public access through the city sewer system. Anyways, I gave this a 2 instead of a 1 just because of its comic value. I laughed the whole way through at the idiocy of everyone involved in this movie.\n0.11381443000722369\n\nSeymour Cassel gives a great performance, a tour de force. His acting as supposed washed up beach stud Duke Slusarski will always have a place in my heart. The film is centered around a nerd who just came to the beach in hopes of honoring his dead brother's dreams. What he gets is lame surf hijinks. Guys cheating, guys fighting, and guys getting drunk going to watch surf documentaries with the whole town of LA on a Friday night. Duke takes the nerd in and tries to teach him how playing volleyball is like touching a woman. Next time my woman talks back I will pretend I'm spiking the ball. &lt;br /&gt;&lt;br /&gt;Back to Seymour Cassel. The end of the movie turns into a good drama, since the first half of the film really had no point. Duke plays a wonderful game of volleyball, the best he's played in over ten years. The way the scene is shot is beautiful. You can feel the heart this man has for the game and the love of being on the beach. Those five minutes will go down as one of my favorites of all time. 3/10 Bad to Fair, the rest of the movie was lame.\n0.06688671652185901\n\n\n\nWe now check the sentences for which the classification error is bigger.\n\nmax_fp = fp_c.index(max(fp_c))\nprint(test_texts[fp_i[max_fp]])\nprint(fp_c[max_fp])\n\nprint()\n\n# for false negatives the confidence is negative\nmax_fn = fn_c.index(min(fn_c))\nprint(test_texts[fp_i[max_fn]])\nprint(fn_c[max_fn])\n\nThis movie was pure genius. John Waters is brilliant. It is hilarious and I am not sick of it even after seeing it about 20 times since I bought it a few months ago. The acting is great, although Ricki Lake could have been better. And Johnny Depp is magnificent. He is such a beautiful man and a very talented actor. And seeing most of Johnny's movies, this is probably my favorite. I give it 9.5/10. Rent it today!\n2.070315526195805\n\nThe breadth & height of the scale of this movie overwhelm me. About a week ago I posted a commentary on the 1926 silent epic MACISTE IN HELL referring to it as \"staggering\". Then I encountered the Pollonia Bros. BLOOD RED PLANET. Wow ...&lt;br /&gt;&lt;br /&gt;It's all about the scale of the thinking behind it. The Pollonia Bros. and colleague John McBride were thinking so big that the musings of Arthur C. Clark & Stanley Kubrick's 2001: A SPACE ODESSEY is lucky it was released thirty-two years before this sweetheart. As a matter of fact, Kubrick had it easy with modelers, a budget, actual actors and major studio backing for his little \"Star Trek\" ripoff, and all that Arthur C. Clarke had to do was write the damn thing. What's so hard about muscling around a typewriter? The Pollonia brothers on the other hand had to think radical, outside of the box, and quickly. They recruited a half dozen of their friends & colleagues -- including a hot lookin' chick -- bought a bunch of colored lightbulbs, raided old office supply dumps for every keyboard, monitor and hands-free phone headset they could find, got everybody identical black turtlenecks, made off with every cupcake tin in the county, learned enough 3d modeling & texture skinning to animate a couple of space ship fly-bys, spent a couple hours drumming up a script, executed what is easily the most frightening space monster puppet since that horrible little monster Jabba the Hut carried around with him was fried by R2D2, spent literally days working with a VHS camera and an Amiga to shoot & edit their film, and came up winners. This is the best D.I.Y. direct to home video fan movie space epic I've ever seen, and in proportion to the amount of talent & resources at the disposal of the filmmakers dwarfs even LOGANS RUN or BATTLESTAR: GALACTICA (the original series) as being a chilling look at our future, right down to a newscaster wearing a mis-matched, ill-fitting 2nd hand suit. As a species, we are doomed.&lt;br /&gt;&lt;br /&gt;Seriously for a minute, though, it is a vast improvement upon such later epics as THE DINOSAUR CHRONICALS or PREYALIEN: ALIEN PREDATORS, which is funny since it was made before either of those masterpieces. Somewhere along the lines the Pollonia's started taking it easy on themselves: This film is executed with a certain conviction that gets you to believe that you really can breathe in a vacuum wearing a dust mask and two biker squirt bottles. And the scene where the alien beast consumes a miniaturized crew member made me immediately think of where I'd seen that before: GODZILLA VS. THE SEA MONSTER, where the giant rampaging monster crab skewers two canoers and eats them on camera. That made me cry when I was a kid.&lt;br /&gt;&lt;br /&gt;This made me cry as an adult because here I am wasting my time trying to be some sort of critic or writer or writer/critic and these guys get to have all the fun, actually making a real feature length sci-fi epic with lots of cool colored lights, a hot chick and a talking slide projector named KAL. One of the flight crew even gets to wear his black space baseball cap backwards & glower with his goatee like that clown from Metallica. Is this supposed to be a joke? No. It is an epic statement about humanity, from humanity and for humanity that deserves to be seen by anybody who has the capability to not take any of it too seriously. Look for it on a 2 disc box set called GALAXY OF TERROR in your favorite discount retailer's cutout DVD bins, which is probably where it belongs, but how many of us can say \"Yeah, they have a couple box sets of our movies for sale at Best Buy.\" &lt;br /&gt;&lt;br /&gt;4/10: Anyone lacking in a sense of humor might want to try SILENT RUNNING or maybe TRON. But someday an alien civilization on the other side of the galaxy will intercept a transmission of this movie and decide that we actually are not to be messed with. Good work, guys!!\n-1.9518755195269524\n\n\nLet’s now consider the stemmed and lemmatized versions of our input features.\n\nsvc_classifier_st = LinearSVC()\n\nsvc_classifier_st.fit(st_X_train, Y_train)\n\nY_pred_train_svc_st = svc_classifier_st.predict(st_X_train)\nY_pred_test_svc_st = svc_classifier_st.predict(st_X_test)\n\n\nevaluate_classification(y_true=Y_train, y_pred=Y_pred_train_svc_st)\nevaluate_classification(y_true=Y_test, y_pred=Y_pred_test_svc_st)\n\n              precision    recall  f1-score   support\n\n         neg       0.99      0.99      0.99     12500\n         pos       0.99      0.99      0.99     12500\n\n    accuracy                           0.99     25000\n   macro avg       0.99      0.99      0.99     25000\nweighted avg       0.99      0.99      0.99     25000\n\n              precision    recall  f1-score   support\n\n         neg       0.87      0.88      0.87     12500\n         pos       0.88      0.86      0.87     12500\n\n    accuracy                           0.87     25000\n   macro avg       0.87      0.87      0.87     25000\nweighted avg       0.87      0.87      0.87     25000\n\n\n\n\nsvc_classifier_lem = LinearSVC()\n\nsvc_classifier_lem.fit(lem_X_train, Y_train)\n\nY_pred_train_svc_lem = svc_classifier_lem.predict(lem_X_train)\nY_pred_test_svc_lem = svc_classifier_lem.predict(lem_X_test)\n\n\nevaluate_classification(y_true=Y_train, y_pred=Y_pred_train_svc_lem)\nevaluate_classification(y_true=Y_test, y_pred=Y_pred_test_svc_lem)\n\n              precision    recall  f1-score   support\n\n         neg       0.99      0.99      0.99     12500\n         pos       0.99      0.99      0.99     12500\n\n    accuracy                           0.99     25000\n   macro avg       0.99      0.99      0.99     25000\nweighted avg       0.99      0.99      0.99     25000\n\n              precision    recall  f1-score   support\n\n         neg       0.86      0.88      0.87     12500\n         pos       0.88      0.86      0.87     12500\n\n    accuracy                           0.87     25000\n   macro avg       0.87      0.87      0.87     25000\nweighted avg       0.87      0.87      0.87     25000\n\n\n\n\n\n3.4.1 Comparisons\nLet’s organize our results in a Table to better compare models.\n\n\n\nModel\nF1-macro\n\n\n\n\nRandom\n.50\n\n\nMajority\n.33\n\n\nNB\n.83\n\n\nSVM\n.88\n\n\nSVM w/ Stem\n.87\n\n\nSVM w/ Lem\n.87\n\n\n\n\nHomework\nTry out the NB classifier with Stemmed and Lemmatized input features, respectively.\nWhat do you observe?"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html",
    "href": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html",
    "title": "Tutorial 2",
    "section": "",
    "text": "Credits: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\nKeywords: Sparse/Dense Word embeddings, Embedding visualization"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#part-0-5-mins",
    "href": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#part-0-5-mins",
    "title": "Tutorial 2",
    "section": "PART 0 ($$5 mins)",
    "text": "PART 0 ($$5 mins)\n\nDownloading a dataset.\nEncoding a a dataset."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#part-i-30-mins",
    "href": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#part-i-30-mins",
    "title": "Tutorial 2",
    "section": "PART I ($$30 mins)",
    "text": "PART I ($$30 mins)\n\nBuilding a vocabulary.\nBuilding a word-word co-occurrence matrix.\nDefining a similarity metric: cosine similarity.\nEmbedding visualization and analysis of their semantic properties."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#part-ii-30-mins",
    "href": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#part-ii-30-mins",
    "title": "Tutorial 2",
    "section": "PART II ($$30 mins)",
    "text": "PART II ($$30 mins)\n\nLoading pre-trained dense word embeddings: Word2Vec, GloVe, FastText.\nChecking out-of-vocabulary (OOV) terms.\nHandling OOV terms."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#preliminaries",
    "href": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#preliminaries",
    "title": "Tutorial 2",
    "section": "Preliminaries",
    "text": "Preliminaries\nFirst of all, we need to import some useful packages that we will use during this hands-on session.\n\n# system packages\nfrom pathlib import Path\nimport shutil\nimport urllib\nimport tarfile\nimport sys\n\n# data and numerical management packages\nimport pandas as pd\nimport numpy as np\n\n# useful during debugging (progress bars)\nfrom tqdm import tqdm\n\n# typing\nfrom typing import List, Callable, Dict\n\n\nfrom notebook.services.config import ConfigManager\ncm = ConfigManager()\ncm.update('livereveal', {\n        'width': 2560,\n        'height': 1440,\n        'scroll': True,\n})\n\n{'width': 2560, 'height': 1440, 'scroll': True}"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#building-a-vocabulary",
    "href": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#building-a-vocabulary",
    "title": "Tutorial 2",
    "section": "1. Building a vocabulary",
    "text": "1. Building a vocabulary\nWe consider words as the atomic units for text representation: each word will be associated with a numeric representation.\nAt this point we can build the word vocabulary of our dataset. This information is the first step of any word embedding method: we need to know the set of atomic entities that build up our corpus.\nDefinition: a vocabulary is a collection of words occurring in a given dataset. More precisely, each word is recognized and assigned to an index.\nExample: Suppose you have the given toy corpus \\(D\\): { “the cat is on the table” }\nAs you notice, the dataset is comprised of only one sentence: “the cat is on the table”. The corresponding vocabulary (a possible one) will be:\nV = {0: ‘the’, 1: ‘cat’, 2: ‘is’, 3: ‘on’, 4: ‘table’}\nIn this case, indexing follows word order, but it is not mandatory!\n\nNote 1\nThe most important thing to remember is that the vocabulary should always be the same one.\n\\(\\rightarrow\\) Thus, make sure that the vocabulary creation routine always returns the same result!\n\n\nNote 2\nA vocabulary is exclusively defined by the tokenization step you define!\n\\(\\rightarrow\\) Characters, sub-words, words are examples of granularity levels.\n\n\n1.1 Text pre-processing\nBefore vocabulary creation, we have to do a little bit of text pre-processing so as to avoid spurious data.\n\\(\\rightarrow\\) Data quality is one of the crucial factors that lead to better performance.\n\\(\\rightarrow\\) Models, even state-of-the-art ones, hardly achieve satisfying results if the dataset is very noisy.\nTypes of pre-processing: there are a lot of pre-processing steps that we can consider, either general or quite task- specific.\n\nText to lower: casing usually doesn’t affect our task, but in some scenarios, such as part-of-speech tagging, it is crucial.\nReplace special characters: special characters are usually employed as variants of a single character like the spacing symbol ’ ’. In other cases (dates, etc..) special characters might have a specific meaning and should not be replaced.\nText stripping: it is important to filter out extra spaces to avoid unwanted distinctions between identical words, such as ‘apple’ and ’ apple ’.\n\nThere are a lot of pre-processing techniques, such as number replacing, lemmatization, stemming, spell correction, acronyms merge and so on.\n\\(\\rightarrow\\) If you are interested you can check here and here some good blogs about the topic.\n\nimport re\nfrom functools import reduce\nimport nltk\nfrom nltk.corpus import stopwords\n\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nGOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\ntry:\n    STOPWORDS = set(stopwords.words('english'))\nexcept LookupError:\n    nltk.download('stopwords')\n    STOPWORDS = set(stopwords.words('english'))\n\n\ndef lower(text: str) -&gt; str:\n    \"\"\"\n    Transforms given text to lower case.\n    \"\"\"\n    return text.lower()\n\ndef replace_special_characters(text: str) -&gt; str:\n    \"\"\"\n    Replaces special characters, such as paranthesis, with spacing character\n    \"\"\"\n    return REPLACE_BY_SPACE_RE.sub(' ', text)\n\ndef replace_br(text: str) -&gt; str:\n    \"\"\"\n    Replaces br characters\n    \"\"\"\n    return text.replace('br', '')\n\n\ndef filter_out_uncommon_symbols(text: str) -&gt; str:\n    \"\"\"\n    Removes any special character that is not in the good symbols list (check regular expression)\n    \"\"\"\n    return GOOD_SYMBOLS_RE.sub('', text)\n\ndef remove_stopwords(text: str) -&gt; str:\n    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n\ndef strip_text(text: str) -&gt; str:\n    \"\"\"\n    Removes any left or right spacing (including carriage return) from text.\n    \"\"\"\n    return text.strip()\n\n\nPREPROCESSING_PIPELINE = [\n                          lower,\n                          replace_special_characters,\n                          replace_br,\n                          filter_out_uncommon_symbols,\n                          remove_stopwords,\n                          strip_text\n                          ]\n\ndef text_prepare(text: str,\n                 filter_methods: List[Callable[[str], str]] = None) -&gt; str:\n    \"\"\"\n    Applies a list of pre-processing functions in sequence (reduce).\n    Note that the order is important here!\n    \"\"\"\n    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n    return reduce(lambda txt, f: f(txt), filter_methods, text)\n\n\nprint('Pre-processing text...')\n\nprint()\nprint(f'[Debug] Before:\\n{df.text.values[50]}')\nprint()\n\n# Replace each sentence with its pre-processed version\ndf['text'] = df['text'].apply(lambda txt: text_prepare(txt))\n\nprint(f'[Debug] After:\\n{df.text.values[50]}')\nprint()\n\nprint(\"Pre-processing completed!\")\n\nPre-processing text...\n\n[Debug] Before:\nI always thought this would be a long and boring Talking-Heads flick full of static interior takes, dude, I was wrong. \"Election\" is a highly fascinating and thoroughly captivating thriller-drama, taking a deep and realistic view behind the origins of Triads-Rituals. Characters are constantly on the move, and although as a viewer you kinda always remain an outsider, it's still possible to feel the suspense coming from certain decisions and ambitions of the characters. Furthermore Johnnie To succeeds in creating some truly opulent images due to meticulously composed lighting and atmospheric light-shadow contrasts. Although there's hardly any action, the ending is still shocking in it's ruthless depicting of brutality. Cool movie that deserves more attention, and I came to like the minimalistic acoustic guitar score quite a bit.\n\n[Debug] After:\nalways thought would long boring talkingheads flick full static interior takes dude wrong election highly fascinating thoroughly captivating thrillerdrama taking deep realistic view behind origins triadsrituals characters constantly move although viewer kinda always remain outsider still possible feel suspense coming certain decisions ambitions characters furthermore johnnie succeeds creating truly opulent images due meticulously composed lighting atmospheric lightshadow contrasts although theres hardly action ending still shocking ruthless depicting utality cool movie deserves attention came like minimalistic acoustic guitar score quite bit\n\nPre-processing completed!\n\n\n\n\n1.2 Vocabulary Creation\nSince the text has been pre-processed, space splitting should work correctly.  We proceed on building the vocabulary and perform some sanity checks.\n\\(\\rightarrow\\) Bare in mind that some packages offers tools for automatic vocabulary creation, such as Keras (check keras.preprocessing.text.Tokenizer).\nNote: In this case, the vocabulary will start from index equal to 1, since the 0 slot is reserved to padding token.\n\nfrom collections import OrderedDict\n\ndef build_vocabulary(df: pd.DataFrame) -&gt; (Dict[int, str],\n                                           Dict[str, int],\n                                           List[str]):\n    \"\"\"\n    Given a dataset, builds the corresponding word vocabulary.\n\n    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n    :return:\n      - word vocabulary: vocabulary index to word\n      - inverse word vocabulary: word to vocabulary index\n      - word listing: set of unique terms that build up the vocabulary\n    \"\"\"\n    idx_to_word = OrderedDict()\n    word_to_idx = OrderedDict()\n    \n    curr_idx = 0\n    for sentence in tqdm(df.text.values):\n        tokens = sentence.split()\n        for token in tokens:\n            if token not in word_to_idx:\n                word_to_idx[token] = curr_idx\n                idx_to_word[curr_idx] = token\n                curr_idx += 1\n\n    word_listing = list(idx_to_word.values())\n    return idx_to_word, word_to_idx, word_listing\n\n\nidx_to_word, word_to_idx, word_listing = build_vocabulary(df)\nprint(f'[Debug] Index -&gt; Word vocabulary size: {len(idx_to_word)}')\nprint(f'[Debug] Word -&gt; Index vocabulary size: {len(word_to_idx)}')\nprint(f'[Debug] Some words: {[(idx_to_word[idx], idx) for idx in np.arange(10) + 1]}')\n\n100%|█████████████████████████████████| 50000/50000 [00:00&lt;00:00, 101101.19it/s]\n\n\n[Debug] Index -&gt; Word vocabulary size: 152534\n[Debug] Word -&gt; Index vocabulary size: 152534\n[Debug] Some words: [('film', 1), ('little', 2), ('ambition', 3), ('nothing', 4), ('sticks', 5), ('screen', 6), ('bad', 7), ('version', 8), ('back', 9), ('future', 10)]\n\n\n\n\n\n\ndef evaluate_vocabulary(idx_to_word: Dict[int, str], word_to_idx: Dict[str, int],\n                        word_listing: List[str], df: pd.DataFrame, check_default_size: bool = False):\n    print(\"[Vocabulary Evaluation] Size checking...\")\n    assert len(idx_to_word) == len(word_to_idx)\n    assert len(idx_to_word) == len(word_listing)\n\n    print(\"[Vocabulary Evaluation] Content checking...\")\n    for i in tqdm(range(0, len(idx_to_word))):\n        assert idx_to_word[i] in word_to_idx\n        assert word_to_idx[idx_to_word[i]] == i\n\n    print(\"[Vocabulary Evaluation] Consistency checking...\")\n    _, _, first_word_listing = build_vocabulary(df)\n    _, _, second_word_listing = build_vocabulary(df)\n    assert first_word_listing == second_word_listing\n\n    print(\"[Vocabulary Evaluation] Toy example checking...\")\n    toy_df = pd.DataFrame.from_dict({\n        'text': [\"all that glitters is not gold\", \"all in all i like this assignment\"]\n    })\n    _, _, toy_word_listing = build_vocabulary(toy_df)\n    toy_valid_vocabulary = set(' '.join(toy_df.text.values).split())\n    assert set(toy_word_listing) == toy_valid_vocabulary\n\n\nprint(\"Vocabulary evaluation...\")\nevaluate_vocabulary(idx_to_word, word_to_idx, word_listing, df)\nprint(\"Evaluation completed!\") \n\nVocabulary evaluation...\n[Vocabulary Evaluation] Size checking...\n[Vocabulary Evaluation] Content checking...\n\n\n100%|██████████████████████████████| 152534/152534 [00:00&lt;00:00, 4199094.03it/s]\n\n\n[Vocabulary Evaluation] Consistency checking...\n\n\n100%|█████████████████████████████████| 50000/50000 [00:00&lt;00:00, 103840.95it/s]\n100%|█████████████████████████████████| 50000/50000 [00:00&lt;00:00, 103299.28it/s]\n\n\n[Vocabulary Evaluation] Toy example checking...\n\n\n100%|██████████████████████████████████████████| 2/2 [00:00&lt;00:00, 55188.21it/s]\n\n\nEvaluation completed!\n\n\n\n\n\n\n\nNote\nDefine intermediary tests for your code in order to inspect data and to assess the correctness of your code!\n\\(\\rightarrow\\) You don’t want to re-run huge and time-consuming experiments due to early pipeline errors!\n\n\n1.3 Saving the vocabulary\nGenerally speaking, it is a good idea to save the dictionary in clear format.\n\\(\\rightarrow\\) In this way you can quickly check for errors or useful words.\nIn this case, we will save the vocabulary dictionary in JSON format.\n\n!pip install simplejson\n\nRequirement already satisfied: simplejson in /home/frgg/deasy_env/lib/python3.7/site-packages (3.18.3)\n\n[notice] A new release of pip is available: 23.2.1 -&gt; 23.3.1\n[notice] To update, run: pip install --upgrade pip\n\n\n\nimport simplejson as sj\n\nvocab_path = Path.cwd().joinpath('Datasets', \"aclImdb\", 'vocab.json')\n\nprint(f\"Saving vocabulary to {vocab_path}\")\nwith vocab_path.open(mode='w') as f:\n    sj.dump(word_to_idx, f, indent=4)\nprint(\"Saving completed!\")\n\nSaving vocabulary to /home/frgg/Repositories/nlp-course-material/2023-2024/Tutorial 2/Datasets/aclImdb/vocab.json\nSaving completed!\n\n\n\nHomework 📖\nSpend some time at checking the built vocabulary!\nIn many tasks (e.g., sequential tagging, embedding analysis, etc…), the way text is tokenized is crucial!"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#sparse-embeddings",
    "href": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#sparse-embeddings",
    "title": "Tutorial 2",
    "section": "2. Sparse embeddings",
    "text": "2. Sparse embeddings\nWorking with text inherently requires a numerical conversion step, known as embedding.\nBag-of-Words (BoW) 1. Count the occurrence of each word in a given corpus 2. Build a word co-occurrence matrix: useful to identify the most common terms in each given document.\nThis type of reasoning is directly related to how meaning is assigned to words.\n\\(\\rightarrow\\) In particular, it is the environment enclosing a word that gives a specific meaning to it.\n\\(\\rightarrow\\) Thus, we look for numerical encoding methods that reflect such point of view.\n\n2.1 A quick simplification\nSince the dataset is quite large, the co-occurrence matrix construction may take a while or may require efficient solutions.\nFor the purpose of this tutorial, we can rely on a small slice of the dataset.\n\\(\\rightarrow\\) Nonetheless, feel free to work with the whole dataset! Suggestions on how to handle this scenario are given below when required.\n\n# This type of slicing is not mandatory, but it is sufficient to our purposes\nnp.random.seed(42)\n\nrandom_indexes = np.random.choice(np.arange(df.shape[0]),\n                                  size=1000,\n                                  replace=False)\n\ndf = df.iloc[random_indexes]\nprint(f'New dataset size: {df.shape}')\nidx_to_word, word_to_idx, word_listing = build_vocabulary(df)\n\nNew dataset size: (1000, 5)\n\n\n100%|████████████████████████████████████| 1000/1000 [00:00&lt;00:00, 67200.26it/s]\n\n\n\n\n2.2 Building the Co-occurence Matrix\nFor each word in the vocabulary we count the number of times each other word appears within the same context window. A simple example is given by image below.\n\nLet’s define the simplest version of a co-occurrence matrix based on word counting.\n\n\nSmall dataset case\nYou should have a vocabulary size that we can afford in terms of memory demand.\n\\(\\rightarrow\\) You can easily instantiate the co-occurrence matrix and populate it iteratively.\n\n\nLarge dataset case\nWe have to work with sparse matrices due to the high vocabulary size and to the low amount of non-zero word counts.\n\\(\\rightarrow\\) The Scipy package allows us to easily define sparse matrices that can be converted ot numpy arrays.\nSuggestion: The simplest way to build the co-occurrence matrix is via an incremental approach: 1. Loop through dataset sentences 2. Split into words 3. Count co-occurrences within the given window frame.\nCombining this approach with sparse matrices is not so efficient (yet possible). However, Scipy offers \\(\\texttt{lil_matrix}\\) sparse format that is suitable to this case.\nYou can check out other sparse formats, such as \\(\\texttt{csr_matrix}\\), and the corresponding building methods.\nWorking with \\(\\texttt{lil_matrix}\\) might take \\(\\sim 1h\\) of time to build the whole dataset co-occurrence matrix. It is also possibile to work with \\(\\texttt{csr_matrix}\\) but the approach is more complex (check the last example of the corresponding documentation page).\n\nimport zipfile\nimport gc\nimport requests\nimport time\n\ndef co_occurrence_count(df: pd.DataFrame,\n                        idx_to_word: Dict[int, str],\n                        word_to_idx: Dict[str, int],\n                        window_size: int = 4) -&gt; np.ndarray:\n    \"\"\"\n    Builds word-word co-occurrence matrix based on word counts.\n\n    :param df: pre-processed dataset (pandas.DataFrame)\n    :param idx_to_word: vocabulary map (index -&gt; word) (dict)\n    :param word_to_idx: vocabulary map (word -&gt; index) (dict)\n\n    :return\n      - co_occurrence symmetric matrix of size |V| x |V| (|V| = vocabulary size)\n    \"\"\"\n    vocab_size = len(idx_to_word)\n    co_occurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n    \n    for sentence in tqdm(df.text.values):\n        tokens = sentence.split()\n        for pos, token in enumerate(tokens):\n            start = max(0, pos - window_size)\n            end = min(pos + window_size + 1, len(tokens))\n\n            first_word_index = word_to_idx[token]\n\n            for pos2 in range(start, end):\n                if pos2 != pos:\n                    second_token = tokens[pos2]\n                    second_word_index = word_to_idx[second_token]\n                    co_occurrence_matrix[first_word_index, second_word_index] += 1\n\n    return co_occurrence_matrix\n\n\n# Clean RAM before re-running this code snippet to avoid possible session crash\nif 'co_occurrence_matrix' in globals():\n    del co_occurrence_matrix\n    gc.collect()\n    time.sleep(10.)     # Give colab some time \n\nprint(\"Building co-occurrence count matrix... (it may take a while...)\")\nco_occurrence_matrix = co_occurrence_count(df, idx_to_word, word_to_idx, window_size=4)\nprint(\"Building completed!\")\n\nBuilding co-occurrence count matrix... (it may take a while...)\n\n\n100%|██████████████████████████████████████| 1000/1000 [00:01&lt;00:00, 732.47it/s]\n\n\nBuilding completed!\n\n\n\n\n\n\n\n2.3 Embedding Visualization\nThe next step is to visualize our sparse word embeddings in a lower dimensional space (2D) in order to have an idea of the meaning attributed to each word.\nHow? We will explore SVD, t-SNE methods, and Umap, without delving into technical details since they are not arguments of this NLP course.\nSVD Memo: SVD stands for Singular Value Decomposition and is a kind of generalized Principal Components Analysis (PCA) and focuses on selecting the top k principal components. For more info, here you can find a brief tutorial.\nt-SNE Memo: t-SNE stands for t-Distributed Stochastic Neighbour Embedding and is an unsupervised non-linear technique. * It preserves small pairwise distance (or local similarities), whereas PCA aims to preserve large pairwise distances in order to maximize variance. * The basic idea of t-SNE is to compute a similarity measure between a pair of instances both at high and low dimensional space and optimize these two similarities via a cost function. * Properly using t-SNE is a bit tricky, a well recommended reading is one of the author’s blog.\nUMAP Memo: UMAP stands for Uniform Manifold Approximation and Projection for Dimensionality Reduction and is an unsupervised non-linear technique like t-SNE.\n\nFaster than t-SNE\nMore accurate than t-SNE in terms of data’s global structure preservation\nInformally constructs a high-dimensional graph representation of the data and then optimizes a low-dimensional graph to be as structurally similar as possibile\nCheck this blog if you are interested!\n\n\nHomework 📖\nPlay with the window size and check if there are some notable differences. \nGenerally: * Small window size \\(\\rightarrow\\) reflects syntactic properties. * Large window size \\(\\rightarrow\\) captures semantic ones.\n\n!pip install umap-learn\n\nRequirement already satisfied: umap-learn in /home/frgg/deasy_env/lib/python3.7/site-packages (0.5.3)\nRequirement already satisfied: numpy&gt;=1.17 in /home/frgg/deasy_env/lib/python3.7/site-packages (from umap-learn) (1.21.6)\nRequirement already satisfied: scikit-learn&gt;=0.22 in /home/frgg/deasy_env/lib/python3.7/site-packages (from umap-learn) (1.0.2)\nRequirement already satisfied: scipy&gt;=1.0 in /home/frgg/deasy_env/lib/python3.7/site-packages (from umap-learn) (1.7.3)\nRequirement already satisfied: numba&gt;=0.49 in /home/frgg/deasy_env/lib/python3.7/site-packages (from umap-learn) (0.56.4)\nRequirement already satisfied: pynndescent&gt;=0.5 in /home/frgg/deasy_env/lib/python3.7/site-packages (from umap-learn) (0.5.10)\nRequirement already satisfied: tqdm in /home/frgg/deasy_env/lib/python3.7/site-packages (from umap-learn) (4.64.1)\nRequirement already satisfied: llvmlite&lt;0.40,&gt;=0.39.0dev0 in /home/frgg/deasy_env/lib/python3.7/site-packages (from numba&gt;=0.49-&gt;umap-learn) (0.39.1)\nRequirement already satisfied: setuptools in /home/frgg/deasy_env/lib/python3.7/site-packages (from numba&gt;=0.49-&gt;umap-learn) (67.1.0)\nRequirement already satisfied: importlib-metadata in /home/frgg/deasy_env/lib/python3.7/site-packages (from numba&gt;=0.49-&gt;umap-learn) (6.0.0)\nRequirement already satisfied: joblib&gt;=0.11 in /home/frgg/deasy_env/lib/python3.7/site-packages (from pynndescent&gt;=0.5-&gt;umap-learn) (1.2.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /home/frgg/deasy_env/lib/python3.7/site-packages (from scikit-learn&gt;=0.22-&gt;umap-learn) (3.1.0)\nRequirement already satisfied: zipp&gt;=0.5 in /home/frgg/deasy_env/lib/python3.7/site-packages (from importlib-metadata-&gt;numba&gt;=0.49-&gt;umap-learn) (3.13.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.4 in /home/frgg/deasy_env/lib/python3.7/site-packages (from importlib-metadata-&gt;numba&gt;=0.49-&gt;umap-learn) (4.5.0)\n\n[notice] A new release of pip is available: 23.2.1 -&gt; 23.3.1\n[notice] To update, run: pip install --upgrade pip\n\n\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nimport umap\nimport matplotlib.pyplot as plt\n\n2023-10-27 09:45:07.609112: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n\n\n\ndef visualize_embeddings(embeddings: np.ndarray,\n                         word_annotations: List[str] = None,\n                         word_to_idx: Dict[str, int] = None):\n    \"\"\"\n    Plots given reduce word embeddings (2D). Users can highlight specific words (word_annotations list).\n\n    :param embeddings: word embedding matrix of shape (words, 2) retrieved via a dimensionality reduction technique.\n    :param word_annotations: list of words to be annotated.\n    :param word_to_idx: vocabulary map (word -&gt; index) (dict)\n    \"\"\"\n    fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n\n    if word_annotations:\n        print(f\"Annotating words: {word_annotations}\")\n\n        word_indexes = []\n        for word in word_annotations:\n            word_index = word_to_idx[word]\n            word_indexes.append(word_index)\n\n        word_indexes = np.array(word_indexes)\n\n        other_embeddings = embeddings[np.setdiff1d(np.arange(embeddings.shape[0]), word_indexes)]\n        target_embeddings = embeddings[word_indexes]\n\n        ax.scatter(other_embeddings[:, 0], other_embeddings[:, 1], alpha=0.1, c='blue')\n        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='red')\n        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='r', s=1000)\n\n        for word, word_index in zip(word_annotations, word_indexes):\n            word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n            ax.annotate(word, xy=(word_x, word_y))\n    else:\n        ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n\n    # We avoid outliers ruining the visualization if they are quite far away\n    axis_x_limit = (np.min(embeddings[:, 0]), np.max(embeddings[:, 0]))\n    axis_y_limit = (np.min(embeddings[:, 1]), np.max(embeddings[:, 1]))\n    plt.xlim(left=axis_x_limit[0] - 0.5, right=axis_x_limit[1] + 0.5)\n    plt.ylim(bottom=axis_y_limit[0] - 0.5, top=axis_y_limit[1] + 0.5)\n    ax.set_xlim(axis_x_limit[0], axis_x_limit[1])\n    ax.set_ylim(axis_y_limit[0], axis_y_limit[1])\n\n\ndef reduce_SVD(embeddings: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Applies SVD dimensionality reduction.\n\n    :param embeddings: word embedding matrix of shape (words, dim). In the case\n                       of a word-word co-occurrence matrix the matrix shape would\n                       be (words, words).\n\n    :return\n        - 2-dimensional word embedding matrix of shape (words, 2)\n    \"\"\"\n    print(\"Running SVD reduction method...\")\n    svd = TruncatedSVD(n_components=2, n_iter=10)\n    reduced = svd.fit_transform(embeddings)\n    print(\"SVD reduction completed!\")\n\n    return reduced\n\ndef reduce_tSNE(embeddings: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Applies t-SNE dimensionality reduction.\n    \"\"\"\n    print(\"Running t-SNE reduction method... (it may take a while...)\")\n    tsne = TSNE(n_components=2, n_iter=1000, metric='cosine')\n    reduced = tsne.fit_transform(embeddings)\n    print(\"t-SNE reduction completed!\")\n\n    return reduced\n\ndef reduce_umap(embeddings: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Applies UMAP dimensionality reduction.\n    \"\"\"\n    print(\"Running UMAP reduction method... (it may take a while...)\")\n    umap_emb = umap.UMAP(n_components=2, metric='cosine')\n    reduced = umap_emb.fit_transform(embeddings)\n    print(\"UMAP reduction completed!\")\n    \n    return reduced\n\n\n# Feel free to play with word_annotations argument!\n# Check the saved dictionary!\ndef run_visualization(method_name: str,\n                      words_list: List[str],\n                      word_to_idx: Dict[str, int],\n                      co_occurrence_matrix):\n    method_name = method_name.lower().strip()\n    method_map = {\n        'svd': reduce_SVD,\n        'tsne': reduce_tSNE,\n        'umap': reduce_umap\n    }\n    \n    if method_name not in method_map:\n        raise RuntimeError(f'Invalid method name! Got {method_name}.')\n    \n    reduced = method_map[method_name](co_occurrence_matrix)\n    visualize_embeddings(reduced, words_list, word_to_idx)\n\n\n# 'umap', 'tsne', 'svd'\nrun_visualization('umap', ['good', 'love', 'beautiful'], word_to_idx, co_occurrence_matrix)\nplt.show()\n\nRunning UMAP reduction method... (it may take a while...)\nUMAP reduction completed!\nAnnotating words: ['good', 'love', 'beautiful']\n\n\n\n\n\n\n\n\n\n\n\nHomework 📖\nPlay with different dimension reduction techniques.\nNote that these techniques are very sensitive to some of their hyper-parameters!\n\n\n\n2.4 Embedding properties\nVisualization can give us a rough idea of how word embeddings are organized and if some semantic properties are reflected in the numerical dimensional space.\n\\(\\rightarrow\\) For example, are synonyms close together? Ideally, if the dataset is big enough, we should see similar vector embeddings since synonyms usually have similar contexts.\n\nHow to do that?\nWe could highlight target words in the visualization step and check if our expectations are met. For instance, synonyms should be close together.\nHowever, this method is rather inaccurate and time-consuming (dimensionality reduction is not a perfect mapping).\n\\(\\rightarrow\\) We need some sort of similarity metric that is independent of the vector dimensionality.\n\n\n\n2.5 Cosine Similarity\nWe want to measure how two word vectors are far apart.\n\nNaive Method\nA naive solution would involve computing the dot product of the two vectors.\nHowever, this metric will give higher similarity either to longer vectors or to vectors that have higher counts.\n\n\nCosine Similarity\nA better metric is cosine similarity which is just a normalized dot product.\n\\[\\begin{align}\ns(p, q) = \\frac{p \\, \\cdot \\, q}{||p|| \\, \\cdot \\, ||q||}\n\\end{align}\\]\nwhere $s(p, q) $, since it computes the cosine of the angle between the two vectors.\nIntuitively, we are bringing vectors down to the d-dimensional unit sphere (d is the vocab size) and then computing their distance (in 2D space we will have a circle).\n\nfrom sklearn.metrics.pairwise import cosine_similarity as sk_cs\n\ndef cosine_similarity(p: np.ndarray,\n                      q: np.ndarray,\n                      transpose_p: bool = False,\n                      transpose_q: bool = False) -&gt; np.ndarray:\n    \"\"\"\n    Computes the cosine similarity of two d-dimensional matrices\n\n    :param p: d-dimensional vector (np.ndarray) of shape (p_samples, d)\n    :param q: d-dimensional vector (np.ndarray) of shape (q_samples, d)\n    :param transpose_p: whether to transpose p or not\n    :param transpose_q: whether to transpose q or not\n\n    :return\n        - cosine similarity matrix S of shape (p_samples, q_samples)\n          where S[i, j] = s(p[i], q[j])\n    \"\"\"\n    if len(p.shape) == 1:\n        p = p.reshape(1, -1)\n    if len(q.shape) == 1:\n        q = q.reshape(1, -1)\n\n    return sk_cs(p.T if transpose_p else p, q.T if transpose_q else q)\n\nsimilarity_matrix = cosine_similarity(co_occurrence_matrix,\n                                      co_occurrence_matrix)\n\n\n\n\n2.6 [Let’s play] Analogies, Bias, Synonyms and Antonyms\nLet’s look for some words and provide a possible explanation of achieved results according to cosine similarity metric.\n\nSynonym pair: (w1, w2) such that w1 and w2 are synonyms\nAntonyms pair: (w1, w2) such that w1 and w2 are antonyms\nSynonym-Antonym triplet: (w1, w2, w3) such that w1 and w2 are synonyms and w1 and w3 are antonyms\n\n\nAnalogy\nAnother useful property to check is analogy resolution via word vectors.\nIn particular, we might want to check if analogies such “man : king == woman : x” bring results like “x = queen”.\nIn order to do so, we first need to define a ranking function that returns the top \\(K\\) most similar words of a given word vector.\n\\(\\rightarrow\\) We might not want to be too much restrictive and play with \\(K \\ge 1\\).\n\nfrom itertools import product\n\ndef show_similarity_for_nary(words: List[str],\n                             similarity_matrix: np.ndarray,\n                             word_to_idx: Dict[str, int],\n                             idx_to_word: Dict[int, str]):\n    \"\"\"\n    Shows similarity values for each pair of input words.\n\n    :param words: a list of candidate words.\n    :param similarity_matrix: np.ndarray containing similarity values \n    between each vocabulary word pair\n    :param idx_to_word: vocabulary map (index -&gt; word) (dict)\n    :param word_to_idx: vocabulary map (word -&gt; index) (dict)\n\n    \"\"\"\n    word_indexes = [word_to_idx[word] for word in words]\n    similarity_dict = {}\n\n    for comb in product(word_indexes, word_indexes):\n        similarity_value = similarity_matrix[comb[0], comb[1]]\n        similarity_dict.setdefault(comb[0], []).append(similarity_value)\n\n    similarity_df = pd.DataFrame.from_dict(similarity_dict)\n    similarity_df.columns = [idx_to_word[col] for col in similarity_df.columns]\n    similarity_df.index = similarity_df.columns\n    similarity_df = similarity_df.transpose()\n    print(F'Similarity values: \\n{similarity_df}')\n\n\nshow_similarity_for_nary(['film', 'movie'], similarity_matrix, word_to_idx, idx_to_word)\nshow_similarity_for_nary(['good', 'bad'], similarity_matrix, word_to_idx, idx_to_word)\nshow_similarity_for_nary(['good', 'well', 'bad'], similarity_matrix, word_to_idx, idx_to_word)\n\nSimilarity values: \n           film     movie\nfilm   1.000000  0.863174\nmovie  0.863174  1.000000\nSimilarity values: \n          good       bad\ngood  1.000000  0.825867\nbad   0.825867  0.999999\nSimilarity values: \n          good      well       bad\ngood  1.000000  0.817286  0.825867\nwell  0.817286  0.999999  0.692384\nbad   0.825867  0.692384  0.999999\n\n\n\ndef get_top_K_indexes(data: np.ndarray, K: int) -&gt; np.ndarray:\n    \"\"\"\n    Returns the top K indexes of a 1-dimensional array (descending order)\n    Example:\n        data = [0, 7, 2, 1]\n        best_indexes:\n        K = 1 -&gt; [1] (data[1] = 7)\n        K = 2 -&gt; [1, 2]\n        K = 3 -&gt; [1, 2, 3]\n        K = 4 -&gt; [1, 2, 3, 4]\n\n    :param data: 1-d dimensional array\n    :param K: number of highest value elements to consider\n\n    :return\n        - array of indexes corresponding to elements of highest value\n    \"\"\"\n    best_indexes = np.argsort(data, axis=0)[::-1]\n    best_indexes = best_indexes[:K]\n\n    return best_indexes\n\n\ndef get_top_K_word_ranking(embedding_matrix: np.ndarray, idx_to_word: Dict[int, str],\n                           word_to_idx: Dict[str, int],  positive_listing: List[str],\n                           negative_listing: List[str],  K: int) -&gt; (List[str], np.ndarray):\n    \"\"\"\n    Finds the top K most similar words following this reasoning:\n        1. words that have highest similarity to words in positive_listing\n        2. words that have highest distance to words in negative_listing\n        \n    :param embedding_matrix: embedding matrix of shape (words, embedding dimension).\n    :param idx_to_word: vocabulary map (index -&gt; word) (dict)\n    :param word_to_idx: vocabulary map (word -&gt; index) (dict)\n    :param positive_listing: list of words that should have high similarity with top K retrieved ones.\n    :param negative_listing: list of words that should have high distance to top K retrieved ones.\n    :param K: number of best word matches to consider\n\n    :return\n        - top K word matches according to aforementioned criterium\n        - similarity values of top K word matches according to aforementioned criterium\n    \"\"\"\n    # Positive words (similarity)\n    positive_indexes = np.array([word_to_idx[word] for word in positive_listing])\n    word_positive_vector = np.sum(embedding_matrix[positive_indexes, :], axis=0)\n\n    # Negative words (distance)\n    negative_indexes = np.array([word_to_idx[word] for word in negative_listing])\n    word_negative_vector = np.sum(embedding_matrix[negative_indexes, :], axis=0)\n\n    # Find candidate words\n    target_vector = (word_positive_vector - word_negative_vector) / (len(positive_listing) + len(negative_listing))\n    total_indexes = np.concatenate((positive_indexes, negative_indexes))\n    valid_indexes = np.setdiff1d(np.arange(similarity_matrix.shape[0]), total_indexes)\n    candidate_vectors = embedding_matrix[valid_indexes]\n\n    candidate_similarities = cosine_similarity(candidate_vectors, target_vector)\n    candidate_similarities = candidate_similarities.ravel()\n\n    relative_indexes = get_top_K_indexes(candidate_similarities, K)\n    top_K_indexes = valid_indexes[relative_indexes]\n    top_K_words = [idx_to_word[idx] for idx in top_K_indexes]\n\n    return top_K_words, candidate_similarities[relative_indexes]\n\n\n\"\"\"\n    Positive and negative listing can be defined accordingly to a given analogy\n    Example:\n        \n        man : king :: woman : x\n    \n    positive_listing = ['king', 'woman']\n    negative_listing = ['man']\n\n    This is equivalent to: compute king - man + woman, and then find the\n    most similar candidate.\n\"\"\"\n\n# Examples\n# tv : episodes :: film : x\n# masterpiece : superb :: x : tragic\ntop_K_words, top_K_values = get_top_K_word_ranking(embedding_matrix=co_occurrence_matrix,\n                                                   idx_to_word=idx_to_word,\n                                                   word_to_idx=word_to_idx,\n                                                   positive_listing=['episodes', 'film'],\n                                                   negative_listing=['tv'],\n                                                   K=10)\nprint(f'Top K words: {top_K_words}')\nprint(f'Top K values: {top_K_values}')\n\nTop K words: ['movie', 'one', 'good', 'well', 'even', 'would', 'time', 'like', 'story', 'films']\nTop K values: [0.8511374  0.8176137  0.7851401  0.78265595 0.7800821  0.7769852\n 0.7755997  0.7706551  0.7676234  0.76355755]\n\n\n\n\nHomework 📖\nTake some time to play with different analogies.\nHow? Check the vocabulary and related tokens frequency!"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#dense-embeddings",
    "href": "nlp-course-material/2023-2024/Tutorial 2/tutorial2-2324.html#dense-embeddings",
    "title": "Tutorial 2",
    "section": "1. Dense embeddings",
    "text": "1. Dense embeddings\nUntil now we’ve worked with sparse embedding methods, which lead to high dimensional word embeddings (dimension equal to \\(|V|\\)).\nThe main drawback of such approach is that words belong to separate dimensions.\n\\(\\rightarrow\\) We need to have a large corpus available to check if two words have similar contexts.\n\nDense Embedding Technique\nTo this end, we might prefer a dense embedding technique.  \\(\\rightarrow\\) All words are encoded into a high dimensional space, much smaller than \\(|V|\\) (generally up to \\(\\sim 1000\\)).\nA dense representation is also convenient from a machine learning point of view: * Fewer parameters to learn and, thus, models are less prone to overfitting. * Words do not belong to separate dimensions anymore and semantic relationships are easily modelled.\n\n\n1.1 Working with a pre-trained model\nThe first step consists in choosing and downloading a pre-trained embedding model.\nFor the purpose of this assignment, we limit to classic models, such as Word2Vec, GloVe and FastText.\nFurthermore, some pre-trained embedding model versions may be quite resource demanding, depending on the embedding dimension and on the vocabulary size.\n\\(\\rightarrow\\) We recommend sticking to low dimensional spaces (50, 100, 200) to avoid being stuck waiting for too much time.\n\n\nA Brief Recap\nFor a full list of available embeddings models, please check here.\n\nWord2Vec: the first example of dense word encoding. There are two well-known strategies:\n\nContinuous Bag-of-words (CBoW): context words are used to predict a target word.\nSkip-gram: a target word is used to predict its own context.\n\nGloVe: it is another techniques that tries to encoded global semantic properties based on the co-occurrence matrix. Conversely, word2vec exploits local information.\nFasttext: an extension of word2vec where words are defined as character n-grams. It works very well with rare words contrarily to Word2Vec and GloVe.\n\n\n!pip install gensim\n\nRequirement already satisfied: gensim in /home/frgg/deasy_env/lib/python3.7/site-packages (4.2.0)\nRequirement already satisfied: numpy&gt;=1.17.0 in /home/frgg/deasy_env/lib/python3.7/site-packages (from gensim) (1.21.6)\nRequirement already satisfied: scipy&gt;=0.18.1 in /home/frgg/deasy_env/lib/python3.7/site-packages (from gensim) (1.7.3)\nRequirement already satisfied: smart-open&gt;=1.8.1 in /home/frgg/deasy_env/lib/python3.7/site-packages (from gensim) (6.3.0)\n\n[notice] A new release of pip is available: 23.2.1 -&gt; 23.3.1\n[notice] To update, run: pip install --upgrade pip\n\n\n\nimport gensim\nimport gensim.downloader as gloader\n\ndef load_embedding_model(model_type: str,\n                         embedding_dimension: int = 50) -&gt; gensim.models.keyedvectors.KeyedVectors:\n    \"\"\"\n    Loads a pre-trained word embedding model via gensim library.\n\n    :param model_type: name of the word embedding model to load.\n    :param embedding_dimension: size of the embedding space to consider\n\n    :return\n        - pre-trained word embedding model (gensim KeyedVectors object)\n    \"\"\"\n    download_path = \"\"\n    if model_type.strip().lower() == 'word2vec':\n        download_path = \"word2vec-google-news-300\"\n\n    elif model_type.strip().lower() == 'glove':\n        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n    elif model_type.strip().lower() == 'fasttext':\n        download_path = \"fasttext-wiki-news-subwords-300\"\n    else:\n        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n        \n    try:\n        emb_model = gloader.load(download_path)\n    except ValueError as e:\n        print(\"Invalid embedding model name! Check the embedding dimension:\")\n        print(\"Word2Vec: 300\")\n        print(\"Glove: 50, 100, 200, 300\")\n        print('FastText: 300')\n        raise e\n\n    return emb_model\n\n\n# Modify these variables as you wish!\n# Glove -&gt; 50, 100, 200, 300\n# Word2Vec -&gt; 300\n# Fasttext -&gt; 300\nembedding_model = load_embedding_model(model_type=\"glove\",\n                                       embedding_dimension=50)\n\n\nHomework 📖\nPlay with different embedding models and embedding sizes.\nCompare the achieved visualizations.\n\n\n\n1.2 Out of vocabulary (OOV) words\nBefore evaluating pre-trained dense word embeddings, it is good practice to check if the model is consistent with our dataset.\n\\(\\rightarrow\\) We check the number of out-of-vocabulary (OOV) terms.\nIf the OOV amount is negligible, we can just keep going. On the other hand, we might want to handle OOV terms by assigning them a specific word vector.\nWhich one? One common practice is to assign a random vector, since the embedding model will be part of a deep learning model and, thus, word vectors might be trained during the learning process.\nEven if that is the case, we can assign an embedding that is more meaningful rather than a random one.\n\\(\\rightarrow\\) We can identify the word embedding of an OOV term as the mean of its neighbour word embeddings.\n\ndef check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n                    word_listing: List[str]):\n    \"\"\"\n    Checks differences between pre-trained embedding model vocabulary\n    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n\n    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n    :param word_listing: dataset specific vocabulary (list)\n\n    :return\n        - list of OOV terms\n    \"\"\"\n    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n    oov = set(word_listing).difference(embedding_vocabulary)\n    return list(oov)\n\n\noov_terms = check_OOV_terms(embedding_model, word_listing)\noov_percentage = float(len(oov_terms)) * 100 / len(word_listing)\nprint(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")\n\nTotal OOV terms: 2551 (12.96%)\n\n\n\n\n1.3 Handling OOV words\nNow we proceed on building the embedding matrix, while handling OOV terms at the same time.\n\ndef build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n                           embedding_dimension: int,\n                           word_to_idx: Dict[str, int],\n                           vocab_size: int,\n                           oov_terms: List[str]) -&gt; np.ndarray:\n    \"\"\"\n    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n\n    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n    :param word_to_idx: vocabulary map (word -&gt; index) (dict)\n    :param vocab_size: size of the vocabulary\n    :param oov_terms: list of OOV terms (list)\n\n    :return\n        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n    \"\"\"\n    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n    for word, idx in tqdm(word_to_idx.items()):\n        try:\n            embedding_vector = embedding_model[word]\n        except (KeyError, TypeError):\n            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n\n        embedding_matrix[idx] = embedding_vector\n\n    return embedding_matrix\n\n\n# Testing\nembedding_dimension = 50\nembedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, len(word_to_idx), oov_terms)\nprint(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n\n100%|█████████████████████████████████| 19681/19681 [00:00&lt;00:00, 520433.86it/s]\n\n\nEmbedding matrix shape: (19681, 50)\n\n\n\n\n\n\n\n1.4 Embedding visualization (cont’d)\nWe are now ready to visualize pre-trained word embeddings!\n\n# UMAP\nreduced_embedding_umap = reduce_umap(embedding_matrix)\nvisualize_embeddings(reduced_embedding_umap, ['good', 'love', 'beautiful'], word_to_idx)\n\nplt.show()\n\nRunning UMAP reduction method... (it may take a while...)\nUMAP reduction completed!\nAnnotating words: ['good', 'love', 'beautiful']\n\n\n\n\n\n\n\n\n\n\n\n1.5 [Let’s play!] Embedding properties (cont’d)\nLet’s consider some previous examples for quick comparison.\n\ntop_K_words, top_K_values = get_top_K_word_ranking(embedding_matrix=embedding_matrix,\n                                                   idx_to_word=idx_to_word,\n                                                   word_to_idx=word_to_idx,\n                                                   positive_listing=['episodes', 'film'],\n                                                   negative_listing=['tv'],\n                                                   K=10)\nprint(f'Top K words: {top_K_words}')\nprint(f'Top K values: {top_K_values}')\n\nTop K words: ['adaptation', 'episode', 'novel', 'films', 'sequel', 'novels', 'screenplay', 'soundtrack', 'cameo', 'drama']\nTop K values: [0.78791964 0.78719956 0.7586307  0.7466293  0.73375267 0.7320131\n 0.73011214 0.72881866 0.708628   0.6958587 ]\n\n\n\nHomework 📖\nCompare word embedding analysis between sparse and dense word embeddings."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html",
    "title": "Tutorial 3",
    "section": "",
    "text": "Credits: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\nKeywords: Transformers, Huggingface, Prompting"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#part-0-5-mins",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#part-0-5-mins",
    "title": "Tutorial 3",
    "section": "PART 0 ($$5 mins)",
    "text": "PART 0 ($$5 mins)\n\nDownloading a dataset.\nEncoding a a dataset."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#part-i-30-mins",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#part-i-30-mins",
    "title": "Tutorial 3",
    "section": "PART I ($$30 mins)",
    "text": "PART I ($$30 mins)\n\nText encoding with transformers.\nModel definition.\nModel training and evaluation with huggingface APIs."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#part-ii-30-mins",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#part-ii-30-mins",
    "title": "Tutorial 3",
    "section": "PART II ($$30 mins)",
    "text": "PART II ($$30 mins)\n\nPrompting 101\nSentiment analysis with prompting\nAdvanced prompting"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#preliminaries",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#preliminaries",
    "title": "Tutorial 3",
    "section": "Preliminaries",
    "text": "Preliminaries\nFirst of all, we need to import some useful packages that we will use during this hands-on session.\n\n# system packages\nfrom pathlib import Path\nimport shutil\nimport urllib\nimport tarfile\nimport sys\n\n# data and numerical management packages\nimport pandas as pd\nimport numpy as np\n\n# useful during debugging (progress bars)\nfrom tqdm import tqdm\n\n\n!pip install torch==1.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n!pip install transformers==4.30.0\n!pip install datasets==2.13.2\n!pip install accelerate -U\n!pip install evaluate\n\n\nimport torch\ntorch.cuda.is_available()\n\n/home/frgg/hf_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nTrue\n\n\n\n!nvidia-smi\n\nFri Nov 10 09:24:44 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n| N/A   32C    P0    22W /  80W |      8MiB /  8192MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      2234      G   /usr/lib/xorg/Xorg                  4MiB |\n+-----------------------------------------------------------------------------+\n\n\n\nfrom notebook.services.config import ConfigManager\ncm = ConfigManager()\ncm.update('livereveal', {\n        'width': 2560,\n        'height': 1440,\n        'scroll': True,\n})\n\n{'width': 2560, 'height': 1440, 'scroll': True}"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#text-encoding-with-transformers.",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#text-encoding-with-transformers.",
    "title": "Tutorial 3",
    "section": "1. Text encoding with Transformers.",
    "text": "1. Text encoding with Transformers.\nIn tutorial 1, we have seen how to define standard machine learning models to address sentiment classification.\nHowever, we know that Transformer-based models are one of the strongest baselines when assessing a task or benchmarking on a novel corpus.\nBefore defining our transformer-based classifier, we need to encode text inputs into numerical format.\nAs in Tutorial 1, we are going to tokenize input texts to perform token indexing.\n\n1.1 Encoding the dataset\nFirst, we are going to use datasets library to encode our dataset into a handy wrapper for computational speedup.\n\nfrom datasets import Dataset\n\n# Slicing for showcasing purposes only!\ntrain_df = df.loc[df['split'] == \"train\"].sample(frac=1.0)[:5000]\ntest_df = df.loc[df['split'] == \"test\"].sample(frac=1.0)[:1000]\n\ntrain_data = Dataset.from_pandas(train_df)\ntest_data = Dataset.from_pandas(test_df)\n\nLet’s inspect the newly defined Dataset instances\n\nprint(train_data)\nprint(test_data)\n\nDataset({\n    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__'],\n    num_rows: 5000\n})\nDataset({\n    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__'],\n    num_rows: 1000\n})\n\n\n\n\n1.2 Tokenization\nTransformers typically use SentencePiece tokenizer to perform sub-word level tokenization.\nIn particular, the transformers library offers the AutoTokenizer class to quickly retrieve our chosen transformer’s ad-hoc tokenizer.\n\nfrom transformers import AutoTokenizer\n\nmodel_card = 'distilbert-base-uncased'\n\ntokenizer = AutoTokenizer.from_pretrained(model_card)\n\nThe model_card variable defines the path where to look for our pre-trained model.\nYou can check huggingface’s hub model hub to pick the model card according to your preference.\nWe proceed on tokenizing movie reviews text with our tokenizer.\n\ndef preprocess_text(texts):\n    return tokenizer(texts['text'], truncation=True)\n\ntrain_data = train_data.map(preprocess_text, batched=True)\ntest_data = test_data.map(preprocess_text, batched=True)\n\n                                                                                \n\n\nLet’s inspect the preprocess Dataset instances\n\nprint(train_data) \nprint(test_data)\n\nDataset({\n    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n    num_rows: 5000\n})\nDataset({\n    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n    num_rows: 1000\n})\n\n\n\nprint(train_data['input_ids'][50])\n\n[101, 1045, 2228, 2008, 2026, 5440, 2112, 1997, 2023, 3185, 1010, 1996, 2028, 2008, 4654, 6633, 24759, 14144, 1996, 11591, 23100, 1010, 28072, 1998, 27118, 22758, 1997, 1996, 8931, 1010, 3310, 2012, 1996, 14463, 1997, 1996, 2143, 1012, 3460, 6945, 5912, 1998, 2010, 17204, 2767, 1996, 6458, 2031, 2633, 25878, 1996, 13721, 2158, 2006, 1037, 4899, 2006, 2070, 5108, 1999, 2019, 5992, 11717, 3269, 1012, 2562, 1999, 2568, 2008, 5912, 2038, 2042, 2559, 2005, 1996, 3461, 2005, 3053, 1996, 2972, 2143, 1010, 1998, 2008, 1996, 3461, 2038, 2730, 1998, 8828, 2195, 2111, 2012, 2023, 2391, 1006, 2164, 2010, 5795, 1007, 1010, 1998, 5912, 2003, 2200, 5204, 2008, 3461, 2003, 14196, 9577, 1998, 7501, 2005, 2529, 5771, 1998, 2668, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2061, 1996, 6458, 2038, 2010, 3282, 4197, 2012, 3461, 1010, 2040, 2003, 1010, 1998, 1045, 2507, 1996, 3185, 1998, 6174, 6243, 24387, 2005, 2023, 1010, 1996, 2087, 19424, 1998, 17082, 4874, 1999, 2529, 2433, 2008, 2057, 2031, 2412, 2464, 1012, 1998, 2002, 22114, 1037, 2200, 2590, 3160, 2000, 3460, 6945, 5912, 1024, 1000, 2054, 2079, 2057, 2079, 2085, 1029, 999, 1029, 999, 1029, 1000, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 4950, 7659, 2058, 2000, 3460, 6945, 5912, 1010, 1998, 2009, 1005, 1055, 5793, 2008, 6945, 2038, 2053, 2801, 2054, 2000, 2079, 2279, 1012, 4593, 6945, 2001, 2061, 7848, 2006, 1996, 3291, 1997, 4531, 1996, 13721, 2158, 1010, 2002, 2196, 2245, 2000, 3288, 2247, 2070, 28285, 5733, 1010, 1037, 27333, 2080, 1010, 2030, 11195, 17364, 3388, 1010, 2030, 1037, 5658, 1010, 2030, 2070, 25283, 26147, 17629, 17493, 1010, 2030, 2672, 1037, 2047, 2287, 6823, 2011, 3158, 12439, 2483, 2000, 28384, 1996, 9576, 6841, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2061, 1996, 6458, 6634, 2015, 1998, 11758, 1010, 1996, 13721, 2158, 3632, 2022, 22573, 8024, 1010, 1998, 7632, 8017, 3012, 4372, 6342, 2229, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2672, 2023, 7607, 2339, 9274, 2038, 2042, 29082, 2105, 2007, 1996, 2686, 10382, 2565, 1999, 4942, 1011, 11926, 2686, 2005, 1996, 2197, 2382, 2086, 2612, 1997, 2183, 2067, 2000, 1996, 4231, 2030, 2041, 2000, 7733, 2066, 3071, 4282, 2027, 11276, 2000, 2022, 2725, 1012, 1045, 14145, 2080, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 4312, 1010, 2008, 1005, 1055, 1996, 2785, 1997, 10223, 6508, 1010, 13971, 3015, 1998, 3257, 2008, 2104, 12690, 2015, 2296, 7814, 1997, 2023, 3185, 1012, 2009, 1005, 1055, 2524, 2000, 2360, 2129, 2204, 1996, 5889, 2941, 2024, 1010, 2138, 1996, 3185, 2038, 3143, 17152, 2005, 2037, 3494, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2048, 2060, 11757, 9145, 10071, 2036, 13276, 2039, 1996, 28072, 1997, 1996, 8931, 1024, 2045, 2003, 1037, 3496, 3794, 1996, 15116, 10458, 2214, 3232, 1999, 1996, 2088, 2667, 2000, 8954, 14380, 2015, 2013, 1037, 7676, 1010, 2069, 2000, 2022, 7950, 4237, 2011, 1996, 13721, 2158, 1012, 2023, 3496, 2003, 1037, 23233, 4313, 1999, 17549, 5988, 1012, 1045, 2064, 11302, 2017, 1005, 2310, 2196, 3427, 1037, 2062, 23100, 1998, 29348, 16437, 2007, 5976, 2121, 2559, 2111, 1999, 102]\n\n\n\nprint(train_data['attention_mask'][50])\n\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n\nWe can perform some quick sanity check to evaluate the tokenization process\n\noriginal_text = train_data['text'][50]\ndecoded_text = tokenizer.decode(train_data['input_ids'][50])\n\nprint(original_text)\nprint()\nprint()\nprint(decoded_text)\n\nI think that my favorite part of this movie, the one that exemplifies the sheer pointless, stupidity and inanity of the proceedings, comes at the climax of the film. DOCTOR TED NELSON and his unmarried friend the Sheriff have finally cornered the Melting Man on a landing on some stairs in an electrical generating plant. Keep in mind that Nelson has been looking for the MM for nearly the entire film, and that the MM has killed and eaten several people at this point (including his boss), and Nelson is very aware that MM is violently insane and hungry for human flesh and blood.&lt;br /&gt;&lt;br /&gt;So the Sheriff has his gun pointed at MM, who is, and I give the movie and Rick Baker props for this, the most disgusting and terrifying object in human form that we have ever seen. And he yells a very important question to DOCTOR TED NELSON: \"WHAT DO WE DO NOW?!?!?\" &lt;br /&gt;&lt;br /&gt;The camera cuts over to DOCTOR TED NELSON, and it's obvious that Ted has no idea what to do next. Apparently Ted was so intent on the problem of FINDING the Melting Man, he never thought to bring along some restraining devices, a lasso, or straitjacket, or a net, or some tranquilizer darts, or maybe a New Age tape by Vangelis to soothe the savage beast.&lt;br /&gt;&lt;br /&gt;So the sheriff panics and shoots, the Melting Man goes berserk, and hilarity ensues. &lt;br /&gt;&lt;br /&gt;Maybe this explains why NASA has been screwing around with the Space Shuttle program in sub-lunar space for the last 30 years instead of going back to the Moon or out to Mars like everyone knows they OUGHT to be doing. I dunno.&lt;br /&gt;&lt;br /&gt;Anyway, that's the kind of lousy, lazy writing and direction that undercuts every aspect of this movie. It's hard to say how good the actors actually are, because the movie has complete contempt for their characters.&lt;br /&gt;&lt;br /&gt;Two other incredibly painful sequences also ramp up the stupidity of the proceedings: There is a scene featuring the lumpiest old couple in the world trying to steal lemons from a grove, only to be torn apart by the Melting Man. This scene is a nadir in 70s cinema. I can guarantee you've never watched a more pointless and irritating setup with odder looking people in your entire life. And the Melting Man's assault on the lady who lives in the house where they keep a horse who pees on the walls defies every attempt to process it.(BTW, I think famous film director Jonathon Demme has a walk-on in this scene as the redneck husband who goes in first to check on the house and never comes out again). The only thing that keeps the actress from literally chewing the scenery is that, as I said, their horse has apparently been peeing on it. And we are forced to watch her hysterics for at least two minutes longer than any SANE film director would hold the shot. &lt;br /&gt;&lt;br /&gt;Burr DeBenning ought to beat the crap out of IMM's director and photographer. I remember him from an old Columbo episode where he looked MUCH better than he does here - no one's idea of a leading man, but solid and unobtrusive. But no one could possibly be as unappealing in real life as his director makes him look here. &lt;br /&gt;&lt;br /&gt;Everyone else comes off a little better except for the old couple (and shut up, I know they were being played for laughs, but I ain't laughing!) but not much. &lt;br /&gt;&lt;br /&gt;This definitely falls into the 'So Bad You Can't Look Away' category of cinema disasters. Still, I'd watch it again before I'd watch a lot of other 70's and 80's abortions ( \"Track of The Moonbeast\" and \"It Lives By Night\" come to mind), and MST's coverage of it is great fun, so if you get a chance, watch the MST version.\n\n\n[CLS] i think that my favorite part of this movie, the one that exemplifies the sheer pointless, stupidity and inanity of the proceedings, comes at the climax of the film. doctor ted nelson and his unmarried friend the sheriff have finally cornered the melting man on a landing on some stairs in an electrical generating plant. keep in mind that nelson has been looking for the mm for nearly the entire film, and that the mm has killed and eaten several people at this point ( including his boss ), and nelson is very aware that mm is violently insane and hungry for human flesh and blood. &lt; br / &gt; &lt; br / &gt; so the sheriff has his gun pointed at mm, who is, and i give the movie and rick baker props for this, the most disgusting and terrifying object in human form that we have ever seen. and he yells a very important question to doctor ted nelson : \" what do we do now?!?!? \" &lt; br / &gt; &lt; br / &gt; the camera cuts over to doctor ted nelson, and it's obvious that ted has no idea what to do next. apparently ted was so intent on the problem of finding the melting man, he never thought to bring along some restraining devices, a lasso, or straitjacket, or a net, or some tranquilizer darts, or maybe a new age tape by vangelis to soothe the savage beast. &lt; br / &gt; &lt; br / &gt; so the sheriff panics and shoots, the melting man goes berserk, and hilarity ensues. &lt; br / &gt; &lt; br / &gt; maybe this explains why nasa has been screwing around with the space shuttle program in sub - lunar space for the last 30 years instead of going back to the moon or out to mars like everyone knows they ought to be doing. i dunno. &lt; br / &gt; &lt; br / &gt; anyway, that's the kind of lousy, lazy writing and direction that undercuts every aspect of this movie. it's hard to say how good the actors actually are, because the movie has complete contempt for their characters. &lt; br / &gt; &lt; br / &gt; two other incredibly painful sequences also ramp up the stupidity of the proceedings : there is a scene featuring the lumpiest old couple in the world trying to steal lemons from a grove, only to be torn apart by the melting man. this scene is a nadir in 70s cinema. i can guarantee you've never watched a more pointless and irritating setup with odder looking people in [SEP]\n\n\n\n\n1.3 Vocabulary\nWe do not necessarily need to build a vocabulary since transformers already come with their own!\nHowever, it is still possible to add new tokens to the vocabulary to adapt the model to the given use case.\ntokenizer.add_tokens(new_tokens=new_tokens)\nThe transformer vocabulary will update its unusued vocabulary indexes with newly provided tokens.\n\n\n1.4 Special tokens\nPay attention to used special tokens and their corresponding token ids.\nEach transformer models has its own special tokens ([CLS], [SEP], [PAD], [EOS], etc…).\nThus, the same special token may be mapped to different token ids in distinct transformer models.\n\n\n1.5 Text cleaning\nWe didn’t perform any kind of text cleaning before performing text encoding.\nThis is usually because transformer tokenizers have their own text cleaning process to perform tokenization.\nThus, models may be sensitive to custom operations!\n\nexample_text = \"couldn't\"\nencoded_example = tokenizer.encode_plus(example_text, add_special_tokens=False)\nprint(encoded_example.tokens())\n\n['couldn', \"'\", 't']\n\n\n\nexample_text = \"At one point,some kids are wandering through the deeper levels, exploring.\"\nencoded_example = tokenizer.encode_plus(example_text, add_special_tokens=False)\nprint(encoded_example.tokens())\n\n['at', 'one', 'point', ',', 'some', 'kids', 'are', 'wandering', 'through', 'the', 'deeper', 'levels', ',', 'exploring', '.']\n\n\n\nExample\nbert-base-uncased is trained with text in lower format.\nCheck model cards on huggingface to know more about the models you use and inspect their text encoding pipeline to understand how they behave.\n\n\nHomework 📖\nExperiment with different model cards.\nExperiment with text cleaning and evaluate its impact on classification."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#model-definition",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#model-definition",
    "title": "Tutorial 3",
    "section": "2. Model definition",
    "text": "2. Model definition\nWe are now ready to define our transformer-based classifier.\n\n2.1 Data Formatting\nWe first need to format input data to be fed as mini-batches in a training/evaluation procedure.\n\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nThe DataCollatorWithPadding receives a batch of\n(input_ids, attention_mask, token_type_ids, label)\ntuples and dynamically pads input_ids, attention_mask and token_type_ids to maximum sequence in the batch.\nIntuitively, this operation saves a lot of memory compared to padding to global maximum sequence, while it introduces a reasonable computational overhead.\n\n\nNote\nThe above example is just one way out of many to perform dynamic batch padding: it really depends on which data structures you are using.\n\n\n2.2 Model definition\nDefining a transformer-based model with huggingface is pretty straightforward!\nSince we are dealing with text classification, we can use off-the-shelf AutoModelForSequenceClassification.\n\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_card,\n                                                           num_labels=2,\n                                                           id2label={0: 'NEG', 1: 'POS'},\n                                                           label2id={'NEG': 0, 'POS': 1})\n\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nLet’s first check the loaded model architecture.\n\nprint(model)\n\nDistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n\n\nThat’s it!\nThat’s the simplicity of huggingface’s APIs.\nThe model is ready to use for classification.\n\n\n2.3 Custom architectures\nThere are plenty of pre-defined model architectures \\(\\rightarrow\\) auto classes\nIn more complex scenarios, we may want to define a custom architecture where the pre-trained model is part of it.\nIn these cases, the way you do it strongly depends on the underlying neural library.\nHowever, there exist several high-level APIs depending on your needs."
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#model-training-and-evaluation",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#model-training-and-evaluation",
    "title": "Tutorial 3",
    "section": "3. Model training and evaluation",
    "text": "3. Model training and evaluation\nWe are now ready to define the training and evaluation procedures to test our model on the IMDB dataset.\nIn particular, we are going to use Trainer APIs to efficiently perform training.\n\n3.1 Metrics\nFirst, we define classification metrics for evaluation.\n\nfrom sklearn.metrics import f1_score, accuracy_score\n\ndef compute_metrics(output_info):\n    predictions, labels = output_info\n    predictions = np.argmax(predictions, axis=-1)\n    \n    f1 = f1_score(y_pred=predictions, y_true=labels, average='macro')\n    acc = accuracy_score(y_pred=predictions, y_true=labels)\n    return {'f1': f1, 'acc': acc}\n\n\n\nHugginface’s metrics\nHuggingface’s offers the Evaluate package that contains several evaluation metrics (e.g., accuracy, f1, squad-f1, etc…)\n\nimport evaluate\n\nacc_metric = evaluate.load('accuracy')\nf1_metric = evaluate.load('f1')\n\ndef compute_metrics(output_info):\n    predictions, labels = output_info\n    predictions = np.argmax(predictions, axis=-1)\n    \n    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n    acc = acc_metric.compute(predictions=predictions, references=labels)\n    return {**f1, **acc}\n    \n\n\n\n3.2 Training Arguments\nThe Trainer object can be extensively customized.\nFeel free to check the documentation on training arguments.\nWe first rename the sentiment column to label as the default input to AutoModelForSequenceClassification.\n\ntrain_data = train_data.rename_column('sentiment', 'label')\ntest_data = test_data.rename_column('sentiment', 'label')\n\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"test_dir\",                 # where to save model\n    learning_rate=2e-5,                   \n    per_device_train_batch_size=8,         # accelerate defines distributed training\n    per_device_eval_batch_size=8,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",           # when to report evaluation metrics/losses\n    save_strategy=\"epoch\",                 # when to save checkpoint\n    load_best_model_at_end=True,\n    report_to='none'                       # disabling wandb (default)\n)\n\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n\n\nTraining schema with collator\n\n\n\n\ntrainer.train()\n\n/home/frgg/hf_env/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n    \n      \n      \n      [625/625 02:48, Epoch 1/1]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nF1\nAccuracy\n\n\n\n\n1\n0.342800\n0.266022\n0.913900\n0.914000\n\n\n\n\n\n\nTrainOutput(global_step=625, training_loss=0.333054443359375, metrics={'train_runtime': 169.1831, 'train_samples_per_second': 29.554, 'train_steps_per_second': 3.694, 'total_flos': 622869987692928.0, 'train_loss': 0.333054443359375, 'epoch': 1.0})\n\n\n\n\n3.3 Evaluation\nWe now evaluate the trained model on the test set.\n\ntest_prediction_info = trainer.predict(test_data)\ntest_predictions, test_labels = test_prediction_info.predictions, test_prediction_info.label_ids\n\nprint(test_predictions.shape)\nprint(test_labels.shape)\n\n\n\n\n(1000, 2)\n(1000,)\n\n\n\ntest_metrics = compute_metrics([test_predictions, test_labels])\nprint(test_metrics)\n\n{'f1': 0.9139004689420969, 'accuracy': 0.914}\n\n\n\n\nSome cleaning before PART II\nLet’s clean the memory and GPU before switching to instruction-tuned models.\n\nimport gc\n\nmodel = None\ndel model\ntrainer = None\ndel trainer\n\nwith torch.no_grad():\n    torch.cuda.empty_cache()\n\ngc.collect()\n\n112"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#prompting-101",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#prompting-101",
    "title": "Tutorial 3",
    "section": "1. Prompting 101",
    "text": "1. Prompting 101\nPrompting is a technique used to adapt a model to a variety of tasks without requiring fine-tuning.\nClassify the text into neutral, negative or positive.\nText: {text}\nSentiment:\nThe model receives the above input prompt and performs text classification via completion.\nClassify the text into neutral, negative or positive.\nText: {text}\nSentiment: {label}\nIn natural language, prompting is a very delicate process since natural language is expressive, flexible, and, ambiguous.\nA certain concept can be expressed in several ways:\n\nThese ways are semantically equivalent\nMay lead to significant model performance drifts\n\n\n1.1 Sensitivity Factors\nThere are two main factors to consider when performing prompt-based learning.\n\nPrompt Engineering\nEventually we have to iteratively find the best performing prompt.\nThis can either done\n\nManually\nAutomatically (via an ad-hoc model).\n\n\n\nGeneration hyper-parameters\nFinding the optimal text generation strategy is a critical point for achieving satisfying performance.\nThese strategies affects how the model iteratively selects tokens during generation to avoid phenomena like repetitions, rare words, coherence with input text, and style.\n\n[Deterministic] Greedy \\(\\rightarrow\\) the most preferred (i.e., highest likelihood) token wins\n[Deterministic] Beam search\n[Stochastic] Top-k sampling\n[Stochastic] Nucleus sampling\nContrastive search \\(\\leftarrow\\) recommended\n\n\n\n\n1.2 Model types\nThere are a lot of different large language models and it is quite easy to be confused.\nEssentially, we have:\n\nBase models (either encoders or encode-decoders): very good at text completion.\nChat-based models: base models specifically fine-tuned to address instructions or to chat.\n\n\nExample\nIn Huggingface, the distinct is easily formatted as:\n\nllama2-7b \\(\\rightarrow\\) base model\nllama2-7b-*-instruct \\(\\rightarrow\\) chat-based model"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#sentiment-analysis-with-prompting",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#sentiment-analysis-with-prompting",
    "title": "Tutorial 3",
    "section": "2. Sentiment analysis with prompting",
    "text": "2. Sentiment analysis with prompting\nLet’s consider our task once again to evaluate prompt-based models.\n\nPreliminaries\nWe first install some package(s) for efficient computation given our hardware limitations.\n\n!git clone https://github.com/timdettmers/bitsandbytes.git\n!cd bitsandbytes\n!python setup.py install\n\n# Alternatively (Colab) --&gt; restart runtime afterwards! (re-run part I and the first code cell of Part I 3.2)\n!pip install bitsandbytes\n\n\n\n2.1 Model pipeline\nFirst, we have to define the model pipeline to digest input prompts.\n\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n\nmodel_card = \"danielhanchen/open_llama_3b_600bt_preview\"\ntokenizer = AutoTokenizer.from_pretrained(model_card)\nmodel = AutoModelForCausalLM.from_pretrained(model_card, load_in_8bit=True, device_map='auto')\n\n\nHomework 📖\nExperiment with different model cards (either base or chat-base models)\n\n\n\n2.2 Inference\nWe are now ready to feed prompts to our model and evaluate its performance.\nLet’s start with an example.\n\ndef complete_prompt(prompt, **kwargs):\n    prompt = tokenizer(prompt, return_tensors='pt').to('cuda')\n    generated = model.generate(input_ids=prompt['input_ids'],\n                           attention_mask=prompt['attention_mask'],\n                           **kwargs)\n    generated = tokenizer.batch_decode(generated, skip_special_tokens=True)\n    return generated[0]\n\nprompt = \"\"\"Classify the text into negative or positive. \nText: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\nSentiment:\n\"\"\"\n\ngenerated = complete_prompt(prompt, max_new_tokens=10)\nprint(generated)\n\nClassify the text into negative or positive. \nText: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\nSentiment:\n\n\n*\n\n*Negative:\n\n\nNow we try with the whole test set.\n\ndef prepend_prompt(example):\n    example['prompt'] = formatting_prompt.format(example['text'])\n    return example\n    \n\nformatting_prompt = \"\"\"Classify the text into negative or positive. \nText: {0}\nSentiment:\n\"\"\"\n\ntest_data = test_data.map(prepend_prompt)\n\n                                                                                \n\n\n\n# first 100 examples for showcasing purposes only (especially given this lazy implementation)\ngenerated = [complete_prompt(prompt, max_new_tokens=10) for prompt in tqdm(test_data['prompt'][:100])]\n\n100%|█████████████████████████████████████████| 100/100 [00:57&lt;00:00,  1.74it/s]\n\n\n\ndef parse_generation(text):\n    label = text.split('Sentiment:')[1].strip()\n    return [0, 1] if 'positive' in label.casefold() else [1, 0]\n\npredictions = [parse_generation(seq) for seq in generated]\n\n\nmetrics = compute_metrics([np.array(predictions), np.array(test_data['label'][:100])])\nprint(metrics)\nprint(generated[4])\n\n{'f1': 0.4542619542619543, 'accuracy': 0.58}\nClassify the text into negative or positive. \nText: Good cinematography, good acting good direction...cannot justify a story that is not and cannot be acceptable to any society. Amitabh has often used the media to make this junk sell able by saying that -- if such an incident happens...then what? I would like to ask him if such a thing happens for your own child or your grandchild (say girl child) then what will you do? I think every parents will have to take special care before interacting with any 60 year old neighbor if you have one -jia- with you. Such films should be banned and discouraged otherwise you inspire more more Nithari cases. Such acts are villainous and villains in films are punished..that should be the moral of the story and not glorify their act or them.\nSentiment:\nI am not sure if this is a sentiment or"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#advanced-prompting",
    "href": "nlp-course-material/2023-2024/Tutorial 3/Tutorial3-2324.html#advanced-prompting",
    "title": "Tutorial 3",
    "section": "3. Advanced Prompting",
    "text": "3. Advanced Prompting\nThere is no rule of thumb to perform well on prompting.\nSome may argue it is art, some others might say it is just engineering.\nHowever, here are some general recommendations:\n\nCheck how the pre-trained model you are using was trained!\nStart simple and then refine.\nInstructions at the start/end of the prompt \\(\\rightarrow\\) based on how most attention layers work.\nSeparate input text from instructions\nProvide clear description of the task: no ambiguity, text format, style, language, etc…\nEvaluate the prompt on several models\nUse advanced techniques: few-shot prompting, Chain-of-thought (CoT), Least-to-Most (LtM)\n\n\n3.1 From Zero- to Few-shot Prompting\nIn many situations, a prompt containing instructions is not sufficient for a model to behave properly.\nWe can improve the prompt by providing a few ground-truth examples showing how the model should behave.\nClassify the text into negative or positive. \nText: {example1}\nSentiment: {label1}\nText: {example2}\nSentiment: {label2}\nText: {example3}\nSentiment: {label3}\nText: {text}\nSentiment:\n\nExamples may be insufficient\nDepending on the task at hand, providing examples may be not sufficient for the model to understand the instructions.\nAlso, the model might ignore provided examples or it might still perform correctly despite using intentionally wrong examples!!\n\n\nLengthy prompts\nAdding examples increases the level of detail of prompt, while it may considerably increases its length.\nPay attention to what model_card you choose since your model may truncate input prompts!\nAdditionally, a lengthy prompt increases computation!\n\n\nExamples quality\nChoosing the right set of examples has an impact on model performance.\nIntuitively, we select examples to maximize (textual) diversity and cover the whole label distribution.\nIn practice, this may be harder than expected: models are sensitive to prompt formatting.\nLet’s try sentiment analysis again with Few-shot prompting!\n\ndef prepend_prompt(example):\n    example['prompt'] = formatting_prompt.format(example['text'])\n    return example\n    \n\nformatting_prompt = \"\"\"Classify the text into negative or positive.\nText: Everything is so well done: acting, directing, visuals, settings, photography, casting. If you can enjoy a story of real people and real love - this is a winner.\nLabel: positive\nText: This is one of the dumbest films, I've ever seen. It rips off nearly ever type of thriller and manages to make a mess of them all.\nSentiment: negative\nText: {0}\nSentiment:\n\"\"\"\n\ntest_data = test_data.map(prepend_prompt)\n\n                                                                                \n\n\n\n# first 100 examples for showcasing purposes only (especially given this lazy implementation)\ngenerated = [complete_prompt(prompt, max_new_tokens=10) for prompt in tqdm(test_data['prompt'][:100])]\n\n100%|█████████████████████████████████████████| 100/100 [01:03&lt;00:00,  1.58it/s]\n\n\n\npredictions = [parse_generation(seq) for seq in generated]\n\nmetrics = compute_metrics([np.array(predictions), np.array(test_data['label'][:100])])\nprint(metrics)\n\n{'f1': 0.36305732484076436, 'accuracy': 0.57}\n\n\n\n\nHomework 📖\nExperiment with different few-shot examples and evaluate corresponding model performance.\n\n\n\n3.2 Chain-of-thought (CoT) Prompting\nProviding examples to improve task performance may fail in complex scenarios like reasoning tasks.\nCoT prompting forces the model to generate intermediate reasoning steps before providing the final output.\nCoT can either be achieved via\n\nFew-shot examples on how to perform reasoning\nDefining the prompt to force reasoning (e.g., let’s think step by step)\n\nLet’s try our sentiment analysis task with CoT prompting\n\ndef prepend_prompt(example):\n    example['prompt'] = formatting_prompt.format(example['text'])\n    return example\n    \n\nformatting_prompt = \"\"\"Classify the text into negative or positive.\nText: Everything is so well done: acting, directing, visuals, settings, photography, casting. If you can enjoy a story of real people and real love - this is a winner.\nLabel: positive\nText: This is one of the dumbest films, I've ever seen. It rips off nearly ever type of thriller and manages to make a mess of them all.\nSentiment: negative\nText: {0}\nLet's think step by step.\nSentiment:\n\"\"\"\n\ntest_data = test_data.map(prepend_prompt)\n\n                                                                                \n\n\n\n# first 100 examples for showcasing purposes only (especially given this lazy implementation)\ngenerated = [complete_prompt(prompt, max_new_tokens=20) for prompt in tqdm(test_data['prompt'][:100])]\n\n100%|█████████████████████████████████████████| 100/100 [01:53&lt;00:00,  1.13s/it]\n\n\n\npredictions = [parse_generation(seq) for seq in generated]\n\nmetrics = compute_metrics([np.array(predictions), np.array(test_data['label'][:100])])\nprint(metrics)\n\n{'f1': 0.36305732484076436, 'accuracy': 0.57}\n\n\n\nHomework 📖\nExperiment with different CoT prompts to enforce intermediate reasoning steps.\nFor more details check this page.\n\n\n\n3.3 Prompting vs Fine-tuning\nAt last, we may wondering on which technique to use.\nIn short, prompting comes at hand when transferring a pre-trained model on a domain that has some affinities with those seen during training.\nIn other cases like:\n\nDifferent domain\nSensitive data\nLow-resource language\nDomain-specific model constraints\n\nFine-tuning is the preferred choice (to maximize improvements)"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Assignment 1/Assignment1.html",
    "href": "nlp-course-material/2023-2024/Assignment 1/Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Credits: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\nKeywords: POS tagging, Sequence labelling, RNNs\n\nContact\nFor any doubt, question, issue or help, you can always contact us at the following email addresses:\nTeaching Assistants:\n\nFederico Ruggeri -&gt; federico.ruggeri6@unibo.it\nEleonora Mancini -&gt; e.mancini@unibo.it\n\nProfessor:\n\nPaolo Torroni -&gt; p.torroni@unibo.it\n\n\n\nIntroduction\nYou are tasked to address the task of POS tagging.\n\n\n\n\n\n[Task 1 - 0.5 points] Corpus\nYou are going to work with the Penn TreeBank corpus.\nIgnore the numeric value in the third column, use only the words/symbols and their POS label.\n\nExample\nPierre   NNP 2 Vinken  NNP 8 ,   ,   2 61  CD  5 years   NNS 6 old JJ  2 ,   ,   2 will    MD  0 join    VB  8 the DT  11 board   NN  9 as  IN  9 a   DT  15 nonexecutive    JJ  15 director    NN  12 Nov.    NNP 9 29  CD  16 .   .   8\n\n\nSplits\nThe corpus contains 200 documents.\n\nTrain: Documents 1-100\nValidation: Documents 101-150\nTest: Documents 151-199\n\n\n\nInstructions\n\nDownload the corpus.\nEncode the corpus into a pandas.DataFrame object.\nSplit it in training, validation, and test sets.\n\n\n\n\n[Task 2 - 0.5 points] Text encoding\nTo train a neural POS tagger, you first need to encode text into numerical format.\n\nInstructions\n\nEmbed words using GloVe embeddings.\nYou are free to pick any embedding dimension.\n[Optional] You are free to experiment with text pre-processing: make sure you do not delete any token!\n\n\n\n\n[Task 3 - 1.0 points] Model definition\nYou are now tasked to define your neural POS tagger.\n\nInstructions\n\nBaseline: implement a Bidirectional LSTM with a Dense layer on top.\nYou are free to experiment with hyper-parameters to define the baseline model.\nModel 1: add an additional LSTM layer to the Baseline model.\nModel 2: add an additional Dense layer to the Baseline model.\nDo not mix Model 1 and Model 2. Each model has its own instructions.\n\nNote: if a document contains many tokens, you are free to split them into chunks or sentences to define your mini-batches.\n\n\n\n[Task 4 - 1.0 points] Metrics\nBefore training the models, you are tasked to define the evaluation metrics for comparison.\n\nInstructions\n\nEvaluate your models using macro F1-score, compute over all tokens.\nConcatenate all tokens in a data split to compute the F1-score. (Hint: accumulate FP, TP, FN, TN iteratively)\nDo not consider punctuation and symbol classes \\(\\rightarrow\\) What is punctuation?\n\nNote: What about OOV tokens? * All the tokens in the training set that are not in GloVe must be added to the vocabulary. * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a special token (e.g., [UNK]) and a static embedding. * You are free to define the static embedding using any strategy (e.g., random, neighbourhood, etc…)\n\n\nMore about OOV\nFor a given token:\n\nIf in train set: add to vocabulary and assign an embedding (use GloVe if token in GloVe, custom embedding otherwise).\nIf in val/test set: assign special token if not in vocabulary and assign custom embedding.\n\nYour vocabulary should:\n\nContain all tokens in train set; or\nUnion of tokens in train set and in GloVe \\(\\rightarrow\\) we make use of existing knowledge!\n\n\n\nToken to embedding mapping\nYou can follow two approaches for encoding tokens in your POS tagger.\n\n\nWork directly with embeddings\n\nCompute the embedding of each input token\nFeed the mini-batches of shape (batch_size, # tokens, embedding_dim) to your model\n\n\n\nWork with Embedding layer\n\nEncode input tokens to token ids\nDefine a Embedding layer as the first layer of your model\nCompute the embedding matrix of all known tokens (i.e., tokens in your vocabulary)\nInitialize the Embedding layer with the computed embedding matrix\nYou are free to set the Embedding layer trainable or not\n\n\nembedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n                                      output_dim=embedding_dimension,\n                                      weights=[embedding_matrix],\n                                      mask_zero=True,                   # automatically masks padding tokens\n                                      name='encoder_embedding')\n\n\n\nPadding\nPay attention to padding tokens!\nYour model should not be penalized on those tokens.\n\nHow to?\nThere are two main ways.\nHowever, their implementation depends on the neural library you are using.\n\nEmbedding layer\nCustom loss to compute average cross-entropy on non-padding tokens only\n\nNote: This is a recommendation, but we do not penalize for missing workarounds.\n\n\n\n\n[Task 5 - 1.0 points] Training and Evaluation\nYou are now tasked to train and evaluate the Baseline, Model 1, and Model 2.\n\nInstructions\n\nTrain all models on the train set.\nEvaluate all models on the validation set.\nCompute metrics on the validation set.\nPick at least three seeds for robust estimation.\nPick the best performing model according to the observed validation set performance.\n\n\n\n\n[Task 6 - 1.0 points] Error Analysis\nYou are tasked to evaluate your best performing model.\n\nInstructions\n\nCompare the errors made on the validation and test sets.\nAggregate model errors into categories (if possible)\nComment the about errors and propose possible solutions on how to address them.\n\n\n\n\n[Task 7 - 1.0 points] Report\nWrap up your experiment in a short report (up to 2 pages).\n\nInstructions\n\nUse the NLP course report template.\nSummarize each task in the report following the provided template.\n\n\n\nRecommendations\nThe report is not a copy-paste of graphs, tables, and command outputs.\n\nSummarize classification performance in Table format.\nDo not report command outputs or screenshots.\nReport learning curves in Figure format.\nThe error analysis section should summarize your findings.\n\n\n\n\nSubmission\n\nSubmit your report in PDF format.\nSubmit your python notebook.\nMake sure your notebook is well organized, with no temporary code, commented sections, tests, etc…\nYou can upload model weights in a cloud repository and report the link in the report.\n\n\n\nFAQ\nPlease check this frequently asked questions before contacting us\n\nExecution Order\nYou are free to address tasks in any order (if multiple orderings are available).\n\n\nTrainable Embeddings\nYou are free to define a trainable or non-trainable Embedding layer to load the GloVe embeddings.\n\n\nModel architecture\nYou should not change the architecture of a model (i.e., its layers).\nHowever, you are free to play with their hyper-parameters.\n\n\nNeural Libraries\nYou are free to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc…)\n\n\nKeras TimeDistributed Dense layer\nIf you are using Keras, we recommend wrapping the final Dense layer with TimeDistributed.\n\n\nRobust Evaluation\nEach model is trained with at least 3 random seeds.\nTask 4 requires you to compute the average performance over the 3 seeds and its corresponding standard deviation.\n\n\nModel Selection for Analysis\nTo carry out the error analysis you are free to either\n\nPick examples or perform comparisons with an individual seed run model (e.g., Baseline seed 1337)\nPerform ensembling via, for instance, majority voting to obtain a single model.\n\n\n\nError Analysis\nSome topics for discussion include: * Model performance on most/less frequent classes. * Precision/Recall curves. * Confusion matrices. * Specific misclassified samples.\n\n\nPunctuation\nDo not remove punctuation from documents since it may be helpful to the model.\nYou should ignore it during metrics computation.\nIf you are curious, you can run additional experiments to verify the impact of removing punctuation.\n\n\n\nThe End"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Assignment 2/Assignment2.html",
    "href": "nlp-course-material/2023-2024/Assignment 2/Assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Credits: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\nKeywords: Human Value Detection, Multi-label classification, Transformers, BERT"
  },
  {
    "objectID": "nlp-course-material/2023-2024/Assignment 2/Assignment2.html#problem-definition",
    "href": "nlp-course-material/2023-2024/Assignment 2/Assignment2.html#problem-definition",
    "title": "Assignment 2",
    "section": "Problem definition",
    "text": "Problem definition\nArguments are paired with their conveyed human values.\nArguments are in the form of premise \\(\\rightarrow\\) conclusion.\n\nExample:\nPremise: ``fast food should be banned because it is really bad for your health and is costly’’\nConclusion: ``We should ban fast food’’\nStance: in favour of"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 1/Assignment1-2425.html",
    "href": "nlp-course-material/2024-2025/Assignment 1/Assignment1-2425.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Credits: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\nKeywords: Sexism Detection, Multi-class Classification, RNNs, Transformers, Huggingface"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 1/Assignment1-2425.html#problem-definition",
    "href": "nlp-course-material/2024-2025/Assignment 1/Assignment1-2425.html#problem-definition",
    "title": "Assignment 1",
    "section": "Problem Definition",
    "text": "Problem Definition\nThe systems have to decide whether or not a given tweet contains or describes sexist expressions or behaviors (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behavior).\n\nExamples:\nText: Can’t go a day without women womening\nLabel: Sexist\nText: ‘’Society’s set norms! Happy men’s day though!#weareequal’’\nLabel: Not sexist\n#[Task 1 - 1.0 points] Corpus\nWe have preparared a small version of EXIST dataset in our dedicated Github repository.\nCheck the A1/data folder. It contains 3 .json files representing training, validation and test sets.\nThe three sets are slightly unbalanced, with a bias toward the Non-sexist class.\n\n\nDataset Description\n\nThe dataset contains tweets in both English and Spanish.\nThere are labels for multiple tasks, but we are focusing on Task 1.\nFor Task 1, soft labels are assigned by six annotators.\nThe labels for Task 1 represent whether the tweet is sexist (“YES”) or not (“NO”).\n\n\n\nExample\n\"203260\": {\n    \"id_EXIST\": \"203260\",\n    \"lang\": \"en\",\n    \"tweet\": \"ik when mandy says “you look like a whore” i look cute as FUCK\",\n    \"number_annotators\": 6,\n    \"annotators\": [\"Annotator_473\", \"Annotator_474\", \"Annotator_475\", \"Annotator_476\", \"Annotator_477\", \"Annotator_27\"],\n    \"gender_annotators\": [\"F\", \"F\", \"M\", \"M\", \"M\", \"F\"],\n    \"age_annotators\": [\"18-22\", \"23-45\", \"18-22\", \"23-45\", \"46+\", \"46+\"],\n    \"labels_task1\": [\"YES\", \"YES\", \"YES\", \"NO\", \"YES\", \"YES\"],\n    \"labels_task2\": [\"DIRECT\", \"DIRECT\", \"REPORTED\", \"-\", \"JUDGEMENTAL\", \"REPORTED\"],\n    \"labels_task3\": [\n      [\"STEREOTYPING-DOMINANCE\"],\n      [\"OBJECTIFICATION\"],\n      [\"SEXUAL-VIOLENCE\"],\n      [\"-\"],\n      [\"STEREOTYPING-DOMINANCE\", \"OBJECTIFICATION\"],\n      [\"OBJECTIFICATION\"]\n    ],\n    \"split\": \"TRAIN_EN\"\n  }\n}\n\n\nInstructions\n\nDownload the A1/data folder.\nLoad the three JSON files and encode them as pandas dataframes.\nGenerate hard labels for Task 1 using majority voting and store them in a new dataframe column called hard_label_task1. Items without a clear majority will be removed from the dataset.\nFilter the DataFrame to keep only rows where the lang column is 'en'.\nRemove unwanted columns: Keep only id_EXIST, lang, tweet, and hard_label_task1.\nEncode the hard_label_task1 column: Use 1 to represent “YES” and 0 to represent “NO”."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html",
    "href": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Credits: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\nKeywords: Sexism Detection, Multi-class Classification, LLMs, Prompting"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#problem-definition",
    "href": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#problem-definition",
    "title": "Assignment 2",
    "section": "Problem definition",
    "text": "Problem definition\nGiven an input text sentence, the task is to label the sentence as sexist or not sexist (binary classification).\n\nExamples:\nText: *``Schedule a date with her, then don’t show up. Then text her “GOTCHA B___H”.’’*\nLabel: Sexist\nText: ``That’s completely ridiculous a woman flashing her boobs is not sexual assault in the slightest.’’\nLabel: Not sexist"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#approach",
    "href": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#approach",
    "title": "Assignment 2",
    "section": "Approach",
    "text": "Approach\nWe will tackle the binary classification task with LLMs.\nIn particular, we’ll consider zero-/few-shot prompting approaches to assess the capability of some popular open-source LLMs on this task."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#preliminaries",
    "href": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#preliminaries",
    "title": "Assignment 2",
    "section": "Preliminaries",
    "text": "Preliminaries\nWe are going to download LLMs from Huggingface.\nMany of these open-source LLMs require you to accept their “Community License Agreement” to download them.\nIn summary:\n\nIf not already, create an account of Huggingface (~2 mins)\nCheck a LLM model card page (e.g., Mistral v3) and accept its “Community License Agreement”.\nGo to your account -&gt; Settings -&gt; Access Tokens -&gt; Create new token -&gt; “Repositories permissions” -&gt; add the LLM model card you want to use.\nSave the token (we’ll need it later)\n\n\nHuggingface Login\nOnce we have created an account and an access token, we need to login to Huggingface via code.\n\nType your token and press Enter\nYou can say No to Github linking\n\n\n!huggingface-cli login\n\nAfter login, you can download all models associated with your access token in addition to those that are not protected by an access token.\n\n\nData Loading\nSince we are only intered in prompting, we do not require a train dataset.\nWe have preparared a small test set version of EDOS in our dedicated Github repository.\nCheck the A2/data folder. It contains:\n\na2_test.csv → a small test set of 300 samples.\ndemonstrations.csv -&gt; a batch of 1000 samples for few-shot prompting.\n\nBoth datasets contain a balanced number of sexist and not sexist samples.\n\n\nInstructions\nIn order to get Task 1 points, we require you to:\n\nDownload the A2/data folder.\nEncode a2_test.csv into a pandas.DataFrame object."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#which-llms",
    "href": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#which-llms",
    "title": "Assignment 2",
    "section": "Which LLMs?",
    "text": "Which LLMs?\nThe pool of LLMs is ever increasing and it’s impossible to keep track of all new entries.\nWe focus on popular open-source models.\n\nMistral v2\nMistral v3\nLlama v3.1\nPhi3-mini\n\nOther open-source models are more than welcome!\n\nInstructions\nIn order to get Task 1 points, we require you to:\n\nPick 2 model cards from the provided list.\nFor each model:\n\nDefine a separate section of your notebook for the model.\nSetup a quantization configuration for the model.\nLoad the model via HuggingFace APIs.\n\n\n\n\nNotes\n\nThere’s a popular library integrated with Huggingface’s transformers to perform quantization.\nDefine two separate sections of your notebook to show that you have implemented the prompting pipeline for each selected model card."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#prompt-template",
    "href": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#prompt-template",
    "title": "Assignment 2",
    "section": "Prompt Template",
    "text": "Prompt Template\nUse the following prompt template to process input texts.\n\nprompt = [\n    {\n        'role': 'system',\n        'content': 'You are an annotator for sexism detection.'\n    },\n    {\n        'role': 'user',\n        'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n\n        TEXT:\n        {text}\n\n        ANSWER:\n        \"\"\"\n    }\n]\n\n\nInstructions\nIn order to get Task 2 points, we require you to:\n\nWrite a prepare_prompts function as the one reported below.\n\n\ndef prepare_prompts(texts, prompt_template, tokenizer):\n  \"\"\"\n    This function format input text samples into instructions prompts.\n\n    Inputs:\n      texts: input texts to classify via prompting\n      prompt_template: the prompt template provided in this assignment\n      tokenizer: the transformers Tokenizer object instance associated with the chosen model card\n\n    Outputs:\n      input texts to classify in the form of instruction prompts\n  \"\"\"\n  pass\n\n\n\nNotes\n\nYou are free to modify the prompt format (not its content) as you like depending on your code implementation.\nNote that the provided prompt has placeholders. You need to format the string to replace placeholders. Huggingface might have dedicated APIs for this."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#notes-2",
    "href": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#notes-2",
    "title": "Assignment 2",
    "section": "Notes",
    "text": "Notes\n\nAccording to our tests, it should take you ~10 mins to perform full inference on 300 samples."
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#instructions-5",
    "href": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#instructions-5",
    "title": "Assignment 2",
    "section": "Instructions",
    "text": "Instructions\nIn order to get Task 5 points, we require you to:\n\nLoad demonstrations.csv and encode it into a pandas.DataFrame object.\nDefine a build_few_shot_demonstrations function as the one reported below.\nPerform few-shot inference as in Task 3.\nCompute metrics as in Task 4.\n\n\ndef build_few_shot_demonstrations(demonstrations, num_per_class=2):\n  \"\"\"\n    Inputs:\n      demonstrations: the pandas.DataFrame object wrapping demonstrations.csv\n      num_per_class: number of demonstrations per class\n\n    Outputs:\n      a list of textual demonstrations to inject into the prompt template.\n  \"\"\"\n  pass"
  },
  {
    "objectID": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#notes-3",
    "href": "nlp-course-material/2024-2025/Assignment 2/Assignment2-2425.html#notes-3",
    "title": "Assignment 2",
    "section": "Notes",
    "text": "Notes\n\nYou are free to pick any value for num_per_class.\nAccording to our tests, few-shot prompting increases inference time by some minutes (we experimented with num_per_class \\(\\in [2, 4]\\))."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP Assignments",
    "section": "",
    "text": "Welcome\nThis site showcases the reports of my assignments on sexism detection, developed for the NLP course. You will find:\n\nAssignment 1: A comparative analysis between LSTM and Transformer models.\nAssignment 2: An exploration of Prompting techniques with Large Language Models (LLMs).\n\nFeel free to explore the links for more details!"
  }
]